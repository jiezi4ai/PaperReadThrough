{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Test for Paper Reaqd Through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- time: 2025-02-20\n",
    "- first trial: on pdf processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import fitz\n",
    "import toml\n",
    "import copy\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Though\"\n",
    "pdf_path = \"/home/jiezi/Code/GitHub/PaperReadThrough/data/2501.04682v1/2501.04682v1.pdf\"\n",
    "data_path = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 13:50:55,217 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Towards+System+2+Reasoning+in+LLMs%3A+Learning+How+to+Think+With+Meta+Chain-of-Though&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=3 \"HTTP/1.1 429 \"\n",
      "2025-03-04 13:51:26,462 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/search?query=Towards+System+2+Reasoning+in+LLMs%3A+Learning+How+to+Think+With+Meta+Chain-of-Though&fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=3 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0e63a3aebf14fc7a68c0df7a922770bde5b77360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 13:51:27,585 - INFO - HTTP Request: GET https://api.semanticscholar.org/graph/v1/paper/0e63a3aebf14fc7a68c0df7a922770bde5b77360/references?fields=contexts%2Cintents%2CcontextsWithIntent%2CisInfluential%2Cabstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear&offset=0&limit=100 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from apis.semanticscholar_tool import SemanticScholarKit\n",
    "\n",
    "ss = SemanticScholarKit()\n",
    "ss_metadata = ss.search_paper_by_keywords(query=title, limit=3)\n",
    "\n",
    "paper_ss_id = ss_metadata[0].get('paperId')\n",
    "print(paper_ss_id)\n",
    "\n",
    "reference_metadata = ss.get_semanticscholar_references(paper_id=paper_ss_id, limit=100)\n",
    "len(reference_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': '0e63a3aebf14fc7a68c0df7a922770bde5b77360',\n",
       " 'externalIds': {'DBLP': 'journals/corr/abs-2501-04682',\n",
       "  'ArXiv': '2501.04682',\n",
       "  'DOI': '10.48550/arXiv.2501.04682',\n",
       "  'CorpusId': 275357763},\n",
       " 'corpusId': 275357763,\n",
       " 'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "  'name': 'arXiv.org',\n",
       "  'alternate_names': ['ArXiv'],\n",
       "  'issn': '2331-8422',\n",
       "  'url': 'https://arxiv.org'},\n",
       " 'url': 'https://www.semanticscholar.org/paper/0e63a3aebf14fc7a68c0df7a922770bde5b77360',\n",
       " 'title': 'Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought',\n",
       " 'abstract': 'We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We present empirical evidence from state-of-the-art models exhibiting behaviors consistent with in-context search, and explore methods for producing Meta-CoT via process supervision, synthetic data generation, and search algorithms. Finally, we outline a concrete pipeline for training a model to produce Meta-CoTs, incorporating instruction tuning with linearized search traces and reinforcement learning post-training. Finally, we discuss open research questions, including scaling laws, verifier roles, and the potential for discovering novel reasoning algorithms. This work provides a theoretical and practical roadmap to enable Meta-CoT in LLMs, paving the way for more powerful and human-like reasoning in artificial intelligence.',\n",
       " 'venue': 'arXiv.org',\n",
       " 'year': 2025,\n",
       " 'referenceCount': 0,\n",
       " 'citationCount': 9,\n",
       " 'influentialCitationCount': 1,\n",
       " 'isOpenAccess': False,\n",
       " 'openAccessPdf': None,\n",
       " 'fieldsOfStudy': ['Computer Science'],\n",
       " 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "  {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       " 'publicationTypes': ['JournalArticle'],\n",
       " 'publicationDate': '2025-01-08',\n",
       " 'journal': {'name': 'ArXiv', 'volume': 'abs/2501.04682'},\n",
       " 'citationStyles': {'bibtex': '@Article{Xiang2025TowardsS2,\\n author = {Violet Xiang and Charlie Snell and Kanishk Gandhi and Alon Albalak and Anikait Singh and Chase Blagden and Duy Phung and Rafael Rafailov and nathan lile and Dakota Mahan and Louis Castricato and Jan-Philipp Franken and Nick Haber and Chelsea Finn},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought},\\n volume = {abs/2501.04682},\\n year = {2025}\\n}\\n'},\n",
       " 'authors': [{'authorId': '2274104658', 'name': 'Violet Xiang'},\n",
       "  {'authorId': '2339001780', 'name': 'Charlie Snell'},\n",
       "  {'authorId': '2333592056', 'name': 'Kanishk Gandhi'},\n",
       "  {'authorId': '2044198106', 'name': 'Alon Albalak'},\n",
       "  {'authorId': '2111007256', 'name': 'Anikait Singh'},\n",
       "  {'authorId': '2326300653', 'name': 'Chase Blagden'},\n",
       "  {'authorId': '2273535247', 'name': 'Duy Phung'},\n",
       "  {'authorId': '102801230', 'name': 'Rafael Rafailov'},\n",
       "  {'authorId': '2283848553', 'name': 'nathan lile'},\n",
       "  {'authorId': '2287928420', 'name': 'Dakota Mahan'},\n",
       "  {'authorId': '28933528', 'name': 'Louis Castricato'},\n",
       "  {'authorId': '2220625823', 'name': 'Jan-Philipp Franken'},\n",
       "  {'authorId': '2274104149', 'name': 'Nick Haber'},\n",
       "  {'authorId': '2284774407', 'name': 'Chelsea Finn'}]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss_metadata[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_process.pdf_outline_gen import PDFOutline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outline = PDFOutline(pdf_path=pdf_path)\n",
    "toc_1 = outline.toc_extraction()\n",
    "toc_2 = outline.toc_detection()\n",
    "\n",
    "toc_1_rvsd = outline.identify_toc_appendix(toc_1)\n",
    "toc_2_rvsd = outline.identify_toc_appendix(toc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apis.mineru_tool import MinerUKit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mineru = MinerUKit(api_key=os.getenv('MINERU_API_KEY_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response success. result:{'code': 0, 'msg': 'ok', 'trace_id': '6db9190fa60ce739c5b13f3a3c5ff28f', 'data': {'batch_id': 'cadaaa92-55b1-4873-90e2-6460416076ee', 'file_urls': ['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/cadaaa92-55b1-4873-90e2-6460416076ee/a2e442af-aefc-4eac-ac17-eeecc2d6c8f3.pdf?Expires=1741153950&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=eNorG4VwceP1Qv8kw4Z1guJYGgY%3D']}}\n",
      "batch_id:cadaaa92-55b1-4873-90e2-6460416076ee,urls:['https://mineru.oss-cn-shanghai.aliyuncs.com/api-upload/cadaaa92-55b1-4873-90e2-6460416076ee/a2e442af-aefc-4eac-ac17-eeecc2d6c8f3.pdf?Expires=1741153950&OSSAccessKeyId=LTAI5t9nGwatk85zetzojXbn&Signature=eNorG4VwceP1Qv8kw4Z1guJYGgY%3D']\n",
      "upload success\n"
     ]
    }
   ],
   "source": [
    "res = mineru.batch_process_files(pdf_files=[pdf_path], if_ocr=False, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cadaaa92-55b1-4873-90e2-6460416076ee\n",
      "200\n",
      "Batch cadaaa92-55b1-4873-90e2-6460416076ee complte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: /home/jiezi/Code/GitHub/PaperReadThrough/paperreadthrough/notes/2501.04682v1.zip\n",
      "Successfully unzipped: /home/jiezi/Code/GitHub/PaperReadThrough/paperreadthrough/notes/2501.04682v1\n"
     ]
    }
   ],
   "source": [
    "if res.status_code == 200:\n",
    "    batch_id = res.json().get('data', {}).get('batch_id')\n",
    "    print(batch_id)\n",
    "    if batch_id:\n",
    "        mineru.monitor_batch_status(\n",
    "            batch_id=batch_id, \n",
    "            save_path=\"/home/jiezi/Code/GitHub/PaperReadThrough/paperreadthrough/notes\", \n",
    "            interval=10, max_retries=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Post Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.basename(pdf_path)\n",
    "file_name_nosuffix = file_name.rsplit('.', 1)[0] \n",
    "processed_file_path = os.path.join(data_path, file_name_nosuffix)\n",
    "\n",
    "md_file = os.path.join(processed_file_path, \"full.md\")\n",
    "content_json_file = os.path.join(processed_file_path, \"content_list.json\")\n",
    "\n",
    "import json\n",
    "with open(content_json_file) as json_data:\n",
    "    content_json = json.load(json_data)\n",
    "\n",
    "with open(md_file, 'r', encoding='utf-8') as f:\n",
    "    markdown_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_process.pdf_post_process import PDFProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PDFProcess(pdf_path=pdf_path, pdf_toc=toc_1_rvsd,pdf_json=content_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.align_md_toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_REGX_NAME_PTRN = r\"(pic|picture|img|image|chart|figure|fig|table|tbl)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id_lst = []  # store all images ids\n",
    "\n",
    "for idx, item in enumerate(pdf.pdf_json):\n",
    "    if item['type'] in ['image']:\n",
    "        if item['img_caption'] == [] and item['img_footnote'] == []:  # without caption and footnote in the block\n",
    "            prev_pos, next_pos = 9999, 9999\n",
    "            prev_id, next_id = None, None\n",
    "            prev_img_ids, next_img_ids = [], []\n",
    "\n",
    "            # check next block for image description info\n",
    "            if idx < len(pdf.pdf_json) - 1 and pdf.pdf_json[idx+1]['type'] == 'text' and pdf.pdf_json[idx+1].get('text') is not None:\n",
    "                next_item = pdf.pdf_json[idx+1]\n",
    "                mtch_rslts = re.finditer(IMG_REGX_NAME_PTRN, next_item['text'], re.IGNORECASE)\n",
    "\n",
    "                next_img_ids = []\n",
    "                for match in mtch_rslts:\n",
    "                    next_img_ids.append(match.group(0)) \n",
    "\n",
    "                for id in next_img_ids:\n",
    "                    if id not in img_id_lst:\n",
    "                        next_id = id\n",
    "                        next_pos = next_item['text'].index(id)\n",
    "                        next_img_ids = next_img_ids.remove(id)\n",
    "                        break\n",
    "\n",
    "            # check previous block for image description info\n",
    "            if idx > 1 and pdf.pdf_json[idx-1]['type'] == 'text' and pdf.pdf_json[idx-1].get('text') is not None:\n",
    "                prev_item = pdf.pdf_json[idx-1]\n",
    "                mtch_rslts = re.finditer(IMG_REGX_NAME_PTRN, prev_item['text'], re.IGNORECASE)\n",
    "\n",
    "                prev_img_ids = []\n",
    "                for match in mtch_rslts:\n",
    "                    prev_img_ids.append(match.group(0)) \n",
    "\n",
    "                for id in prev_img_ids:\n",
    "                    if id not in img_id_lst:\n",
    "                        prev_id = id\n",
    "                        prev_pos = prev_item['text'].index(id)\n",
    "                        prev_img_ids = prev_img_ids.remove(id)\n",
    "                        break\n",
    "            \n",
    "            if next_pos < prev_pos:\n",
    "                item['id'] = next_id\n",
    "                item['related_ids'] = next_img_ids\n",
    "                item['if_aligned'] = True\n",
    "                item['img_caption'] = [next_item['text']]\n",
    "                next_item['if_aligned'] = False\n",
    "\n",
    "            elif prev_pos < next_pos:\n",
    "                item['id'] = prev_id\n",
    "                item['related_ids'] = prev_img_ids\n",
    "                item['if_aligned'] = True\n",
    "                item['img_caption'] = [prev_item['text']]\n",
    "                prev_item['if_aligned'] = False\n",
    "                \n",
    "        else:\n",
    "            desc = \"\\n\".join(item.get('img_caption', [])) + \"\\n\" + \"\\n\".join(item.get('img_footnote', []))\n",
    "            mtch_rslts = re.finditer(IMG_REGX_NAME_PTRN, desc, re.IGNORECASE)\n",
    "\n",
    "            img_ids = []\n",
    "            for match in mtch_rslts:\n",
    "                img_ids.append(match.group(0)) \n",
    "            \n",
    "            for id in img_ids:\n",
    "                if id not in img_id_lst:\n",
    "                    break\n",
    "\n",
    "            item['id'] = id\n",
    "            item['related_ids'] = img_ids.remove(id)\n",
    "            item['if_aligned'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBL_REGX_NAME_PTRN = r\"(tbl|table|chart)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_id_lst = []  # store all table ids\n",
    "\n",
    "for idx, item in enumerate(pdf.pdf_json):\n",
    "    if item['type'] in ['table']:\n",
    "        if item['table_caption'] == [] and item['table_footnote'] == []:  # without caption and footnote in the block\n",
    "            prev_pos, next_pos = 9999, 9999\n",
    "            prev_id, next_id = None, None\n",
    "            prev_img_ids, next_img_ids = [], []\n",
    "\n",
    "            # check next block for image description info\n",
    "            if idx < len(pdf.pdf_json) - 1 and pdf.pdf_json[idx+1]['type'] == 'text' and pdf.pdf_json[idx+1].get('text') is not None:\n",
    "                next_item = pdf.pdf_json[idx+1]\n",
    "                mtch_rslts = re.finditer(IMG_REGX_NAME_PTRN, next_item['text'], re.IGNORECASE)\n",
    "\n",
    "                next_tbl_ids = []\n",
    "                for match in mtch_rslts:\n",
    "                    next_tbl_ids.append(match.group(0)) \n",
    "\n",
    "                for id in next_tbl_ids:\n",
    "                    if id not in tbl_id_lst:\n",
    "                        next_id = id\n",
    "                        next_pos = next_item['text'].index(id)\n",
    "                        next_tbl_ids = next_tbl_ids.remove(id)\n",
    "                        break\n",
    "\n",
    "            # check previous block for image description info\n",
    "            if idx > 1 and pdf.pdf_json[idx-1]['type'] == 'text' and pdf.pdf_json[idx-1].get('text') is not None:\n",
    "                prev_item = pdf.pdf_json[idx-1]\n",
    "                mtch_rslts = re.finditer(IMG_REGX_NAME_PTRN, prev_item['text'], re.IGNORECASE)\n",
    "\n",
    "                prev_tbl_ids = []\n",
    "                for match in mtch_rslts:\n",
    "                    prev_tbl_ids.append(match.group(0)) \n",
    "\n",
    "                for id in prev_tbl_ids:\n",
    "                    if id not in tbl_id_lst:\n",
    "                        prev_id = id\n",
    "                        prev_pos = prev_item['text'].index(id)\n",
    "                        prev_tbl_ids = prev_tbl_ids.remove(id)\n",
    "                        break\n",
    "            \n",
    "            if next_pos < prev_pos:\n",
    "                item['id'] = next_id\n",
    "                item['related_ids'] = next_img_ids\n",
    "                item['if_aligned'] = True\n",
    "                item['table_caption'] = [next_item['text']]\n",
    "                next_item['if_aligned'] = False\n",
    "\n",
    "            elif prev_pos < next_pos:\n",
    "                item['id'] = prev_id\n",
    "                item['related_ids'] = prev_img_ids\n",
    "                item['if_aligned'] = True\n",
    "                item['table_caption'] = [prev_item['text']]\n",
    "                prev_item['if_aligned'] = False\n",
    "                \n",
    "        else:\n",
    "            desc = \"\\n\".join(item.get('img_caption', [])) + \"\\n\" + \"\\n\".join(item.get('img_footnote', []))\n",
    "            mtch_rslts = re.finditer(IMG_REGX_NAME_PTRN, desc, re.IGNORECASE)\n",
    "\n",
    "            tbl_ids = []\n",
    "            for match in mtch_rslts:\n",
    "                tbl_ids.append(match.group(0)) \n",
    "            \n",
    "            for id in tbl_ids:\n",
    "                if id not in tbl_id_lst:\n",
    "                    item['id'] = id\n",
    "                    item['related_ids'] = tbl_ids.remove(id)\n",
    "                    item['if_aligned'] = True\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_example_json = {\n",
    " 'title': 'Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought',\n",
    " 'url': 'https://www.semanticscholar.org/paper/0e63a3aebf14fc7a68c0df7a922770bde5b77360',\n",
    " 'venue': 'arXiv.org',\n",
    " 'year': 2025,\n",
    " 'journal': {'name': 'ArXiv', 'volume': 'abs/2501.04682'},\n",
    " 'doi': '10.48550/arXiv.2501.04682',\n",
    "}\n",
    "        \n",
    "extract_ref_prompt = \"\"\"Please extract reference paper information from the input text. \n",
    "Output in json with format like:\n",
    "{reference_example_json}\n",
    "\n",
    "## INPUT\n",
    "{input_text}\n",
    "\n",
    "## OUTPUT\n",
    "Now get started. Provide only json result here:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_non_text_chars(text, with_digits: Optional[bool]=True):\n",
    "    \"\"\"remove non text chars\n",
    "    \"\"\"\n",
    "    valid_chars = string.ascii_letters\n",
    "    if with_digits == True:\n",
    "        valid_chars += string.digits  # 包含所有字母和数字的字符串\n",
    "    cleaned_text = ''\n",
    "    for char in text:\n",
    "        if char in valid_chars:\n",
    "            cleaned_text += char\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz # pip install thefuzz  https://github.com/seatgeek/thefuzz\n",
    "\n",
    "def text_match(text_a, text_b, with_digits: Optional[bool]=True):\n",
    "    \"\"\"\"fuzzy match between text_a and text_b\"\"\"\n",
    "    text_a = remove_non_text_chars(text_a, with_digits).lower()\n",
    "    text_b = remove_non_text_chars(text_b, with_digits).lower()\n",
    "    return fuzz.ratio(text_a, text_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pos = 0\n",
    "end_pos = len(pdf.pdf_json)\n",
    "\n",
    "# search for reference title postion as start \n",
    "for i in range(len(pdf.pdf_json)):\n",
    "    if pdf.pdf_json[i].get('text_level') == 1:\n",
    "        if text_match(pdf.pdf_json[i].get('text'), 'Reference', False) > 90:\n",
    "            start_pos = i + 1\n",
    "            break\n",
    "\n",
    "if start_pos > 0:\n",
    "    for j in range(start_pos, len(pdf.pdf_json)):\n",
    "        if pdf.pdf_json[j].get('text_level') is not None:\n",
    "            end_pos = j\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(357, 414)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_pos, end_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for idx, item in enumerate(pdf.pdf_json):\n",
    "    if idx >= start_pos and idx < end_pos:\n",
    "        line = {'line_id': idx, 'bib_text': item.get('text')}\n",
    "        lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 17:22:10,481 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-03-04 17:22:34,790 - INFO - AFC remote call 1 is done.\n",
      "2025-03-04 17:22:34,792 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-03-04 17:22:46,987 - INFO - AFC remote call 1 is done.\n",
      "2025-03-04 17:22:46,988 - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-03-04 17:23:00,813 - INFO - AFC remote call 1 is done.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from paper_comprehension.models import llm_gen_w_retry\n",
    "\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY_1')\n",
    "gemini_model_name = \"gemini-2.0-flash\"\n",
    "n = math.ceil(len(lines) / 20)\n",
    "\n",
    "results = []\n",
    "for i in range(n):\n",
    "    input_text = lines[i*20:i*20+20]\n",
    "    qa_prompt = extract_ref_prompt.format(\n",
    "        reference_example_json=reference_example_json,\n",
    "        input_text=input_text)\n",
    "    res = llm_gen_w_retry(gemini_api_key, gemini_model_name, qa_prompt)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json_repair import repair_json  # https://github.com/mangiucugna/json_repair/\n",
    "\n",
    "refs = []\n",
    "for res in results:\n",
    "    res_json = (json.loads(repair_json(res)))\n",
    "    for item in res_json:\n",
    "        refs.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_arxiv_ids = []\n",
    "for item in refs:\n",
    "    if item.get('doi') is not None:\n",
    "        doi_arxiv_ids.append(item.get('doi'))\n",
    "    elif item.get('url', '').startswith(\"https://arxiv.org\"):\n",
    "        arxiv_no = item.get('url', '').split('/')[-1]\n",
    "        arxiv_id = re.sub(r'v\\d+$', '', arxiv_no)\n",
    "        doi = f\"10.48550/arXiv.{arxiv_id}\"\n",
    "        doi_arxiv_ids.append(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(doi_arxiv_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 17:41:10,606 - INFO - HTTP Request: POST https://api.semanticscholar.org/graph/v1/paper/batch?fields=abstract%2Cauthors%2CcitationCount%2CcitationStyles%2CcorpusId%2CexternalIds%2CfieldsOfStudy%2CinfluentialCitationCount%2CisOpenAccess%2Cjournal%2CopenAccessPdf%2CpaperId%2CpublicationDate%2CpublicationTypes%2CpublicationVenue%2CreferenceCount%2Cs2FieldsOfStudy%2Ctitle%2Curl%2Cvenue%2Cyear \"HTTP/1.1 200 OK\"\n",
      "2025-03-04 17:41:11,314 - WARNING - IDs not found: ['10.48550/arXiv.2402.14740', '10.48550/arXiv.2211.12588', '10.48550/arXiv.2110.14168', '10.48550/arXiv.1807.03819', '10.48550/arXiv.1611.02779', '10.48550/arXiv.2107.06277', '10.48550/arXiv.2103.03874', '10.48550/arXiv.1905.06424', '10.48550/arXiv.2104.03113', '10.48550/arXiv.1612.03651', '10.48550/arXiv.1607.01759', '10.48550/arXiv.1312.6114', '10.48550/arXiv.2006.08785', '10.48550/arXiv.2410.05229', '10.48550/arXiv.2303.08774', '10.48550/arXiv.1910.00177', '10.48550/arXiv.1903.08254', '10.48550/arXiv.2403.05530', '10.48550/arXiv.1605.06065', '10.48550/arXiv.1803.01118']\n"
     ]
    }
   ],
   "source": [
    "reference_meatadata = ss.search_paper_by_ids(id_list=doi_arxiv_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reference_meatadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '638b08154fbb71fd34db2aae6cb40045577fe0de',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2303-09540',\n",
       "   'ArXiv': '2303.09540',\n",
       "   'DOI': '10.48550/arXiv.2303.09540',\n",
       "   'CorpusId': 257557221},\n",
       "  'corpusId': 257557221,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/638b08154fbb71fd34db2aae6cb40045577fe0de',\n",
       "  'title': 'SemDeDup: Data-efficient learning at web-scale through semantic deduplication',\n",
       "  'abstract': 'Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with less data.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 49,\n",
       "  'citationCount': 133,\n",
       "  'influentialCitationCount': 10,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2303.09540',\n",
       "   'status': 'CLOSED'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2023-03-16',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2303.09540'},\n",
       "  'citationStyles': {'bibtex': '@Article{Abbas2023SemDeDupDL,\\n author = {Amro Abbas and Kushal Tirumala and Daniel Simig and S. Ganguli and Ari S. Morcos},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {SemDeDup: Data-efficient learning at web-scale through semantic deduplication},\\n volume = {abs/2303.09540},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2176785233', 'name': 'Amro Abbas'},\n",
       "   {'authorId': '2551387', 'name': 'Kushal Tirumala'},\n",
       "   {'authorId': '2082239112', 'name': 'Daniel Simig'},\n",
       "   {'authorId': '25769960', 'name': 'S. Ganguli'},\n",
       "   {'authorId': '4690624', 'name': 'Ari S. Morcos'}]},\n",
       " {'paperId': 'e95bb39748a497fbeed1b221fb3d1296c2b1eec2',\n",
       "  'externalIds': {'ArXiv': '2402.16827',\n",
       "   'DBLP': 'journals/tmlr/AlbalakEXLL0MHP24',\n",
       "   'DOI': '10.48550/arXiv.2402.16827',\n",
       "   'CorpusId': 268032975},\n",
       "  'corpusId': 268032975,\n",
       "  'publicationVenue': None,\n",
       "  'url': 'https://www.semanticscholar.org/paper/e95bb39748a497fbeed1b221fb3d1296c2b1eec2',\n",
       "  'title': 'A Survey on Data Selection for Language Models',\n",
       "  'abstract': 'A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required. Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies. To narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches. By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers. Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research.',\n",
       "  'venue': 'Trans. Mach. Learn. Res.',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 0,\n",
       "  'citationCount': 88,\n",
       "  'influentialCitationCount': 7,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle', 'Review'],\n",
       "  'publicationDate': '2024-02-26',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2402.16827'},\n",
       "  'citationStyles': {'bibtex': '@Article{Albalak2024ASO,\\n author = {Alon Albalak and Yanai Elazar and Sang Michael Xie and Shayne Longpre and Nathan Lambert and Xinyi Wang and Niklas Muennighoff and Bairu Hou and Liangming Pan and Haewon Jeong and Colin Raffel and Shiyu Chang and Tatsunori Hashimoto and W. Wang},\\n booktitle = {Trans. Mach. Learn. Res.},\\n journal = {ArXiv},\\n title = {A Survey on Data Selection for Language Models},\\n volume = {abs/2402.16827},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2044198106', 'name': 'Alon Albalak'},\n",
       "   {'authorId': '51131518', 'name': 'Yanai Elazar'},\n",
       "   {'authorId': '2114080615', 'name': 'Sang Michael Xie'},\n",
       "   {'authorId': '2283848744', 'name': 'Shayne Longpre'},\n",
       "   {'authorId': '2052363815', 'name': 'Nathan Lambert'},\n",
       "   {'authorId': '2115553132', 'name': 'Xinyi Wang'},\n",
       "   {'authorId': '2037383772', 'name': 'Niklas Muennighoff'},\n",
       "   {'authorId': '1955614986', 'name': 'Bairu Hou'},\n",
       "   {'authorId': '2256983134', 'name': 'Liangming Pan'},\n",
       "   {'authorId': '2287920497', 'name': 'Haewon Jeong'},\n",
       "   {'authorId': '2269733851', 'name': 'Colin Raffel'},\n",
       "   {'authorId': '2191955409', 'name': 'Shiyu Chang'},\n",
       "   {'authorId': '2266400315', 'name': 'Tatsunori Hashimoto'},\n",
       "   {'authorId': '2257130314', 'name': 'W. Wang'}]},\n",
       " {'paperId': 'f403082b101821f5377ae82fe4ed7d02f6abd3ad',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2301-08028',\n",
       "   'ArXiv': '2301.08028',\n",
       "   'DOI': '10.48550/arXiv.2301.08028',\n",
       "   'CorpusId': 255999767},\n",
       "  'corpusId': 255999767,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/f403082b101821f5377ae82fe4ed7d02f6abd3ad',\n",
       "  'title': 'A Survey of Meta-Reinforcement Learning',\n",
       "  'abstract': 'While deep reinforcement learning (RL) has fueled multiple high-profile successes in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 278,\n",
       "  'citationCount': 101,\n",
       "  'influentialCitationCount': 2,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2301.08028',\n",
       "   'status': 'CLOSED'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle', 'Review'],\n",
       "  'publicationDate': '2023-01-19',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2301.08028'},\n",
       "  'citationStyles': {'bibtex': '@Article{Beck2023ASO,\\n author = {Jacob Beck and Risto Vuorio and E. Liu and Zheng Xiong and L. Zintgraf and Chelsea Finn and Shimon Whiteson},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {A Survey of Meta-Reinforcement Learning},\\n volume = {abs/2301.08028},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2065113032', 'name': 'Jacob Beck'},\n",
       "   {'authorId': '50990874', 'name': 'Risto Vuorio'},\n",
       "   {'authorId': '7301007', 'name': 'E. Liu'},\n",
       "   {'authorId': '2061496038', 'name': 'Zheng Xiong'},\n",
       "   {'authorId': '3378188', 'name': 'L. Zintgraf'},\n",
       "   {'authorId': '46881670', 'name': 'Chelsea Finn'},\n",
       "   {'authorId': '1766767', 'name': 'Shimon Whiteson'}]},\n",
       " {'paperId': 'f66b6049946a10080d1f2a7ee6a40e5cca3ee6a0',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2407-21787',\n",
       "   'ArXiv': '2407.21787',\n",
       "   'DOI': '10.48550/arXiv.2407.21787',\n",
       "   'CorpusId': 271571035},\n",
       "  'corpusId': 271571035,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/f66b6049946a10080d1f2a7ee6a40e5cca3ee6a0',\n",
       "  'title': 'Large Language Monkeys: Scaling Inference Compute with Repeated Sampling',\n",
       "  'abstract': 'Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 58,\n",
       "  'citationCount': 116,\n",
       "  'influentialCitationCount': 5,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'},\n",
       "   {'category': 'Linguistics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-07-31',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2407.21787'},\n",
       "  'citationStyles': {'bibtex': \"@Article{Brown2024LargeLM,\\n author = {Bradley Brown and Jordan Juravsky and Ryan Ehrlich and Ronald Clark and Quoc V. Le and Christopher R'e and Azalia Mirhoseini},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Large Language Monkeys: Scaling Inference Compute with Repeated Sampling},\\n volume = {abs/2407.21787},\\n year = {2024}\\n}\\n\"},\n",
       "  'authors': [{'authorId': '2283198901', 'name': 'Bradley Brown'},\n",
       "   {'authorId': '50875781', 'name': 'Jordan Juravsky'},\n",
       "   {'authorId': '2283134957', 'name': 'Ryan Ehrlich'},\n",
       "   {'authorId': '2313919316', 'name': 'Ronald Clark'},\n",
       "   {'authorId': '2151097303', 'name': 'Quoc V. Le'},\n",
       "   {'authorId': '2313917068', 'name': \"Christopher R'e\"},\n",
       "   {'authorId': '1861312', 'name': 'Azalia Mirhoseini'}]},\n",
       " {'paperId': '84c149537dee8ef3a7ff54b6bb01e9a4c8eaa17c',\n",
       "  'externalIds': {'ArXiv': '2412.21187',\n",
       "   'DBLP': 'journals/corr/abs-2412-21187',\n",
       "   'DOI': '10.48550/arXiv.2412.21187',\n",
       "   'CorpusId': 275133600},\n",
       "  'corpusId': 275133600,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/84c149537dee8ef3a7ff54b6bb01e9a4c8eaa17c',\n",
       "  'title': 'Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs',\n",
       "  'abstract': 'The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 52,\n",
       "  'citationCount': 22,\n",
       "  'influentialCitationCount': 2,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-12-30',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2412.21187'},\n",
       "  'citationStyles': {'bibtex': '@Article{Chen2024DoNT,\\n author = {Xingyu Chen and Jiahao Xu and Tian Liang and Zhiwei He and Jianhui Pang and Dian Yu and Linfeng Song and Qiuzhi Liu and Mengfei Zhou and Zhuosheng Zhang and Rui Wang and Zhaopeng Tu and Haitao Mi and Dong Yu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs},\\n volume = {abs/2412.21187},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2261360871', 'name': 'Xingyu Chen'},\n",
       "   {'authorId': '2311310481', 'name': 'Jiahao Xu'},\n",
       "   {'authorId': '2336871884', 'name': 'Tian Liang'},\n",
       "   {'authorId': '2280369306', 'name': 'Zhiwei He'},\n",
       "   {'authorId': '2337797310', 'name': 'Jianhui Pang'},\n",
       "   {'authorId': '2273685776', 'name': 'Dian Yu'},\n",
       "   {'authorId': '2273672063', 'name': 'Linfeng Song'},\n",
       "   {'authorId': '2324835420', 'name': 'Qiuzhi Liu'},\n",
       "   {'authorId': '2338027059', 'name': 'Mengfei Zhou'},\n",
       "   {'authorId': '2279826813', 'name': 'Zhuosheng Zhang'},\n",
       "   {'authorId': '2279803148', 'name': 'Rui Wang'},\n",
       "   {'authorId': '2332542850', 'name': 'Zhaopeng Tu'},\n",
       "   {'authorId': '2238955130', 'name': 'Haitao Mi'},\n",
       "   {'authorId': '2239076081', 'name': 'Dong Yu'}]},\n",
       " {'paperId': '40e8af970329135ec95057d73e239dab805ad128',\n",
       "  'externalIds': {'ArXiv': '2407.21783',\n",
       "   'DBLP': 'journals/corr/abs-2407-21783',\n",
       "   'DOI': '10.48550/arXiv.2407.21783',\n",
       "   'CorpusId': 271571434},\n",
       "  'corpusId': 271571434,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128',\n",
       "  'title': 'The Llama 3 Herd of Models',\n",
       "  'abstract': 'Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 0,\n",
       "  'citationCount': 2448,\n",
       "  'influentialCitationCount': 519,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'},\n",
       "   {'category': 'Linguistics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-07-31',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2407.21783'},\n",
       "  'citationStyles': {'bibtex': \"@Article{Dubey2024TheL3,\\n author = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony S. Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurélien Rodriguez and Austen Gregerson and Ava Spataru and Bap-tiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and C. Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Cantón Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab A. AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Frank Zhang and Gabriele Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Graeme Nail and Grégoire Mialon and Guanglong Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel M. Kloumann and Ishan Misra and Ivan Evtimov and Jade Copet and Jaewon Lee and J. Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and J. V. D. Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Ju-Qing Jia and Kalyan Vasuden Alwala and K. Upasani and Kate Plawiak and Keqian Li and K. Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuen-ley Chiu and Kunal Bhalla and Lauren Rantala-Yeary and L. Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and M. Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Mathew Oldham and Mathieu Rita and Maya Pavlova and M. Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri S. Chatterji and Olivier Duchenne and Onur cCelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasić and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohit Girdhar and Rohit Patel and R. Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and S. Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and S. Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and S. Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whit-ney Meers and Xavier Martinet and Xiaodong Wang and Xiaoqing Ellen Tan and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yiqian Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zhengxu Yan and Zhengxing Chen and Zoe Papakipos and Aaditya K. Singh and Aaron Grattafiori and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adi Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alex Vaughan and Alexei Baevski and Allie Feinstein and A. Kallet and Amit Sangani and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Franco and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Ben Leonhardi and Po-Yao (Bernie) Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Damon Civin and Dana Beaty and Daniel Kreymer and Shang-Wen Li and Danny Wyatt and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Firat Ozgenel and Francesco Caggioni and Francisco Guzm'an and Frank J. Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and G. Thattai and Grant Herman and G. Sizov and Guangyi Zhang and Guna Lakshminarayanan and Hamid Shojanazeri and Han Zou and Hannah Wang and Han Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Igor Molybog and Igor Tufanov and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kaixing(Kai) Wu and U. KamHou and Karan Saxena and Karthik Prasad and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and K. Veeraraghavan and Kelly Michelena and Keqian Li and Kun Huang and Kunal Chawla and Kushal Lakhotia and Kyle Huang and Lailin Chen and Lakshya Garg and A. Lavender and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and M. Tsimpoukelli and Martynas Mankus and Matan Hasson and M. Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and M. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikolay Pavlovich Laptev and Ning Dong and Ning Zhang and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollár and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Rohan Maheswari and Russ Howes and Ruty Rinott and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Sheng Feng and Shenghao Lin and S. Zha and Shiva Shankar and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and S. Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Sung-Bae Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and T. Remez and Tamar Glaser and Tamara Best and Thilo Kohler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and V. Poenaru and Vlad T. Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xia Tang and Xiaofang Wang and Xiaojian Wu and Xiaolan Wang and Xide Xia and Xilun Wu and Xinbo Gao and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu Wang and Yuchen Hao and Yundi Qian and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The Llama 3 Herd of Models},\\n volume = {abs/2407.21783},\\n year = {2024}\\n}\\n\"},\n",
       "  'authors': [{'authorId': '2479521', 'name': 'Abhimanyu Dubey'},\n",
       "   {'authorId': '2369482', 'name': 'Abhinav Jauhri'},\n",
       "   {'authorId': '2299944289', 'name': 'Abhinav Pandey'},\n",
       "   {'authorId': '89942851', 'name': 'Abhishek Kadian'},\n",
       "   {'authorId': '2313916217', 'name': 'Ahmad Al-Dahle'},\n",
       "   {'authorId': '2313924937', 'name': 'Aiesha Letman'},\n",
       "   {'authorId': '2313975817', 'name': 'Akhil Mathur'},\n",
       "   {'authorId': '14279694', 'name': 'Alan Schelten'},\n",
       "   {'authorId': '2329138320', 'name': 'Amy Yang'},\n",
       "   {'authorId': '2247818297', 'name': 'Angela Fan'},\n",
       "   {'authorId': '2313918197', 'name': 'Anirudh Goyal'},\n",
       "   {'authorId': '2129047988', 'name': 'Anthony S. Hartshorn'},\n",
       "   {'authorId': '2269467670', 'name': 'Aobo Yang'},\n",
       "   {'authorId': '2313926281', 'name': 'Archi Mitra'},\n",
       "   {'authorId': '2313918952', 'name': 'Archie Sravankumar'},\n",
       "   {'authorId': '2294453195', 'name': 'Artem Korenev'},\n",
       "   {'authorId': '2279336258', 'name': 'Arthur Hinsvark'},\n",
       "   {'authorId': '2314521400', 'name': 'Arun Rao'},\n",
       "   {'authorId': '2313922587', 'name': 'Aston Zhang'},\n",
       "   {'authorId': '2166043087', 'name': 'Aurélien Rodriguez'},\n",
       "   {'authorId': '2313910288', 'name': 'Austen Gregerson'},\n",
       "   {'authorId': '2295667288', 'name': 'Ava Spataru'},\n",
       "   {'authorId': '2313953240', 'name': 'Bap-tiste Roziere'},\n",
       "   {'authorId': '2313916233', 'name': 'Bethany Biron'},\n",
       "   {'authorId': '2237987675', 'name': 'Binh Tang'},\n",
       "   {'authorId': '2079950350', 'name': 'Bobbie Chern'},\n",
       "   {'authorId': '83928755', 'name': 'C. Caucheteux'},\n",
       "   {'authorId': '2313917653', 'name': 'Chaya Nayak'},\n",
       "   {'authorId': '2313909658', 'name': 'Chloe Bi'},\n",
       "   {'authorId': '2313913576', 'name': 'Chris Marra'},\n",
       "   {'authorId': '2217959550', 'name': 'Chris McConnell'},\n",
       "   {'authorId': '2313909741', 'name': 'Christian Keller'},\n",
       "   {'authorId': '103277778', 'name': 'Christophe Touret'},\n",
       "   {'authorId': '2268428822', 'name': 'Chunyang Wu'},\n",
       "   {'authorId': '2273700455', 'name': 'Corinne Wong'},\n",
       "   {'authorId': '66286536', 'name': 'Cristian Cantón Ferrer'},\n",
       "   {'authorId': '2273414632', 'name': 'Cyrus Nikolaidis'},\n",
       "   {'authorId': '51882206', 'name': 'Damien Allonsius'},\n",
       "   {'authorId': '2273006690', 'name': 'Daniel Song'},\n",
       "   {'authorId': '2313909437', 'name': 'Danielle Pintz'},\n",
       "   {'authorId': '2313918299', 'name': 'Danny Livshits'},\n",
       "   {'authorId': '71039937', 'name': 'David Esiobu'},\n",
       "   {'authorId': '2303390957', 'name': 'Dhruv Choudhary'},\n",
       "   {'authorId': '2267338678', 'name': 'Dhruv Mahajan'},\n",
       "   {'authorId': '2269456985', 'name': 'Diego Garcia-Olano'},\n",
       "   {'authorId': '2306842160', 'name': 'Diego Perino'},\n",
       "   {'authorId': '3449411', 'name': 'Dieuwke Hupkes'},\n",
       "   {'authorId': '2343773325', 'name': 'Egor Lakomkin'},\n",
       "   {'authorId': '1394834533', 'name': 'Ehab A. AlBadawy'},\n",
       "   {'authorId': '2313918680', 'name': 'Elina Lobanova'},\n",
       "   {'authorId': '31461304', 'name': 'Emily Dinan'},\n",
       "   {'authorId': '2268821751', 'name': 'Eric Michael Smith'},\n",
       "   {'authorId': '2708577', 'name': 'Filip Radenovic'},\n",
       "   {'authorId': '2313967211', 'name': 'Frank Zhang'},\n",
       "   {'authorId': '2282469774', 'name': 'Gabriele Synnaeve'},\n",
       "   {'authorId': '2314074302', 'name': 'Gabrielle Lee'},\n",
       "   {'authorId': '2313919767', 'name': 'Georgia Lewis Anderson'},\n",
       "   {'authorId': '2268397654', 'name': 'Graeme Nail'},\n",
       "   {'authorId': '51888120', 'name': 'Grégoire Mialon'},\n",
       "   {'authorId': '2264339927', 'name': 'Guanglong Pang'},\n",
       "   {'authorId': '2313924900', 'name': 'Guillem Cucurell'},\n",
       "   {'authorId': '2314075528', 'name': 'Hailey Nguyen'},\n",
       "   {'authorId': '103405110', 'name': 'Hannah Korevaar'},\n",
       "   {'authorId': '2314125186', 'name': 'Hu Xu'},\n",
       "   {'authorId': '2290402489', 'name': 'Hugo Touvron'},\n",
       "   {'authorId': '121929334', 'name': 'Iliyan Zarov'},\n",
       "   {'authorId': '34921162', 'name': 'Imanol Arrieta Ibarra'},\n",
       "   {'authorId': '2207049', 'name': 'Isabel M. Kloumann'},\n",
       "   {'authorId': '2267241285', 'name': 'Ishan Misra'},\n",
       "   {'authorId': '2264288587', 'name': 'Ivan Evtimov'},\n",
       "   {'authorId': '1805998294', 'name': 'Jade Copet'},\n",
       "   {'authorId': '2314056661', 'name': 'Jaewon Lee'},\n",
       "   {'authorId': '50825669', 'name': 'J. Geffert'},\n",
       "   {'authorId': '2313917660', 'name': 'Jana Vranes'},\n",
       "   {'authorId': '2314078634', 'name': 'Jason Park'},\n",
       "   {'authorId': '3222225', 'name': 'Jay Mahadeokar'},\n",
       "   {'authorId': '2313919733', 'name': 'Jeet Shah'},\n",
       "   {'authorId': '35721567', 'name': 'J. V. D. Linde'},\n",
       "   {'authorId': '2313909388', 'name': 'Jennifer Billock'},\n",
       "   {'authorId': '2287049560', 'name': 'Jenny Hong'},\n",
       "   {'authorId': '2223749565', 'name': 'Jenya Lee'},\n",
       "   {'authorId': '2223974989', 'name': 'Jeremy Fu'},\n",
       "   {'authorId': '31357678', 'name': 'Jianfeng Chi'},\n",
       "   {'authorId': '2314428565', 'name': 'Jianyu Huang'},\n",
       "   {'authorId': '2314080357', 'name': 'Jiawen Liu'},\n",
       "   {'authorId': '2314602001', 'name': 'Jie Wang'},\n",
       "   {'authorId': '2314078877', 'name': 'Jiecao Yu'},\n",
       "   {'authorId': '1749686057', 'name': 'Joanna Bitton'},\n",
       "   {'authorId': '90591458', 'name': 'Joe Spisak'},\n",
       "   {'authorId': '2149161568', 'name': 'Jongsoo Park'},\n",
       "   {'authorId': '2313925205', 'name': 'Joseph Rocca'},\n",
       "   {'authorId': '2313912873', 'name': 'Joshua Johnstun'},\n",
       "   {'authorId': '2273413914', 'name': 'Joshua Saxe'},\n",
       "   {'authorId': '2211671694', 'name': 'Ju-Qing Jia'},\n",
       "   {'authorId': '2313918589', 'name': 'Kalyan Vasuden Alwala'},\n",
       "   {'authorId': '17097160', 'name': 'K. Upasani'},\n",
       "   {'authorId': '2313918427', 'name': 'Kate Plawiak'},\n",
       "   {'authorId': '2313920868', 'name': 'Keqian Li'},\n",
       "   {'authorId': '2285859430', 'name': 'K. Heafield'},\n",
       "   {'authorId': '2282542714', 'name': 'Kevin Stone'},\n",
       "   {'authorId': '1405642252', 'name': 'Khalid El-Arini'},\n",
       "   {'authorId': '2273645788', 'name': 'Krithika Iyer'},\n",
       "   {'authorId': '2279924280', 'name': 'Kshitiz Malik'},\n",
       "   {'authorId': '2313925316', 'name': 'Kuen-ley Chiu'},\n",
       "   {'authorId': '2313913532', 'name': 'Kunal Bhalla'},\n",
       "   {'authorId': '2313915935', 'name': 'Lauren Rantala-Yeary'},\n",
       "   {'authorId': '1803520', 'name': 'L. Maaten'},\n",
       "   {'authorId': '2314080073', 'name': 'Lawrence Chen'},\n",
       "   {'authorId': '2313924605', 'name': 'Liang Tan'},\n",
       "   {'authorId': '2313918409', 'name': 'Liz Jenkins'},\n",
       "   {'authorId': '2249724552', 'name': 'Louis Martin'},\n",
       "   {'authorId': '151093281', 'name': 'Lovish Madaan'},\n",
       "   {'authorId': '2313912794', 'name': 'Lubo Malo'},\n",
       "   {'authorId': '2040305955', 'name': 'Lukas Blecher'},\n",
       "   {'authorId': '2313925619', 'name': 'Lukas Landzaat'},\n",
       "   {'authorId': '2314194315', 'name': 'Luke de Oliveira'},\n",
       "   {'authorId': '2000839712', 'name': 'Madeline Muzzi'},\n",
       "   {'authorId': '2047114741', 'name': 'M. Pasupuleti'},\n",
       "   {'authorId': '152964870', 'name': 'Mannat Singh'},\n",
       "   {'authorId': '2210374', 'name': 'Manohar Paluri'},\n",
       "   {'authorId': '2059886128', 'name': 'Marcin Kardas'},\n",
       "   {'authorId': '2313909379', 'name': 'Mathew Oldham'},\n",
       "   {'authorId': '2313912870', 'name': 'Mathieu Rita'},\n",
       "   {'authorId': '2313905186', 'name': 'Maya Pavlova'},\n",
       "   {'authorId': '2165660870', 'name': 'M. Kambadur'},\n",
       "   {'authorId': '2247796743', 'name': 'Mike Lewis'},\n",
       "   {'authorId': '2310234768', 'name': 'Min Si'},\n",
       "   {'authorId': '2247874378', 'name': 'Mitesh Kumar Singh'},\n",
       "   {'authorId': '2314434867', 'name': 'Mona Hassan'},\n",
       "   {'authorId': '39589154', 'name': 'Naman Goyal'},\n",
       "   {'authorId': '2911626', 'name': 'Narjes Torabi'},\n",
       "   {'authorId': '2223756247', 'name': 'Nikolay Bashlykov'},\n",
       "   {'authorId': '3444222', 'name': 'Nikolay Bogoychev'},\n",
       "   {'authorId': '22193324', 'name': 'Niladri S. Chatterji'},\n",
       "   {'authorId': '2096643450', 'name': 'Olivier Duchenne'},\n",
       "   {'authorId': '2166310112', 'name': 'Onur cCelebi'},\n",
       "   {'authorId': '2037772368', 'name': 'Patrick Alrassy'},\n",
       "   {'authorId': '2257643167', 'name': 'Pengchuan Zhang'},\n",
       "   {'authorId': '2273574279', 'name': 'Pengwei Li'},\n",
       "   {'authorId': '2268221163', 'name': 'Petar Vasić'},\n",
       "   {'authorId': '2313915931', 'name': 'Peter Weng'},\n",
       "   {'authorId': '51229603', 'name': 'Prajjwal Bhargava'},\n",
       "   {'authorId': '46175439', 'name': 'Pratik Dubal'},\n",
       "   {'authorId': '2304450089', 'name': 'Praveen Krishnan'},\n",
       "   {'authorId': '2146367061', 'name': 'Punit Singh Koura'},\n",
       "   {'authorId': '2214843767', 'name': 'Puxin Xu'},\n",
       "   {'authorId': '2314186827', 'name': 'Qing He'},\n",
       "   {'authorId': '2313912601', 'name': 'Qingxiao Dong'},\n",
       "   {'authorId': '2313910187', 'name': 'Ragavan Srinivasan'},\n",
       "   {'authorId': '2313925467', 'name': 'Raj Ganapathy'},\n",
       "   {'authorId': '98804036', 'name': 'Ramon Calderer'},\n",
       "   {'authorId': '2313909428', 'name': 'Ricardo Silveira Cabral'},\n",
       "   {'authorId': '1962768', 'name': 'Robert Stojnic'},\n",
       "   {'authorId': '48647153', 'name': 'Roberta Raileanu'},\n",
       "   {'authorId': '3102850', 'name': 'Rohit Girdhar'},\n",
       "   {'authorId': '2313913363', 'name': 'Rohit Patel'},\n",
       "   {'authorId': '2007285239', 'name': 'R. Sauvestre'},\n",
       "   {'authorId': '2313925210', 'name': 'Ronnie Polidoro'},\n",
       "   {'authorId': '1722889', 'name': 'Roshan Sumbaly'},\n",
       "   {'authorId': '2110697298', 'name': 'Ross Taylor'},\n",
       "   {'authorId': '2214818043', 'name': 'Ruan Silva'},\n",
       "   {'authorId': '2266467782', 'name': 'Rui Hou'},\n",
       "   {'authorId': '2248766592', 'name': 'Rui Wang'},\n",
       "   {'authorId': '2268759462', 'name': 'S. Hosseini'},\n",
       "   {'authorId': '2273416143', 'name': 'Sahana Chennabasappa'},\n",
       "   {'authorId': '2313985129', 'name': 'Sanjay Singh'},\n",
       "   {'authorId': '2277511475', 'name': 'Sean Bell'},\n",
       "   {'authorId': '2281792543', 'name': 'Seohyun Sonia Kim'},\n",
       "   {'authorId': '2068070', 'name': 'Sergey Edunov'},\n",
       "   {'authorId': '35557488', 'name': 'Shaoliang Nie'},\n",
       "   {'authorId': '46617804', 'name': 'Sharan Narang'},\n",
       "   {'authorId': '1498636613', 'name': 'S. Raparthy'},\n",
       "   {'authorId': '2191455', 'name': 'Sheng Shen'},\n",
       "   {'authorId': '2272846244', 'name': 'Shengye Wan'},\n",
       "   {'authorId': '2116473', 'name': 'Shruti Bhosale'},\n",
       "   {'authorId': '2314071119', 'name': 'Shun Zhang'},\n",
       "   {'authorId': '83754395', 'name': 'Simon Vandenhende'},\n",
       "   {'authorId': '47505161', 'name': 'Soumya Batra'},\n",
       "   {'authorId': '2273415395', 'name': 'Spencer Whitman'},\n",
       "   {'authorId': '31460313', 'name': 'Sten Sootla'},\n",
       "   {'authorId': '2313909594', 'name': 'Stephane Collot'},\n",
       "   {'authorId': '40895369', 'name': 'Suchin Gururangan'},\n",
       "   {'authorId': '148016419', 'name': 'S. Borodinsky'},\n",
       "   {'authorId': '2313925454', 'name': 'Tamar Herman'},\n",
       "   {'authorId': '2313918585', 'name': 'Tara Fowler'},\n",
       "   {'authorId': '2313917558', 'name': 'Tarek Sheasha'},\n",
       "   {'authorId': '2313910328', 'name': 'Thomas Georgiou'},\n",
       "   {'authorId': '2073456043', 'name': 'Thomas Scialom'},\n",
       "   {'authorId': '2313915815', 'name': 'Tobias Speckbacher'},\n",
       "   {'authorId': '39980906', 'name': 'Todor Mihaylov'},\n",
       "   {'authorId': '2313914277', 'name': 'Tong Xiao'},\n",
       "   {'authorId': '46907106', 'name': 'Ujjwal Karn'},\n",
       "   {'authorId': '28554843', 'name': 'Vedanuj Goswami'},\n",
       "   {'authorId': '2314332514', 'name': 'Vibhor Gupta'},\n",
       "   {'authorId': '34066479', 'name': 'Vignesh Ramanathan'},\n",
       "   {'authorId': '2190957318', 'name': 'Viktor Kerkez'},\n",
       "   {'authorId': '2313913380', 'name': 'Vincent Gonguet'},\n",
       "   {'authorId': '2313918349', 'name': 'Virginie Do'},\n",
       "   {'authorId': '2232955561', 'name': 'Vish Vogeti'},\n",
       "   {'authorId': '2162195471', 'name': 'Vladan Petrovic'},\n",
       "   {'authorId': '2266751414', 'name': 'Weiwei Chu'},\n",
       "   {'authorId': '2290750668', 'name': 'Wenhan Xiong'},\n",
       "   {'authorId': '2223742000', 'name': 'Wenyin Fu'},\n",
       "   {'authorId': '2313913371', 'name': 'Whit-ney Meers'},\n",
       "   {'authorId': '1490887583', 'name': 'Xavier Martinet'},\n",
       "   {'authorId': '2297930724', 'name': 'Xiaodong Wang'},\n",
       "   {'authorId': '2249851858', 'name': 'Xiaoqing Ellen Tan'},\n",
       "   {'authorId': '2285798957', 'name': 'Xinfeng Xie'},\n",
       "   {'authorId': '2313910170', 'name': 'Xuchao Jia'},\n",
       "   {'authorId': '2314067352', 'name': 'Xuewei Wang'},\n",
       "   {'authorId': '1404341450', 'name': 'Yaelle Goldschlag'},\n",
       "   {'authorId': '2286511206', 'name': 'Yashesh Gaur'},\n",
       "   {'authorId': '2223764353', 'name': 'Yasmine Babaei'},\n",
       "   {'authorId': '148416622', 'name': 'Yiqian Wen'},\n",
       "   {'authorId': '2314381758', 'name': 'Yiwen Song'},\n",
       "   {'authorId': '2108473229', 'name': 'Yuchen Zhang'},\n",
       "   {'authorId': '2297839847', 'name': 'Yue Li'},\n",
       "   {'authorId': '2272672481', 'name': 'Yuning Mao'},\n",
       "   {'authorId': '2297187212', 'name': 'Zacharie Delpierre Coudert'},\n",
       "   {'authorId': '2293992938', 'name': 'Zhengxu Yan'},\n",
       "   {'authorId': '2266490735', 'name': 'Zhengxing Chen'},\n",
       "   {'authorId': '51149919', 'name': 'Zoe Papakipos'},\n",
       "   {'authorId': '2306863572', 'name': 'Aaditya K. Singh'},\n",
       "   {'authorId': '2233294011', 'name': 'Aaron Grattafiori'},\n",
       "   {'authorId': '2312019070', 'name': 'Abha Jain'},\n",
       "   {'authorId': '2313915967', 'name': 'Adam Kelsey'},\n",
       "   {'authorId': '116814432', 'name': 'Adam Shajnfeld'},\n",
       "   {'authorId': '2077604116', 'name': 'Adi Gangidi'},\n",
       "   {'authorId': '2313910256', 'name': 'Adolfo Victoria'},\n",
       "   {'authorId': '2313913221', 'name': 'Ahuva Goldstand'},\n",
       "   {'authorId': '2313925780', 'name': 'Ajay Menon'},\n",
       "   {'authorId': '2314067298', 'name': 'Ajay Sharma'},\n",
       "   {'authorId': '2313915843', 'name': 'Alex Boesenberg'},\n",
       "   {'authorId': '2313910116', 'name': 'Alex Vaughan'},\n",
       "   {'authorId': '14667698', 'name': 'Alexei Baevski'},\n",
       "   {'authorId': '2313918472', 'name': 'Allie Feinstein'},\n",
       "   {'authorId': '122882087', 'name': 'A. Kallet'},\n",
       "   {'authorId': '2313918666', 'name': 'Amit Sangani'},\n",
       "   {'authorId': '2313925473', 'name': 'Anam Yunus'},\n",
       "   {'authorId': '2266838640', 'name': 'Andrei Lupu'},\n",
       "   {'authorId': '2243192949', 'name': 'Andres Alvarado'},\n",
       "   {'authorId': '2313925570', 'name': 'Andrew Caples'},\n",
       "   {'authorId': '2313913152', 'name': 'Andrew Gu'},\n",
       "   {'authorId': '2313919243', 'name': 'Andrew Ho'},\n",
       "   {'authorId': '2282542314', 'name': 'Andrew Poulton'},\n",
       "   {'authorId': '2313909933', 'name': 'Andrew Ryan'},\n",
       "   {'authorId': '1453469113', 'name': 'Ankit Ramchandani'},\n",
       "   {'authorId': '2313918528', 'name': 'Annie Franco'},\n",
       "   {'authorId': '51912276', 'name': 'Aparajita Saraf'},\n",
       "   {'authorId': '2313917455', 'name': 'Arkabandhu Chowdhury'},\n",
       "   {'authorId': '2313925699', 'name': 'Ashley Gabriel'},\n",
       "   {'authorId': '2313909987', 'name': 'Ashwin Bharambe'},\n",
       "   {'authorId': '35198582', 'name': 'Assaf Eisenman'},\n",
       "   {'authorId': '2313915928', 'name': 'Azadeh Yazdan'},\n",
       "   {'authorId': '2313918606', 'name': 'Beau James'},\n",
       "   {'authorId': '2313913272', 'name': 'Ben Maurer'},\n",
       "   {'authorId': '2897362', 'name': 'Ben Leonhardi'},\n",
       "   {'authorId': '2319973', 'name': 'Po-Yao (Bernie) Huang'},\n",
       "   {'authorId': '2313918673', 'name': 'Beth Loyd'},\n",
       "   {'authorId': '2313909983', 'name': 'Beto De Paola'},\n",
       "   {'authorId': '8005713', 'name': 'Bhargavi Paranjape'},\n",
       "   {'authorId': '2314014642', 'name': 'Bing Liu'},\n",
       "   {'authorId': '2314069142', 'name': 'Bo Wu'},\n",
       "   {'authorId': '2313909094', 'name': 'Boyu Ni'},\n",
       "   {'authorId': '2313916282', 'name': 'Braden Hancock'},\n",
       "   {'authorId': '46240090', 'name': 'Bram Wasti'},\n",
       "   {'authorId': '2313918213', 'name': 'Brandon Spence'},\n",
       "   {'authorId': '2313918219', 'name': 'Brani Stojkovic'},\n",
       "   {'authorId': '2313925572', 'name': 'Brian Gamido'},\n",
       "   {'authorId': '2313917587', 'name': 'Britt Montalvo'},\n",
       "   {'authorId': '2313919256', 'name': 'Carl Parker'},\n",
       "   {'authorId': '2313913658', 'name': 'Carly Burton'},\n",
       "   {'authorId': '2313917281', 'name': 'Catalina Mejia'},\n",
       "   {'authorId': '2313925537', 'name': 'Changhan Wang'},\n",
       "   {'authorId': '2314071777', 'name': 'Changkyu Kim'},\n",
       "   {'authorId': '2314072280', 'name': 'Chao Zhou'},\n",
       "   {'authorId': '2313982652', 'name': 'Chester Hu'},\n",
       "   {'authorId': '2290129157', 'name': 'Ching-Hsiang Chu'},\n",
       "   {'authorId': '2263867885', 'name': 'Chris Cai'},\n",
       "   {'authorId': '2313925773', 'name': 'Chris Tindal'},\n",
       "   {'authorId': '2322150', 'name': 'Christoph Feichtenhofer'},\n",
       "   {'authorId': '50986776', 'name': 'Damon Civin'},\n",
       "   {'authorId': '2313913237', 'name': 'Dana Beaty'},\n",
       "   {'authorId': '3046707', 'name': 'Daniel Kreymer'},\n",
       "   {'authorId': '2530311', 'name': 'Shang-Wen Li'},\n",
       "   {'authorId': '2313916159', 'name': 'Danny Wyatt'},\n",
       "   {'authorId': '2161835643', 'name': 'David Adkins'},\n",
       "   {'authorId': '2313915190', 'name': 'David Xu'},\n",
       "   {'authorId': '2273657478', 'name': 'Davide Testuggine'},\n",
       "   {'authorId': '2311498203', 'name': 'Delia David'},\n",
       "   {'authorId': '2248278031', 'name': 'Devi Parikh'},\n",
       "   {'authorId': '2145259939', 'name': 'Diana Liskovich'},\n",
       "   {'authorId': '2313925798', 'name': 'Didem Foss'},\n",
       "   {'authorId': '2283843884', 'name': 'Dingkang Wang'},\n",
       "   {'authorId': '145267619', 'name': 'Duc Le'},\n",
       "   {'authorId': '2313913567', 'name': 'Dustin Holland'},\n",
       "   {'authorId': '2313916034', 'name': 'Edward Dowling'},\n",
       "   {'authorId': '2313916009', 'name': 'Eissa Jamil'},\n",
       "   {'authorId': '2313925401', 'name': 'Elaine Montgomery'},\n",
       "   {'authorId': '6072807', 'name': 'Eleonora Presani'},\n",
       "   {'authorId': '2313914699', 'name': 'Emily Hahn'},\n",
       "   {'authorId': '2313913986', 'name': 'Emily Wood'},\n",
       "   {'authorId': '2313913160', 'name': 'Erik Brinkman'},\n",
       "   {'authorId': '2064373270', 'name': 'Esteban Arcaute'},\n",
       "   {'authorId': '2313915853', 'name': 'Evan Dunbar'},\n",
       "   {'authorId': '2313918562', 'name': 'Evan Smothers'},\n",
       "   {'authorId': '2314197755', 'name': 'Fei Sun'},\n",
       "   {'authorId': '32653170', 'name': 'Felix Kreuk'},\n",
       "   {'authorId': '2313907929', 'name': 'Feng Tian'},\n",
       "   {'authorId': '2160885118', 'name': 'Firat Ozgenel'},\n",
       "   {'authorId': '31292058', 'name': 'Francesco Caggioni'},\n",
       "   {'authorId': '2061585840', 'name': \"Francisco Guzm'an\"},\n",
       "   {'authorId': '3360115', 'name': 'Frank J. Kanayet'},\n",
       "   {'authorId': '2243280567', 'name': 'Frank Seide'},\n",
       "   {'authorId': '2313913137', 'name': 'Gabriela Medina Florez'},\n",
       "   {'authorId': '2313925846', 'name': 'Gabriella Schwarz'},\n",
       "   {'authorId': '2313918570', 'name': 'Gada Badeer'},\n",
       "   {'authorId': '2313916006', 'name': 'Georgia Swee'},\n",
       "   {'authorId': '2313925677', 'name': 'Gil Halpern'},\n",
       "   {'authorId': '2028300167', 'name': 'G. Thattai'},\n",
       "   {'authorId': '2313918648', 'name': 'Grant Herman'},\n",
       "   {'authorId': '2266304177', 'name': 'G. Sizov'},\n",
       "   {'authorId': '47776500', 'name': 'Guangyi Zhang'},\n",
       "   {'authorId': '2289832027', 'name': 'Guna Lakshminarayanan'},\n",
       "   {'authorId': '2343773236', 'name': 'Hamid Shojanazeri'},\n",
       "   {'authorId': '2313908554', 'name': 'Han Zou'},\n",
       "   {'authorId': '2314725059', 'name': 'Hannah Wang'},\n",
       "   {'authorId': '2261827291', 'name': 'Han Zha'},\n",
       "   {'authorId': '30279076', 'name': 'Haroun Habeeb'},\n",
       "   {'authorId': '2313913215', 'name': 'Harrison Rudolph'},\n",
       "   {'authorId': '2297942583', 'name': 'Helen Suk'},\n",
       "   {'authorId': '87085411', 'name': 'Henry Aspegren'},\n",
       "   {'authorId': '2313910892', 'name': 'Hunter Goldman'},\n",
       "   {'authorId': '2322981055', 'name': 'Igor Molybog'},\n",
       "   {'authorId': '2032201719', 'name': 'Igor Tufanov'},\n",
       "   {'authorId': '2127473751', 'name': 'Irina-Elena Veliche'},\n",
       "   {'authorId': '2064713742', 'name': 'Itai Gat'},\n",
       "   {'authorId': '2313913125', 'name': 'Jake Weissman'},\n",
       "   {'authorId': '2313913133', 'name': 'James Geboski'},\n",
       "   {'authorId': '2313917982', 'name': 'James Kohli'},\n",
       "   {'authorId': '2313909751', 'name': 'Japhet Asher'},\n",
       "   {'authorId': '2131867883', 'name': 'Jean-Baptiste Gaya'},\n",
       "   {'authorId': '2313917933', 'name': 'Jeff Marcus'},\n",
       "   {'authorId': '2314079720', 'name': 'Jeff Tang'},\n",
       "   {'authorId': '2313924089', 'name': 'Jennifer Chan'},\n",
       "   {'authorId': '2313906372', 'name': 'Jenny Zhen'},\n",
       "   {'authorId': '39906022', 'name': 'Jeremy Reizenstein'},\n",
       "   {'authorId': '2313925697', 'name': 'Jeremy Teboul'},\n",
       "   {'authorId': '2314712137', 'name': 'Jessica Zhong'},\n",
       "   {'authorId': '2314696266', 'name': 'Jian Jin'},\n",
       "   {'authorId': '2314170063', 'name': 'Jingyi Yang'},\n",
       "   {'authorId': '2313921009', 'name': 'Joe Cummings'},\n",
       "   {'authorId': '2313916920', 'name': 'Jon Carvill'},\n",
       "   {'authorId': '2313918017', 'name': 'Jon Shepard'},\n",
       "   {'authorId': '2313925667', 'name': 'Jonathan McPhie'},\n",
       "   {'authorId': '2314554743', 'name': 'Jonathan Torres'},\n",
       "   {'authorId': '2313925786', 'name': 'Josh Ginsburg'},\n",
       "   {'authorId': '2314072216', 'name': 'Junjie Wang'},\n",
       "   {'authorId': '2115598555', 'name': 'Kaixing(Kai) Wu'},\n",
       "   {'authorId': '2313913073', 'name': 'U. KamHou'},\n",
       "   {'authorId': '2313917036', 'name': 'Karan Saxena'},\n",
       "   {'authorId': '2313913208', 'name': 'Karthik Prasad'},\n",
       "   {'authorId': '40267343', 'name': 'Kartikay Khandelwal'},\n",
       "   {'authorId': '2158995926', 'name': 'Katayoun Zand'},\n",
       "   {'authorId': '2313926373', 'name': 'Kathy Matosich'},\n",
       "   {'authorId': '2262920209', 'name': 'K. Veeraraghavan'},\n",
       "   {'authorId': '2313925800', 'name': 'Kelly Michelena'},\n",
       "   {'authorId': '2313920868', 'name': 'Keqian Li'},\n",
       "   {'authorId': '2314883034', 'name': 'Kun Huang'},\n",
       "   {'authorId': '2313918835', 'name': 'Kunal Chawla'},\n",
       "   {'authorId': '1410624139', 'name': 'Kushal Lakhotia'},\n",
       "   {'authorId': '2314883036', 'name': 'Kyle Huang'},\n",
       "   {'authorId': '2197533966', 'name': 'Lailin Chen'},\n",
       "   {'authorId': '2313913092', 'name': 'Lakshya Garg'},\n",
       "   {'authorId': '2313926370', 'name': 'A. Lavender'},\n",
       "   {'authorId': '2314073913', 'name': 'Leandro Silva'},\n",
       "   {'authorId': '2313926731', 'name': 'Lee Bell'},\n",
       "   {'authorId': '2313951052', 'name': 'Lei Zhang'},\n",
       "   {'authorId': '2314460078', 'name': 'Liangpeng Guo'},\n",
       "   {'authorId': '2269696579', 'name': 'Licheng Yu'},\n",
       "   {'authorId': '2313918301', 'name': 'Liron Moshkovich'},\n",
       "   {'authorId': '2331511165', 'name': 'Luca Wehrstedt'},\n",
       "   {'authorId': '2072010', 'name': 'Madian Khabsa'},\n",
       "   {'authorId': '2313918577', 'name': 'Manav Avalani'},\n",
       "   {'authorId': '2273002871', 'name': 'Manish Bhatt'},\n",
       "   {'authorId': '2010057', 'name': 'M. Tsimpoukelli'},\n",
       "   {'authorId': '98800979', 'name': 'Martynas Mankus'},\n",
       "   {'authorId': '2093466943', 'name': 'Matan Hasson'},\n",
       "   {'authorId': '83174287', 'name': 'M. Lennie'},\n",
       "   {'authorId': '2248340971', 'name': 'Matthias Reso'},\n",
       "   {'authorId': '2313918566', 'name': 'Maxim Groshev'},\n",
       "   {'authorId': '2290016941', 'name': 'Maxim Naumov'},\n",
       "   {'authorId': '52097509', 'name': 'Maya Lathi'},\n",
       "   {'authorId': '2313926376', 'name': 'Meghan Keneally'},\n",
       "   {'authorId': '1727524', 'name': 'M. Seltzer'},\n",
       "   {'authorId': '2259912893', 'name': 'Michal Valko'},\n",
       "   {'authorId': '2313917797', 'name': 'Michelle Restrepo'},\n",
       "   {'authorId': '2314105463', 'name': 'Mihir Patel'},\n",
       "   {'authorId': '2313909990', 'name': 'Mik Vyatskov'},\n",
       "   {'authorId': '49089678', 'name': 'Mikayel Samvelyan'},\n",
       "   {'authorId': '2314111844', 'name': 'Mike Clark'},\n",
       "   {'authorId': '2313910746', 'name': 'Mike Macey'},\n",
       "   {'authorId': '2314078208', 'name': 'Mike Wang'},\n",
       "   {'authorId': '147487949', 'name': 'Miquel Jubert Hermoso'},\n",
       "   {'authorId': '2313913313', 'name': 'Mo Metanat'},\n",
       "   {'authorId': '32371083', 'name': 'Mohammad Rastegari'},\n",
       "   {'authorId': '2313910172', 'name': 'Munish Bansal'},\n",
       "   {'authorId': '2265554054', 'name': 'Nandhini Santhanam'},\n",
       "   {'authorId': '2313916856', 'name': 'Natascha Parks'},\n",
       "   {'authorId': '2313910535', 'name': 'Natasha White'},\n",
       "   {'authorId': '2313918859', 'name': 'Navyata Bawa'},\n",
       "   {'authorId': '40943290', 'name': 'Nayan Singhal'},\n",
       "   {'authorId': '2313909893', 'name': 'Nick Egebo'},\n",
       "   {'authorId': '1746841', 'name': 'Nicolas Usunier'},\n",
       "   {'authorId': '2285597551', 'name': 'Nikolay Pavlovich Laptev'},\n",
       "   {'authorId': '2313910396', 'name': 'Ning Dong'},\n",
       "   {'authorId': '2314687456', 'name': 'Ning Zhang'},\n",
       "   {'authorId': '2313916655', 'name': 'Norman Cheng'},\n",
       "   {'authorId': '1405690366', 'name': 'Oleg Chernoguz'},\n",
       "   {'authorId': '2313918980', 'name': 'Olivia Hart'},\n",
       "   {'authorId': '1778909324', 'name': 'Omkar Salpekar'},\n",
       "   {'authorId': '1729960', 'name': 'Ozlem Kalinli'},\n",
       "   {'authorId': '2313916098', 'name': 'Parkin Kent'},\n",
       "   {'authorId': '2313909999', 'name': 'Parth Parekh'},\n",
       "   {'authorId': '2313915995', 'name': 'Paul Saab'},\n",
       "   {'authorId': '2307470796', 'name': 'Pavan Balaji'},\n",
       "   {'authorId': '31915793', 'name': 'Pedro Rittner'},\n",
       "   {'authorId': '14171685', 'name': 'Philip Bontrager'},\n",
       "   {'authorId': '2313919367', 'name': 'Pierre Roux'},\n",
       "   {'authorId': '2257254817', 'name': 'Piotr Dollár'},\n",
       "   {'authorId': '2163709899', 'name': 'Polina Zvyagina'},\n",
       "   {'authorId': '2459737', 'name': 'Prashant Ratanchandani'},\n",
       "   {'authorId': '41020300', 'name': 'Pritish Yuvraj'},\n",
       "   {'authorId': '2313916229', 'name': 'Qian Liang'},\n",
       "   {'authorId': '2313909804', 'name': 'Rachad Alao'},\n",
       "   {'authorId': '2313934481', 'name': 'Rachel Rodriguez'},\n",
       "   {'authorId': '14714641', 'name': 'Rafi Ayub'},\n",
       "   {'authorId': '2313909891', 'name': 'Raghotham Murthy'},\n",
       "   {'authorId': '2313918294', 'name': 'Raghu Nayani'},\n",
       "   {'authorId': '2264459586', 'name': 'Rahul Mitra'},\n",
       "   {'authorId': '2313919671', 'name': 'Raymond Li'},\n",
       "   {'authorId': '2313918488', 'name': 'Rebekkah Hogan'},\n",
       "   {'authorId': '2313913081', 'name': 'Robin Battey'},\n",
       "   {'authorId': '46886013', 'name': 'Rocky Wang'},\n",
       "   {'authorId': '2313918943', 'name': 'Rohan Maheswari'},\n",
       "   {'authorId': '1410913697', 'name': 'Russ Howes'},\n",
       "   {'authorId': '1905713', 'name': 'Ruty Rinott'},\n",
       "   {'authorId': '2313916859', 'name': 'Sai Jayesh Bondu'},\n",
       "   {'authorId': '19200118', 'name': 'Samyak Datta'},\n",
       "   {'authorId': '2313913104', 'name': 'Sara Chugh'},\n",
       "   {'authorId': '2313916525', 'name': 'Sara Hunt'},\n",
       "   {'authorId': '2313919311', 'name': 'Sargun Dhillon'},\n",
       "   {'authorId': '2313918976', 'name': 'Sasha Sidorov'},\n",
       "   {'authorId': '2314108295', 'name': 'Satadru Pan'},\n",
       "   {'authorId': '2314005184', 'name': 'Saurabh Verma'},\n",
       "   {'authorId': '2314406059', 'name': 'Seiji Yamamoto'},\n",
       "   {'authorId': '48347720', 'name': 'Sharadh Ramaswamy'},\n",
       "   {'authorId': '2313926214', 'name': 'Shaun Lindsay'},\n",
       "   {'authorId': '2314693028', 'name': 'Sheng Feng'},\n",
       "   {'authorId': '2311565327', 'name': 'Shenghao Lin'},\n",
       "   {'authorId': '2268757277', 'name': 'S. Zha'},\n",
       "   {'authorId': '2313916766', 'name': 'Shiva Shankar'},\n",
       "   {'authorId': '2237076180', 'name': 'Shuqiang Zhang'},\n",
       "   {'authorId': '2237101143', 'name': 'Sinong Wang'},\n",
       "   {'authorId': '50230355', 'name': 'Sneha Agarwal'},\n",
       "   {'authorId': '2423558', 'name': 'S. Sajuyigbe'},\n",
       "   {'authorId': '2127604', 'name': 'Soumith Chintala'},\n",
       "   {'authorId': '2313919129', 'name': 'Stephanie Max'},\n",
       "   {'authorId': '2125208891', 'name': 'Stephen Chen'},\n",
       "   {'authorId': '2313926357', 'name': 'Steve Kehoe'},\n",
       "   {'authorId': '2313909787', 'name': 'Steve Satterfield'},\n",
       "   {'authorId': '2313918283', 'name': 'Sudarshan Govindaprasad'},\n",
       "   {'authorId': '2157683980', 'name': 'Sumit Gupta'},\n",
       "   {'authorId': '2286537482', 'name': 'Sung-Bae Cho'},\n",
       "   {'authorId': '2313919117', 'name': 'Sunny Virk'},\n",
       "   {'authorId': '2313914940', 'name': 'Suraj Subramanian'},\n",
       "   {'authorId': '89754631', 'name': 'Sy Choudhury'},\n",
       "   {'authorId': '2313908725', 'name': 'Sydney Goldman'},\n",
       "   {'authorId': '2211633', 'name': 'T. Remez'},\n",
       "   {'authorId': '2071335303', 'name': 'Tamar Glaser'},\n",
       "   {'authorId': '2313910221', 'name': 'Tamara Best'},\n",
       "   {'authorId': '2189305275', 'name': 'Thilo Kohler'},\n",
       "   {'authorId': '2313919760', 'name': 'Thomas Robinson'},\n",
       "   {'authorId': '2314332791', 'name': 'Tianhe Li'},\n",
       "   {'authorId': '1993655237', 'name': 'Tianjun Zhang'},\n",
       "   {'authorId': '2313919132', 'name': 'Tim Matthews'},\n",
       "   {'authorId': '2313919289', 'name': 'Timothy Chou'},\n",
       "   {'authorId': '2313919174', 'name': 'Tzook Shaked'},\n",
       "   {'authorId': '2273415095', 'name': 'Varun Vontimitta'},\n",
       "   {'authorId': '2313918541', 'name': 'Victoria Ajayi'},\n",
       "   {'authorId': '2313910202', 'name': 'Victoria Montanez'},\n",
       "   {'authorId': '2313916470', 'name': 'Vijai Mohan'},\n",
       "   {'authorId': '2314056846', 'name': 'Vinay Satish Kumar'},\n",
       "   {'authorId': '71203676', 'name': 'Vishal Mangla'},\n",
       "   {'authorId': '2313685593', 'name': 'Vlad Ionescu'},\n",
       "   {'authorId': '144386035', 'name': 'V. Poenaru'},\n",
       "   {'authorId': '2051654054', 'name': 'Vlad T. Mihailescu'},\n",
       "   {'authorId': '2313920446', 'name': 'Vladimir Ivanov'},\n",
       "   {'authorId': '2293767405', 'name': 'Wei Li'},\n",
       "   {'authorId': '2314069334', 'name': 'Wenchen Wang'}]},\n",
       " {'paperId': 'f1a3cd5cc340f47e3e966709f7dfddef23460aa2',\n",
       "  'externalIds': {'ArXiv': '2305.19165',\n",
       "   'DBLP': 'journals/corr/abs-2305-19165',\n",
       "   'DOI': '10.48550/arXiv.2305.19165',\n",
       "   'CorpusId': 258968043},\n",
       "  'corpusId': 258968043,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/f1a3cd5cc340f47e3e966709f7dfddef23460aa2',\n",
       "  'title': 'Strategic Reasoning with Language Models',\n",
       "  'abstract': 'Strategic reasoning enables agents to cooperate, communicate, and compete with other agents in diverse situations. Existing approaches to solving strategic games rely on extensive training, yielding strategies that do not generalize to new scenarios or games without retraining. Large Language Models (LLMs), with their ability to comprehend and generate complex, context-rich language, could prove powerful as tools for strategic gameplay. This paper introduces an approach that uses pretrained LLMs with few-shot chain-of-thought examples to enable strategic reasoning for AI agents. Our approach uses systematically generated demonstrations of reasoning about states, values, and beliefs to prompt the model. Using extensive variations of simple matrix games, we show that strategies that are derived based on systematically generated prompts generalize almost perfectly to new game structures, alternate objectives, and hidden information. Additionally, we demonstrate our approach can lead to human-like negotiation strategies in realistic scenarios without any extra training or fine-tuning. Our results highlight the ability of LLMs, guided by systematic reasoning demonstrations, to adapt and excel in diverse strategic scenarios.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 51,\n",
       "  'citationCount': 30,\n",
       "  'influentialCitationCount': 3,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2305.19165',\n",
       "   'status': 'CLOSED'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2023-05-30',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2305.19165'},\n",
       "  'citationStyles': {'bibtex': '@Article{Gandhi2023StrategicRW,\\n author = {Kanishk Gandhi and Dorsa Sadigh and Noah D. Goodman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Strategic Reasoning with Language Models},\\n volume = {abs/2305.19165},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '144841994', 'name': 'Kanishk Gandhi'},\n",
       "   {'authorId': '1779671', 'name': 'Dorsa Sadigh'},\n",
       "   {'authorId': '144002017', 'name': 'Noah D. Goodman'}]},\n",
       " {'paperId': 'b2688b3a0ddf190cd99b11b6bf589a6e071c5369',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2404-03683',\n",
       "   'ArXiv': '2404.03683',\n",
       "   'DOI': '10.48550/arXiv.2404.03683',\n",
       "   'CorpusId': 268987503},\n",
       "  'corpusId': 268987503,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/b2688b3a0ddf190cd99b11b6bf589a6e071c5369',\n",
       "  'title': 'Stream of Search (SoS): Learning to Search in Language',\n",
       "  'abstract': 'Language models are rarely shown fruitful mistakes while training. They then struggle to look beyond the next token, suffering from a snowballing of errors and struggling to predict the consequence of their actions several steps ahead. In this paper, we show how language models can be taught to search by representing the process of search in language, as a flattened string -- a stream of search (SoS). We propose a unified language for search that captures an array of different symbolic search strategies. We demonstrate our approach using the simple yet difficult game of Countdown, where the goal is to combine input numbers with arithmetic operations to reach a target number. We pretrain a transformer-based language model from scratch on a dataset of streams of search generated by heuristic solvers. We find that SoS pretraining increases search accuracy by 25% over models trained to predict only the optimal search trajectory. We further finetune this model with two policy improvement methods: Advantage-Induced Policy Alignment (APA) and Self-Taught Reasoner (STaR). The finetuned SoS models solve 36% of previously unsolved problems, including problems that cannot be solved by any of the heuristic solvers. Our results indicate that language models can learn to solve problems via search, self-improve to flexibly use different search strategies, and potentially discover new ones.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 28,\n",
       "  'citationCount': 19,\n",
       "  'influentialCitationCount': 6,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'},\n",
       "   {'category': 'Linguistics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-04-01',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2404.03683'},\n",
       "  'citationStyles': {'bibtex': '@Article{Gandhi2024StreamOS,\\n author = {Kanishk Gandhi and Denise Lee and Gabriel Grand and Muxin Liu and Winson Cheng and Archit Sharma and Noah D. Goodman},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Stream of Search (SoS): Learning to Search in Language},\\n volume = {abs/2404.03683},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '144841994', 'name': 'Kanishk Gandhi'},\n",
       "   {'authorId': '2295586561', 'name': 'Denise Lee'},\n",
       "   {'authorId': '2295510687', 'name': 'Gabriel Grand'},\n",
       "   {'authorId': '2264511729', 'name': 'Muxin Liu'},\n",
       "   {'authorId': '2295592705', 'name': 'Winson Cheng'},\n",
       "   {'authorId': '2295593182', 'name': 'Archit Sharma'},\n",
       "   {'authorId': '2265069313', 'name': 'Noah D. Goodman'}]},\n",
       " {'paperId': 'dba4e0c1690650a73a4a4d1a3651a719df9986f5',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2410-07985',\n",
       "   'ArXiv': '2410.07985',\n",
       "   'DOI': '10.48550/arXiv.2410.07985',\n",
       "   'CorpusId': 273233571},\n",
       "  'corpusId': 273233571,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/dba4e0c1690650a73a4a4d1a3651a719df9986f5',\n",
       "  'title': 'Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models',\n",
       "  'abstract': \"Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. However, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8\\\\% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54\\\\% and 52.55\\\\% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 24,\n",
       "  'citationCount': 12,\n",
       "  'influentialCitationCount': 3,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'},\n",
       "   {'category': 'Mathematics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle', 'Review'],\n",
       "  'publicationDate': '2024-10-10',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2410.07985'},\n",
       "  'citationStyles': {'bibtex': '@Article{Gao2024OmniMATHAU,\\n author = {Bofei Gao and Feifan Song and Zhe Yang and Zefan Cai and Yibo Miao and Qingxiu Dong and Lei Li and Chenghao Ma and Liang Chen and Runxin Xu and Zhengyang Tang and Benyou Wang and Daoguang Zan and Shanghaoran Quan and Ge Zhang and Lei Sha and Yichang Zhang and Xuancheng Ren and Tianyu Liu and Baobao Chang},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models},\\n volume = {abs/2410.07985},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2186406832', 'name': 'Bofei Gao'},\n",
       "   {'authorId': '2325153259', 'name': 'Feifan Song'},\n",
       "   {'authorId': '2257389788', 'name': 'Zhe Yang'},\n",
       "   {'authorId': '2117632647', 'name': 'Zefan Cai'},\n",
       "   {'authorId': '2319604149', 'name': 'Yibo Miao'},\n",
       "   {'authorId': '2047143813', 'name': 'Qingxiu Dong'},\n",
       "   {'authorId': '49192881', 'name': 'Lei Li'},\n",
       "   {'authorId': '2325163484', 'name': 'Chenghao Ma'},\n",
       "   {'authorId': '2286955403', 'name': 'Liang Chen'},\n",
       "   {'authorId': '1748844142', 'name': 'Runxin Xu'},\n",
       "   {'authorId': '2284834012', 'name': 'Zhengyang Tang'},\n",
       "   {'authorId': '2284827140', 'name': 'Benyou Wang'},\n",
       "   {'authorId': '2134434187', 'name': 'Daoguang Zan'},\n",
       "   {'authorId': '2325151882', 'name': 'Shanghaoran Quan'},\n",
       "   {'authorId': '2319593518', 'name': 'Ge Zhang'},\n",
       "   {'authorId': '39058310', 'name': 'Lei Sha'},\n",
       "   {'authorId': '29343468', 'name': 'Yichang Zhang'},\n",
       "   {'authorId': '2312110505', 'name': 'Xuancheng Ren'},\n",
       "   {'authorId': '2254792825', 'name': 'Tianyu Liu'},\n",
       "   {'authorId': '2261083637', 'name': 'Baobao Chang'}]},\n",
       " {'paperId': '585e95a43f4ceb3b9fdd8408b7b0b5df468c1030',\n",
       "  'externalIds': {'ArXiv': '2410.02089',\n",
       "   'DBLP': 'journals/corr/abs-2410-02089',\n",
       "   'DOI': '10.48550/arXiv.2410.02089',\n",
       "   'CorpusId': 273098785},\n",
       "  'corpusId': 273098785,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/585e95a43f4ceb3b9fdd8408b7b0b5df468c1030',\n",
       "  'title': 'RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning',\n",
       "  'abstract': 'Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve the desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks, where we achieve new state-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 48,\n",
       "  'citationCount': 2,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-10-02',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2410.02089'},\n",
       "  'citationStyles': {'bibtex': '@Article{Gehring2024RLEFGC,\\n author = {Jonas Gehring and Kunhao Zheng and Jade Copet and Vegard Mella and Taco Cohen and Gabriele Synnaeve},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning},\\n volume = {abs/2410.02089},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2294361198', 'name': 'Jonas Gehring'},\n",
       "   {'authorId': '2324058509', 'name': 'Kunhao Zheng'},\n",
       "   {'authorId': '1805998294', 'name': 'Jade Copet'},\n",
       "   {'authorId': '51994279', 'name': 'Vegard Mella'},\n",
       "   {'authorId': '2324027946', 'name': 'Taco Cohen'},\n",
       "   {'authorId': '2282469774', 'name': 'Gabriele Synnaeve'}]},\n",
       " {'paperId': '5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35',\n",
       "  'externalIds': {'ArXiv': '2305.15717',\n",
       "   'DBLP': 'journals/corr/abs-2305-15717',\n",
       "   'DOI': '10.48550/arXiv.2305.15717',\n",
       "   'CorpusId': 258887629},\n",
       "  'corpusId': 258887629,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35',\n",
       "  'title': 'The False Promise of Imitating Proprietary LLMs',\n",
       "  'abstract': \"An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 47,\n",
       "  'citationCount': 188,\n",
       "  'influentialCitationCount': 10,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2305.15717',\n",
       "   'status': 'CLOSED'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2023-05-25',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2305.15717'},\n",
       "  'citationStyles': {'bibtex': '@Article{Gudibande2023TheFP,\\n author = {Arnav Gudibande and Eric Wallace and Charles Burton Snell and Xinyang Geng and Hao Liu and P. Abbeel and S. Levine and Dawn Song},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {The False Promise of Imitating Proprietary LLMs},\\n volume = {abs/2305.15717},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2218767764', 'name': 'Arnav Gudibande'},\n",
       "   {'authorId': '145217343', 'name': 'Eric Wallace'},\n",
       "   {'authorId': '117111135', 'name': 'Charles Burton Snell'},\n",
       "   {'authorId': '3468192', 'name': 'Xinyang Geng'},\n",
       "   {'authorId': '2143855835', 'name': 'Hao Liu'},\n",
       "   {'authorId': '1689992', 'name': 'P. Abbeel'},\n",
       "   {'authorId': '1736651', 'name': 'S. Levine'},\n",
       "   {'authorId': '2112739650', 'name': 'Dawn Song'}]},\n",
       " {'paperId': '5dbffedcabe3fa43060ebbe2b1789500edfd871f',\n",
       "  'externalIds': {'DBLP': 'conf/emnlp/HaoGMHWWH23',\n",
       "   'ArXiv': '2305.14992',\n",
       "   'DOI': '10.48550/arXiv.2305.14992',\n",
       "   'CorpusId': 258865812},\n",
       "  'corpusId': 258865812,\n",
       "  'publicationVenue': {'id': '41bf9ed3-85b3-4c90-b015-150e31690253',\n",
       "   'name': 'Conference on Empirical Methods in Natural Language Processing',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Empir Method Nat Lang Process',\n",
       "    'Empirical Methods in Natural Language Processing',\n",
       "    'Conf Empir Method Nat Lang Process',\n",
       "    'EMNLP'],\n",
       "   'url': 'https://www.aclweb.org/portal/emnlp'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f',\n",
       "  'title': 'Reasoning with Language Model is Planning with World Model',\n",
       "  'abstract': 'Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\\\textit{world model}$ to predict the world $\\\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\\\underline{R}$easoning vi$\\\\underline{a}$ $\\\\underline{P}$lanning $\\\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.',\n",
       "  'venue': 'Conference on Empirical Methods in Natural Language Processing',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 94,\n",
       "  'citationCount': 397,\n",
       "  'influentialCitationCount': 53,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2305.14992',\n",
       "   'status': 'CLOSED'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle', 'Conference'],\n",
       "  'publicationDate': '2023-05-24',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2305.14992'},\n",
       "  'citationStyles': {'bibtex': '@Article{Hao2023ReasoningWL,\\n author = {Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and D. Wang and Zhiting Hu},\\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\\n journal = {ArXiv},\\n title = {Reasoning with Language Model is Planning with World Model},\\n volume = {abs/2305.14992},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2128965713', 'name': 'Shibo Hao'},\n",
       "   {'authorId': '2112578816', 'name': 'Yi Gu'},\n",
       "   {'authorId': '2110816708', 'name': 'Haodi Ma'},\n",
       "   {'authorId': '2218162745', 'name': 'Joshua Jiahua Hong'},\n",
       "   {'authorId': '47197370', 'name': 'Zhen Wang'},\n",
       "   {'authorId': '2111220343', 'name': 'D. Wang'},\n",
       "   {'authorId': '2749311', 'name': 'Zhiting Hu'}]},\n",
       " {'paperId': 'c78350e81298ca87bc1d59b466fa40081232caaa',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2403-04642',\n",
       "   'ArXiv': '2403.04642',\n",
       "   'DOI': '10.48550/arXiv.2403.04642',\n",
       "   'CorpusId': 268264399},\n",
       "  'corpusId': 268264399,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/c78350e81298ca87bc1d59b466fa40081232caaa',\n",
       "  'title': 'Teaching Large Language Models to Reason with Reinforcement Learning',\n",
       "  'abstract': 'Reinforcement Learning from Human Feedback (\\\\textbf{RLHF}) has emerged as a dominant approach for aligning LLM outputs with human preferences. Inspired by the success of RLHF, we study the performance of multiple algorithms that learn from feedback (Expert Iteration, Proximal Policy Optimization (\\\\textbf{PPO}), Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate both sparse and dense rewards provided to the LLM both heuristically and via a learned reward model. We additionally start from multiple model sizes and initializations both with and without supervised fine-tuning (\\\\textbf{SFT}) data. Overall, we find all algorithms perform comparably, with Expert Iteration performing best in most cases. Surprisingly, we find the sample complexity of Expert Iteration is similar to that of PPO, requiring at most on the order of $10^6$ samples to converge from a pretrained checkpoint. We investigate why this is the case, concluding that during RL training models fail to explore significantly beyond solutions already produced by SFT models. Additionally, we discuss a trade off between maj@1 and pass@96 metric performance during SFT training and how conversely RL training improves both simultaneously. We then conclude by discussing the implications of our findings for RLHF and the future role of RL in LLM fine-tuning.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 69,\n",
       "  'citationCount': 48,\n",
       "  'influentialCitationCount': 2,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'},\n",
       "   {'category': 'Linguistics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-03-07',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2403.04642'},\n",
       "  'citationStyles': {'bibtex': '@Article{Havrilla2024TeachingLL,\\n author = {Alex Havrilla and Yuqing Du and S. Raparthy and Christoforos Nalmpantis and Jane Dwivedi-Yu and Maksym Zhuravinskyi and Eric Hambro and Sainbayar Sukhbaatar and Roberta Raileanu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Teaching Large Language Models to Reason with Reinforcement Learning},\\n volume = {abs/2403.04642},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2279337437', 'name': 'Alex Havrilla'},\n",
       "   {'authorId': '2290246964', 'name': 'Yuqing Du'},\n",
       "   {'authorId': '1498636613', 'name': 'S. Raparthy'},\n",
       "   {'authorId': '31434304', 'name': 'Christoforos Nalmpantis'},\n",
       "   {'authorId': '2173509991', 'name': 'Jane Dwivedi-Yu'},\n",
       "   {'authorId': '2273535109', 'name': 'Maksym Zhuravinskyi'},\n",
       "   {'authorId': '2072738644', 'name': 'Eric Hambro'},\n",
       "   {'authorId': '2265067', 'name': 'Sainbayar Sukhbaatar'},\n",
       "   {'authorId': '48647153', 'name': 'Roberta Raileanu'}]},\n",
       " {'paperId': '93e2bf97b2664443b108e03950829cc79f3f8419',\n",
       "  'externalIds': {'ArXiv': '2410.01679',\n",
       "   'DBLP': 'journals/corr/abs-2410-01679',\n",
       "   'DOI': '10.48550/arXiv.2410.01679',\n",
       "   'CorpusId': 273026145},\n",
       "  'corpusId': 273026145,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/93e2bf97b2664443b108e03950829cc79f3f8419',\n",
       "  'title': 'VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment',\n",
       "  'abstract': \"Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in RL finetuning of LLM and demonstrate VinePPO's potential as a superior alternative.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 47,\n",
       "  'citationCount': 6,\n",
       "  'influentialCitationCount': 1,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-10-02',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2410.01679'},\n",
       "  'citationStyles': {'bibtex': '@Article{Kazemnejad2024VinePPOUR,\\n author = {Amirhossein Kazemnejad and Milad Aghajohari and Eva Portelance and Alessandro Sordoni and Siva Reddy and Aaron C. Courville and Nicolas Le Roux},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment},\\n volume = {abs/2410.01679},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '1754452702', 'name': 'Amirhossein Kazemnejad'},\n",
       "   {'authorId': '113383974', 'name': 'Milad Aghajohari'},\n",
       "   {'authorId': '27729635', 'name': 'Eva Portelance'},\n",
       "   {'authorId': '2041695', 'name': 'Alessandro Sordoni'},\n",
       "   {'authorId': '145732771', 'name': 'Siva Reddy'},\n",
       "   {'authorId': '2285877959', 'name': 'Aaron C. Courville'},\n",
       "   {'authorId': '2299948386', 'name': 'Nicolas Le Roux'}]},\n",
       " {'paperId': '9345e55a21959948499cee997522aa5eac7ed588',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2407-01476',\n",
       "   'ArXiv': '2407.01476',\n",
       "   'DOI': '10.48550/arXiv.2407.01476',\n",
       "   'CorpusId': 270870063},\n",
       "  'corpusId': 270870063,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/9345e55a21959948499cee997522aa5eac7ed588',\n",
       "  'title': 'Tree Search for Language Model Agents',\n",
       "  'abstract': 'Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. Towards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. Our approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. It is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. On the challenging VisualWebArena benchmark, applying our search algorithm on top of a GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. Our experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute. We conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work. Our code and models are publicly released at https://jykoh.com/search-agents.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 64,\n",
       "  'citationCount': 30,\n",
       "  'influentialCitationCount': 3,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-07-01',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2407.01476'},\n",
       "  'citationStyles': {'bibtex': '@Article{Koh2024TreeSF,\\n author = {Jing Yu Koh and Stephen McAleer and Daniel Fried and Ruslan Salakhutdinov},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Tree Search for Language Model Agents},\\n volume = {abs/2407.01476},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '23978705', 'name': 'Jing Yu Koh'},\n",
       "   {'authorId': '2296993699', 'name': 'Stephen McAleer'},\n",
       "   {'authorId': '2259931814', 'name': 'Daniel Fried'},\n",
       "   {'authorId': '2280902225', 'name': 'Ruslan Salakhutdinov'}]},\n",
       " {'paperId': 'c4ff8bc44d88cd267baf18ac5d3a3a1fe86d08eb',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2409-12917',\n",
       "   'ArXiv': '2409.12917',\n",
       "   'DOI': '10.48550/arXiv.2409.12917',\n",
       "   'CorpusId': 272753259},\n",
       "  'corpusId': 272753259,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/c4ff8bc44d88cd267baf18ac5d3a3a1fe86d08eb',\n",
       "  'title': 'Training Language Models to Self-Correct via Reinforcement Learning',\n",
       "  'abstract': \"Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs. Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are often insufficient for instilling self-correction behavior. In particular, we observe that training via SFT falls prey to either a distribution mismatch between mistakes made by the data-collection policy and the model's own responses, or to behavior collapse, where learning implicitly prefers only a certain mode of correction behavior that is often not effective at self-correction on test problems. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction behavior that is effective at test time as opposed to fitting high-reward responses for a given prompt. This regularization process includes an initial phase of multi-turn RL on a base model to generate a policy initialization that is less susceptible to collapse, followed by using a reward bonus to amplify self-correction. With Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on MATH and HumanEval.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 49,\n",
       "  'citationCount': 44,\n",
       "  'influentialCitationCount': 5,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-09-19',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2409.12917'},\n",
       "  'citationStyles': {'bibtex': '@Article{Kumar2024TrainingLM,\\n author = {Aviral Kumar and Vincent Zhuang and Rishabh Agarwal and Yi Su and John D. Co-Reyes and Avi Singh and Kate Baumli and Shariq Iqbal and Colton Bishop and Rebecca Roelofs and Lei M. Zhang and Kay McKinney and Disha Shrivastava and Cosmin Paduraru and George Tucker and D. Precup and Feryal M. P. Behbahani and Aleksandra Faust},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Training Language Models to Self-Correct via Reinforcement Learning},\\n volume = {abs/2409.12917},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2317038858', 'name': 'Aviral Kumar'},\n",
       "   {'authorId': '2284692849', 'name': 'Vincent Zhuang'},\n",
       "   {'authorId': '2258553001', 'name': 'Rishabh Agarwal'},\n",
       "   {'authorId': '2321906289', 'name': 'Yi Su'},\n",
       "   {'authorId': '2273515439', 'name': 'John D. Co-Reyes'},\n",
       "   {'authorId': '2258779676', 'name': 'Avi Singh'},\n",
       "   {'authorId': '1734809439', 'name': 'Kate Baumli'},\n",
       "   {'authorId': '2899335', 'name': 'Shariq Iqbal'},\n",
       "   {'authorId': '2224524201', 'name': 'Colton Bishop'},\n",
       "   {'authorId': '2258551343', 'name': 'Rebecca Roelofs'},\n",
       "   {'authorId': '2297176886', 'name': 'Lei M. Zhang'},\n",
       "   {'authorId': '2274102650', 'name': 'Kay McKinney'},\n",
       "   {'authorId': '2275113487', 'name': 'Disha Shrivastava'},\n",
       "   {'authorId': '3316271', 'name': 'Cosmin Paduraru'},\n",
       "   {'authorId': '2258552736', 'name': 'George Tucker'},\n",
       "   {'authorId': '2249762747', 'name': 'D. Precup'},\n",
       "   {'authorId': '145124447', 'name': 'Feryal M. P. Behbahani'},\n",
       "   {'authorId': '2258552654', 'name': 'Aleksandra Faust'}]},\n",
       " {'paperId': '501f8a0200a12a6f3906c1e4f3f40715e0e7d23a',\n",
       "  'externalIds': {'ArXiv': '2406.18629',\n",
       "   'DBLP': 'journals/corr/abs-2406-18629',\n",
       "   'DOI': '10.48550/arXiv.2406.18629',\n",
       "   'CorpusId': 270764693},\n",
       "  'corpusId': 270764693,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/501f8a0200a12a6f3906c1e4f3f40715e0e7d23a',\n",
       "  'title': 'Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs',\n",
       "  'abstract': \"Mathematical reasoning presents a significant challenge for Large Language Models (LLMs) due to the extensive and precise chain of reasoning required for accuracy. Ensuring the correctness of each reasoning step is critical. To address this, we aim to enhance the robustness and factuality of LLMs by learning from human feedback. However, Direct Preference Optimization (DPO) has shown limited benefits for long-chain mathematical reasoning, as models employing DPO struggle to identify detailed errors in incorrect answers. This limitation stems from a lack of fine-grained process supervision. We propose a simple, effective, and data-efficient method called Step-DPO, which treats individual reasoning steps as units for preference optimization rather than evaluating answers holistically. Additionally, we have developed a data construction pipeline for Step-DPO, enabling the creation of a high-quality dataset containing 10K step-wise preference pairs. We also observe that in DPO, self-generated data is more effective than data generated by humans or GPT-4, due to the latter's out-of-distribution nature. Our findings demonstrate that as few as 10K preference data pairs and fewer than 500 Step-DPO training steps can yield a nearly 3% gain in accuracy on MATH for models with over 70B parameters. Notably, Step-DPO, when applied to Qwen2-72B-Instruct, achieves scores of 70.8% and 94.0% on the test sets of MATH and GSM8K, respectively, surpassing a series of closed-source models, including GPT-4-1106, Claude-3-Opus, and Gemini-1.5-Pro. Our code, data, and models are available at https://github.com/dvlab-research/Step-DPO.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 48,\n",
       "  'citationCount': 45,\n",
       "  'influentialCitationCount': 9,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-06-26',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2406.18629'},\n",
       "  'citationStyles': {'bibtex': '@Article{Lai2024StepDPOSP,\\n author = {Xin Lai and Zhuotao Tian and Yukang Chen and Senqiao Yang and Xiangru Peng and Jiaya Jia},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs},\\n volume = {abs/2406.18629},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2237802684', 'name': 'Xin Lai'},\n",
       "   {'authorId': '148438070', 'name': 'Zhuotao Tian'},\n",
       "   {'authorId': '2109297557', 'name': 'Yukang Chen'},\n",
       "   {'authorId': '2276746547', 'name': 'Senqiao Yang'},\n",
       "   {'authorId': '2309117756', 'name': 'Xiangru Peng'},\n",
       "   {'authorId': '2291196160', 'name': 'Jiaya Jia'}]},\n",
       " {'paperId': 'df5bb57e03c38a439d664d3c609a1c03a9a64009',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2402-14083',\n",
       "   'ArXiv': '2402.14083',\n",
       "   'DOI': '10.48550/arXiv.2402.14083',\n",
       "   'CorpusId': 267782588},\n",
       "  'corpusId': 267782588,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/df5bb57e03c38a439d664d3c609a1c03a9a64009',\n",
       "  'title': 'Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping',\n",
       "  'abstract': \"While Transformers have enabled tremendous progress in various application settings, such architectures still trail behind traditional symbolic planners for solving complex decision making tasks. In this work, we demonstrate how to train Transformers to solve complex planning tasks. This is accomplished by training an encoder-decoder Transformer model to predict the search dynamics of the $A^*$ search algorithm. We fine tune this model to obtain a Searchformer, a Transformer model that optimally solves previously unseen Sokoban puzzles 93.7% of the time, while using up to 26.8% fewer search steps than the $A^*$ implementation that was used for training initially. In our training method, $A^*$'s search dynamics are expressed as a token sequence outlining when task states are added and removed into the search tree during symbolic planning. Searchformer significantly outperforms baselines that predict the optimal plan directly with a 5-10$\\\\times$ smaller model size and a 10$\\\\times$ smaller training dataset. Lastly, we demonstrate how Searchformer scales to larger and more complex decision making tasks with improved percentage of solved tasks and shortened search dynamics.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 52,\n",
       "  'citationCount': 40,\n",
       "  'influentialCitationCount': 2,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-02-21',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2402.14083'},\n",
       "  'citationStyles': {'bibtex': '@Article{Lehnert2024BeyondAB,\\n author = {Lucas Lehnert and Sainbayar Sukhbaatar and Paul Mcvay and Michael Rabbat and Yuandong Tian},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping},\\n volume = {abs/2402.14083},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2284988047', 'name': 'Lucas Lehnert'},\n",
       "   {'authorId': '2265067', 'name': 'Sainbayar Sukhbaatar'},\n",
       "   {'authorId': '2284990505', 'name': 'Paul Mcvay'},\n",
       "   {'authorId': '2284991448', 'name': 'Michael Rabbat'},\n",
       "   {'authorId': '2285362895', 'name': 'Yuandong Tian'}]},\n",
       " {'paperId': '5a20aa49b81b4e14fdb36814e557b3da60259ce9',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2402-12875',\n",
       "   'ArXiv': '2402.12875',\n",
       "   'DOI': '10.48550/arXiv.2402.12875',\n",
       "   'CorpusId': 267760184},\n",
       "  'corpusId': 267760184,\n",
       "  'publicationVenue': {'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40',\n",
       "   'name': 'International Conference on Learning Representations',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Int Conf Learn Represent', 'ICLR'],\n",
       "   'url': 'https://iclr.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/5a20aa49b81b4e14fdb36814e557b3da60259ce9',\n",
       "  'title': 'Chain of Thought Empowers Transformers to Solve Inherently Serial Problems',\n",
       "  'abstract': 'Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\\\\mathsf{poly}(n)$ embedding size can only solve problems in $\\\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\\\mathsf{AC}^0$, a proper subset of $ \\\\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.',\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 74,\n",
       "  'citationCount': 66,\n",
       "  'influentialCitationCount': 7,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science', 'Mathematics'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Mathematics', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-02-20',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2402.12875'},\n",
       "  'citationStyles': {'bibtex': '@Article{Li2024ChainOT,\\n author = {Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},\\n booktitle = {International Conference on Learning Representations},\\n journal = {ArXiv},\\n title = {Chain of Thought Empowers Transformers to Solve Inherently Serial Problems},\\n volume = {abs/2402.12875},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2284829110', 'name': 'Zhiyuan Li'},\n",
       "   {'authorId': '2284940181', 'name': 'Hong Liu'},\n",
       "   {'authorId': '2284824080', 'name': 'Denny Zhou'},\n",
       "   {'authorId': '2284835869', 'name': 'Tengyu Ma'}]},\n",
       " {'paperId': 'be8db99310602d66bba64bcf41a572c45816fbfc',\n",
       "  'externalIds': {'ArXiv': '2305.20050',\n",
       "   'DBLP': 'conf/iclr/LightmanKBEBLLS24',\n",
       "   'DOI': '10.48550/arXiv.2305.20050',\n",
       "   'CorpusId': 258987659},\n",
       "  'corpusId': 258987659,\n",
       "  'publicationVenue': {'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40',\n",
       "   'name': 'International Conference on Learning Representations',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Int Conf Learn Represent', 'ICLR'],\n",
       "   'url': 'https://iclr.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/be8db99310602d66bba64bcf41a572c45816fbfc',\n",
       "  'title': \"Let's Verify Step by Step\",\n",
       "  'abstract': 'In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.',\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 33,\n",
       "  'citationCount': 585,\n",
       "  'influentialCitationCount': 78,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2305.20050',\n",
       "   'status': 'CLOSED'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2023-05-31',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2305.20050'},\n",
       "  'citationStyles': {'bibtex': \"@Article{Lightman2023LetsVS,\\n author = {Hunter Lightman and Vineet Kosaraju and Yura Burda and Harrison Edwards and Bowen Baker and Teddy Lee and J. Leike and John Schulman and I. Sutskever and K. Cobbe},\\n booktitle = {International Conference on Learning Representations},\\n journal = {ArXiv},\\n title = {Let's Verify Step by Step},\\n volume = {abs/2305.20050},\\n year = {2023}\\n}\\n\"},\n",
       "  'authors': [{'authorId': '2219360905', 'name': 'Hunter Lightman'},\n",
       "   {'authorId': '13622184', 'name': 'Vineet Kosaraju'},\n",
       "   {'authorId': '51178856', 'name': 'Yura Burda'},\n",
       "   {'authorId': '144632352', 'name': 'Harrison Edwards'},\n",
       "   {'authorId': '40566201', 'name': 'Bowen Baker'},\n",
       "   {'authorId': '2110664120', 'name': 'Teddy Lee'},\n",
       "   {'authorId': '2990741', 'name': 'J. Leike'},\n",
       "   {'authorId': '47971768', 'name': 'John Schulman'},\n",
       "   {'authorId': '1701686', 'name': 'I. Sutskever'},\n",
       "   {'authorId': '6062736', 'name': 'K. Cobbe'}]},\n",
       " {'paperId': '3aaf6a2cbad5850ad81ab5c163599cb3d523436f',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2303-17651',\n",
       "   'ArXiv': '2303.17651',\n",
       "   'DOI': '10.48550/arXiv.2303.17651',\n",
       "   'CorpusId': 257900871},\n",
       "  'corpusId': 257900871,\n",
       "  'publicationVenue': {'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd',\n",
       "   'name': 'Neural Information Processing Systems',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'],\n",
       "   'url': 'http://neurips.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/3aaf6a2cbad5850ad81ab5c163599cb3d523436f',\n",
       "  'title': 'Self-Refine: Iterative Refinement with Self-Feedback',\n",
       "  'abstract': 'Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 52,\n",
       "  'citationCount': 1146,\n",
       "  'influentialCitationCount': 94,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2303.17651',\n",
       "   'status': 'GREEN'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2023-03-30',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2303.17651'},\n",
       "  'citationStyles': {'bibtex': '@Article{Madaan2023SelfRefineIR,\\n author = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and S. Welleck and Bodhisattwa Prasad Majumder and Shashank Gupta and A. Yazdanbakhsh and Peter Clark},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {Self-Refine: Iterative Refinement with Self-Feedback},\\n volume = {abs/2303.17651},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '21626987', 'name': 'Aman Madaan'},\n",
       "   {'authorId': '1721168', 'name': 'Niket Tandon'},\n",
       "   {'authorId': '1491232062', 'name': 'Prakhar Gupta'},\n",
       "   {'authorId': '1474550731', 'name': 'Skyler Hallinan'},\n",
       "   {'authorId': '49715441', 'name': 'Luyu Gao'},\n",
       "   {'authorId': '35823986', 'name': 'Sarah Wiegreffe'},\n",
       "   {'authorId': '47051926', 'name': 'Uri Alon'},\n",
       "   {'authorId': '46217681', 'name': 'Nouha Dziri'},\n",
       "   {'authorId': '9358910', 'name': 'Shrimai Prabhumoye'},\n",
       "   {'authorId': '46286308', 'name': 'Yiming Yang'},\n",
       "   {'authorId': '2129663', 'name': 'S. Welleck'},\n",
       "   {'authorId': '3165738', 'name': 'Bodhisattwa Prasad Majumder'},\n",
       "   {'authorId': '2152953535', 'name': 'Shashank Gupta'},\n",
       "   {'authorId': '2112229', 'name': 'A. Yazdanbakhsh'},\n",
       "   {'authorId': '48323507', 'name': 'Peter Clark'}]},\n",
       " {'paperId': '59eed4c468846a4d45105a4603dabf72e2bef830',\n",
       "  'externalIds': {'ArXiv': '2410.12832',\n",
       "   'DBLP': 'journals/corr/abs-2410-12832',\n",
       "   'DOI': '10.48550/arXiv.2410.12832',\n",
       "   'CorpusId': 273404003},\n",
       "  'corpusId': 273404003,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/59eed4c468846a4d45105a4603dabf72e2bef830',\n",
       "  'title': 'Generative Reward Models',\n",
       "  'abstract': 'Reinforcement Learning from Human Feedback (RLHF) has greatly improved the performance of modern Large Language Models (LLMs). The RLHF process is resource-intensive and technically challenging, generally requiring a large collection of human preference labels over model-generated outputs. Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection challenge by leveraging synthetic preferences generated by an LLM. However, recent work has shown that synthetic preferences labels may not align well with human preference judgments. To address this, we propose a hybrid approach that unifies RLHF and RLAIF methodologies. We introduce GenRM, an iterative algorithm that trains an LLM on self-generated reasoning traces, leading to synthetic preference labels matching human preference judgments. Empirically, we show that zero-shot LLM-based judgments under-perform compared to Bradley-Terry reward models on in-distribution tasks (between 9-36%). In contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 2- 6%). Our results show that combining the strengths of RLHF and RLAIF offers a promising approach for improving the quality of synthetic preference labels.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 41,\n",
       "  'citationCount': 7,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-10-02',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2410.12832'},\n",
       "  'citationStyles': {'bibtex': '@Article{Mahan2024GenerativeRM,\\n author = {Dakota Mahan and Duy Phung and Rafael Rafailov and Chase Blagden and nathan lile and Louis Castricato and Jan-Philipp Franken and Chelsea Finn and Alon Albalak},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Generative Reward Models},\\n volume = {abs/2410.12832},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2287928420', 'name': 'Dakota Mahan'},\n",
       "   {'authorId': '2273535247', 'name': 'Duy Phung'},\n",
       "   {'authorId': '2301519261', 'name': 'Rafael Rafailov'},\n",
       "   {'authorId': '2326300653', 'name': 'Chase Blagden'},\n",
       "   {'authorId': '2283848553', 'name': 'nathan lile'},\n",
       "   {'authorId': '28933528', 'name': 'Louis Castricato'},\n",
       "   {'authorId': '2220625823', 'name': 'Jan-Philipp Franken'},\n",
       "   {'authorId': '2284774407', 'name': 'Chelsea Finn'},\n",
       "   {'authorId': '2044198106', 'name': 'Alon Albalak'}]},\n",
       " {'paperId': '75c19f3249f644f5cb2182282fc117c089fd3f65',\n",
       "  'externalIds': {'DBLP': 'conf/iclr/MerrillS24',\n",
       "   'DOI': '10.48550/arXiv.2310.07923',\n",
       "   'CorpusId': 263909434},\n",
       "  'corpusId': 263909434,\n",
       "  'publicationVenue': {'id': '939c6e1d-0d17-4d6e-8a82-66d960df0e40',\n",
       "   'name': 'International Conference on Learning Representations',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Int Conf Learn Represent', 'ICLR'],\n",
       "   'url': 'https://iclr.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/75c19f3249f644f5cb2182282fc117c089fd3f65',\n",
       "  'title': 'The Expressive Power of Transformers with Chain of Thought',\n",
       "  'abstract': 'Recent theoretical work has identiﬁed surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating ﬁnite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers’ reasoning can be improved by allowing them to use a “chain of thought” or “scratchpad”, i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this',\n",
       "  'venue': 'International Conference on Learning Representations',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 29,\n",
       "  'citationCount': 85,\n",
       "  'influentialCitationCount': 5,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2310.07923',\n",
       "   'status': 'CLOSED'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'},\n",
       "   {'category': 'Mathematics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': None,\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2310.07923'},\n",
       "  'citationStyles': {'bibtex': '@Article{Merrill2024TheEP,\\n author = {William Merrill and Ashish Sabharwal},\\n booktitle = {International Conference on Learning Representations},\\n journal = {ArXiv},\\n title = {The Expressive Power of Transformers with Chain of Thought},\\n volume = {abs/2310.07923},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2257349351', 'name': 'William Merrill'},\n",
       "   {'authorId': '2257349398', 'name': 'Ashish Sabharwal'}]},\n",
       " {'paperId': 'ad2be51acf42f686a8d1de92d7435d84274ee62d',\n",
       "  'externalIds': {'ArXiv': '2402.14830',\n",
       "   'DBLP': 'journals/corr/abs-2402-14830',\n",
       "   'DOI': '10.48550/arXiv.2402.14830',\n",
       "   'CorpusId': 267897618},\n",
       "  'corpusId': 267897618,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/ad2be51acf42f686a8d1de92d7435d84274ee62d',\n",
       "  'title': 'Orca-Math: Unlocking the potential of SLMs in Grade School Math',\n",
       "  'abstract': 'Mathematical word problem-solving has long been recognized as a complex task for small language models (SLMs). A recent study hypothesized that the smallest model size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34 billion parameters. To reach this level of performance with smaller models, researcher often train SLMs to generate Python code or use tools to help avoid calculation errors. Additionally, they employ ensembling, where outputs of up to 100 model runs are combined to arrive at a more accurate result. Result selection is done using consensus, majority vote or a separate a verifier model used in conjunction with the SLM. Ensembling provides a substantial boost in accuracy but at a significant cost increase with multiple calls to the model (e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5). In this work, we present Orca-Math, a 7-billion-parameter SLM based on the Mistral-7B, which achieves 86.81% on GSM8k without the need for multiple model calls or the use of verifiers, code execution or any other external tools. Our approach has the following key elements: (1) A high quality synthetic dataset of 200K math problems created using a multi-agent setup where agents collaborate to create the data, (2) An iterative learning techniques that enables the SLM to practice solving problems, receive feedback on its solutions and learn from preference pairs incorporating the SLM solutions and the feedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves 81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math achieves 86.81% pass@1. Orca-Math surpasses the performance of significantly larger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It also significantly outperforms other smaller models while using much smaller data (hundreds of thousands vs. millions of problems).',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 38,\n",
       "  'citationCount': 49,\n",
       "  'influentialCitationCount': 5,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Mathematics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-02-16',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2402.14830'},\n",
       "  'citationStyles': {'bibtex': '@Article{Mitra2024OrcaMathUT,\\n author = {Arindam Mitra and Hamed Khanpour and Corby Rosset and Ahmed Awadallah},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Orca-Math: Unlocking the potential of SLMs in Grade School Math},\\n volume = {abs/2402.14830},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2256988075', 'name': 'Arindam Mitra'},\n",
       "   {'authorId': '2286299350', 'name': 'Hamed Khanpour'},\n",
       "   {'authorId': '41016119', 'name': 'Corby Rosset'},\n",
       "   {'authorId': '113916198', 'name': 'Ahmed Awadallah'}]},\n",
       " {'paperId': '533ad140ba5c273f500da9976aa08efef8a0a49c',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2410-06238',\n",
       "   'ArXiv': '2410.06238',\n",
       "   'DOI': '10.48550/arXiv.2410.06238',\n",
       "   'CorpusId': 273229116},\n",
       "  'corpusId': 273229116,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/533ad140ba5c273f500da9976aa08efef8a0a49c',\n",
       "  'title': 'EVOLvE: Evaluating and Optimizing LLMs For Exploration',\n",
       "  'abstract': \"Despite their success in many domains, large language models (LLMs) remain under-studied in scenarios requiring optimal decision-making under uncertainty. This is crucial as many real-world applications, ranging from personalized recommendations to healthcare interventions, demand that LLMs not only predict but also actively learn to make optimal decisions through exploration. In this work, we measure LLMs' (in)ability to make optimal decisions in bandits, a state-less reinforcement learning setting relevant to many applications. We develop a comprehensive suite of environments, including both context-free and contextual bandits with varying task difficulties, to benchmark LLMs' performance. Motivated by the existence of optimal exploration algorithms, we propose efficient ways to integrate this algorithmic knowledge into LLMs: by providing explicit algorithm-guided support during inference; and through algorithm distillation via in-context demonstrations and fine-tuning, using synthetic data generated from these algorithms. Impressively, these techniques allow us to achieve superior exploration performance with smaller models, surpassing larger models on various tasks. We conducted an extensive ablation study to shed light on various factors, such as task difficulty and data representation, that influence the efficiency of LLM exploration. Additionally, we conduct a rigorous analysis of the LLM's exploration efficiency using the concept of regret, linking its ability to explore to the model size and underlying algorithm.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 49,\n",
       "  'citationCount': 1,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-10-08',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2410.06238'},\n",
       "  'citationStyles': {'bibtex': '@Article{Nie2024EVOLvEEA,\\n author = {Allen Nie and Yi Su and B. Chang and Jonathan N. Lee and E. H. Chi and Quoc V. Le and Minmin Chen},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {EVOLvE: Evaluating and Optimizing LLMs For Exploration},\\n volume = {abs/2410.06238},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2325096570', 'name': 'Allen Nie'},\n",
       "   {'authorId': '2240772267', 'name': 'Yi Su'},\n",
       "   {'authorId': '2325042917', 'name': 'B. Chang'},\n",
       "   {'authorId': '2325099093', 'name': 'Jonathan N. Lee'},\n",
       "   {'authorId': '2301210096', 'name': 'E. H. Chi'},\n",
       "   {'authorId': '2256995069', 'name': 'Quoc V. Le'},\n",
       "   {'authorId': '2302529202', 'name': 'Minmin Chen'}]},\n",
       " {'paperId': '250d920ba5cf1e8dcb521eee47e181cf3eb3755a',\n",
       "  'externalIds': {'ArXiv': '2410.18252',\n",
       "   'DBLP': 'journals/corr/abs-2410-18252',\n",
       "   'DOI': '10.48550/arXiv.2410.18252',\n",
       "   'CorpusId': 273549959},\n",
       "  'corpusId': 273549959,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/250d920ba5cf1e8dcb521eee47e181cf3eb3755a',\n",
       "  'title': 'Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models',\n",
       "  'abstract': \"The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical deep RL literature, we propose separating generation and learning in RLHF. This enables asynchronous generation of new samples while simultaneously training on old samples, leading to faster training and more compute-optimal scaling. However, asynchronous training relies on an underexplored regime, online but off-policy RLHF: learning on samples from previous iterations of our model. To understand the challenges in this regime, we investigate a fundamental question: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? Among several RLHF algorithms we tested, we find that online DPO is most robust to off-policy data, and robustness increases with the scale of the policy model. We study further compute optimizations for asynchronous RLHF but find that they come at a performance cost, giving rise to a trade-off. Finally, we verify the scalability of asynchronous RLHF by training LLaMA 3.1 8B on an instruction-following task 40% faster than a synchronous run while matching final performance.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 55,\n",
       "  'citationCount': 1,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-10-23',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2410.18252'},\n",
       "  'citationStyles': {'bibtex': '@Article{Noukhovitch2024AsynchronousRF,\\n author = {Michael Noukhovitch and Shengyi Huang and Sophie Xhonneux and Arian Hosseini and Rishabh Agarwal and Aaron C. Courville},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models},\\n volume = {abs/2410.18252},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '41020834', 'name': 'Michael Noukhovitch'},\n",
       "   {'authorId': '2261900258', 'name': 'Shengyi Huang'},\n",
       "   {'authorId': '2204462616', 'name': 'Sophie Xhonneux'},\n",
       "   {'authorId': '2090537547', 'name': 'Arian Hosseini'},\n",
       "   {'authorId': '2317013277', 'name': 'Rishabh Agarwal'},\n",
       "   {'authorId': '2058336670', 'name': 'Aaron C. Courville'}]},\n",
       " {'paperId': 'a31918e9d8b50095f64d5d15187349ffbd927d4d',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2406-14197',\n",
       "   'ArXiv': '2406.14197',\n",
       "   'DOI': '10.48550/arXiv.2406.14197',\n",
       "   'CorpusId': 270620647},\n",
       "  'corpusId': 270620647,\n",
       "  'publicationVenue': {'id': '1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44',\n",
       "   'name': 'Annual Meeting of the Association for Computational Linguistics',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Annu Meet Assoc Comput Linguistics',\n",
       "    'Meeting of the Association for Computational Linguistics',\n",
       "    'ACL',\n",
       "    'Meet Assoc Comput Linguistics'],\n",
       "   'url': 'https://www.aclweb.org/anthology/venues/acl/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/a31918e9d8b50095f64d5d15187349ffbd927d4d',\n",
       "  'title': 'On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning',\n",
       "  'abstract': \"The performance of modern language models (LMs) has been improved by chain-of-thought (CoT) reasoning, i.e., the process of generating intermediate results that guide the model towards a final answer. A possible explanation for this improvement is that CoT reasoning extends an LM's computational power, as RNNs and transformers with additional scratch space are known to be Turing complete. Comparing LMs to Turing machines, however, introduces a category error - Turing machines decide language membership, whereas LMs define distributions over strings. To bridge this gap, we formalize CoT reasoning in a probabilistic setting. We present several results on the representational capacity of recurrent and transformer LMs with CoT reasoning, showing that they can represent the same family of distributions over strings as probabilistic Turing machines.\",\n",
       "  'venue': 'Annual Meeting of the Association for Computational Linguistics',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 51,\n",
       "  'citationCount': 10,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle', 'Conference'],\n",
       "  'publicationDate': '2024-06-20',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2406.14197'},\n",
       "  'citationStyles': {'bibtex': '@Article{Nowak2024OnTR,\\n author = {Franz Nowak and Anej Svete and Alexandra Butoi and Ryan Cotterell},\\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\\n journal = {ArXiv},\\n title = {On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning},\\n volume = {abs/2406.14197},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2219240211', 'name': 'Franz Nowak'},\n",
       "   {'authorId': '1492179067', 'name': 'Anej Svete'},\n",
       "   {'authorId': '2187683147', 'name': 'Alexandra Butoi'},\n",
       "   {'authorId': '2256986716', 'name': 'Ryan Cotterell'}]},\n",
       " {'paperId': '94e3badba15476a0f4058ab9066426e14fb06bf9',\n",
       "  'externalIds': {'DBLP': 'conf/nips/HoffmanPDDLPSSV23',\n",
       "   'ArXiv': '2312.02179',\n",
       "   'DOI': '10.48550/arXiv.2312.02179',\n",
       "   'CorpusId': 265659062},\n",
       "  'corpusId': 265659062,\n",
       "  'publicationVenue': {'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd',\n",
       "   'name': 'Neural Information Processing Systems',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'],\n",
       "   'url': 'http://neurips.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/94e3badba15476a0f4058ab9066426e14fb06bf9',\n",
       "  'title': 'Training Chain-of-Thought via Latent-Variable Inference',\n",
       "  'abstract': \"Large language models (LLMs) solve problems more accurately and interpretably when instructed to work out the answer step by step using a ``chain-of-thought'' (CoT) prompt. One can also improve LLMs' performance on a specific task by supervised fine-tuning, i.e., by using gradient ascent on some tunable parameters to maximize the average log-likelihood of correct answers from a labeled training set. Naively combining CoT with supervised tuning requires supervision not just of the correct answers, but also of detailed rationales that lead to those answers; these rationales are expensive to produce by hand. Instead, we propose a fine-tuning strategy that tries to maximize the \\\\emph{marginal} log-likelihood of generating a correct answer using CoT prompting, approximately averaging over all possible rationales. The core challenge is sampling from the posterior over rationales conditioned on the correct answer; we address it using a simple Markov-chain Monte Carlo (MCMC) expectation-maximization (EM) algorithm inspired by the self-taught reasoner (STaR), memoized wake-sleep, Markovian score climbing, and persistent contrastive divergence. This algorithm also admits a novel control-variate technique that drives the variance of our gradient estimates to zero as the model improves. Applying our technique to GSM8K and the tasks in BIG-Bench Hard, we find that this MCMC-EM fine-tuning technique typically improves the model's accuracy on held-out examples more than STaR or prompt-tuning with or without CoT.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 43,\n",
       "  'citationCount': 15,\n",
       "  'influentialCitationCount': 1,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2023-11-28',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2312.02179'},\n",
       "  'citationStyles': {'bibtex': '@Article{Phan2023TrainingCV,\\n author = {Du Phan and Matthew Hoffman and David Dohan and Sholto Douglas and Tuan Anh Le and Aaron T Parisi and Pavel Sountsov and Charles Sutton and S. Vikram and R. Saurous},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {Training Chain-of-Thought via Latent-Variable Inference},\\n volume = {abs/2312.02179},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2269733925', 'name': 'Du Phan'},\n",
       "   {'authorId': '2269733656', 'name': 'Matthew Hoffman'},\n",
       "   {'authorId': '35363891', 'name': 'David Dohan'},\n",
       "   {'authorId': '2269733876', 'name': 'Sholto Douglas'},\n",
       "   {'authorId': '2269759404', 'name': 'Tuan Anh Le'},\n",
       "   {'authorId': '2266464462', 'name': 'Aaron T Parisi'},\n",
       "   {'authorId': '3140888', 'name': 'Pavel Sountsov'},\n",
       "   {'authorId': '2269732453', 'name': 'Charles Sutton'},\n",
       "   {'authorId': '2425230', 'name': 'S. Vikram'},\n",
       "   {'authorId': '2278009', 'name': 'R. Saurous'}]},\n",
       " {'paperId': 'b393f619a87c5b6aa63c7abc7118263205b6aa62',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2408-07199',\n",
       "   'ArXiv': '2408.07199',\n",
       "   'DOI': '10.48550/arXiv.2408.07199',\n",
       "   'CorpusId': 271865516},\n",
       "  'corpusId': 271865516,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/b393f619a87c5b6aa63c7abc7118263205b6aa62',\n",
       "  'title': 'Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents',\n",
       "  'abstract': \"Large Language Models (LLMs) have shown remarkable capabilities in natural language tasks requiring complex reasoning, yet their application in agentic, multi-step reasoning within interactive environments remains a difficult challenge. Traditional supervised pre-training on static datasets falls short in enabling autonomous agent capabilities needed to perform complex decision-making in dynamic settings like web navigation. Previous attempts to bridge this ga-through supervised fine-tuning on curated expert demonstrations-often suffer from compounding errors and limited exploration data, resulting in sub-optimal policy outcomes. To overcome these challenges, we propose a framework that combines guided Monte Carlo Tree Search (MCTS) search with a self-critique mechanism and iterative fine-tuning on agent interactions using an off-policy variant of the Direct Preference Optimization (DPO) algorithm. Our method allows LLM agents to learn effectively from both successful and unsuccessful trajectories, thereby improving their generalization in complex, multi-step reasoning tasks. We validate our approach in the WebShop environment-a simulated e-commerce platform where it consistently outperforms behavior cloning and reinforced fine-tuning baseline, and beats average human performance when equipped with the capability to do online search. In real-world booking scenarios, our methodology boosts Llama-3 70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340% relative increase) after a single day of data collection and further to 95.4% with online search. We believe this represents a substantial leap forward in the capabilities of autonomous agents, paving the way for more sophisticated and reliable decision-making in real-world settings.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 63,\n",
       "  'citationCount': 32,\n",
       "  'influentialCitationCount': 1,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-08-13',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2408.07199'},\n",
       "  'citationStyles': {'bibtex': '@Article{Putta2024AgentQA,\\n author = {Pranav Putta and Edmund Mills and Naman Garg and S. Motwani and Chelsea Finn and Divyansh Garg and Rafael Rafailov},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents},\\n volume = {abs/2408.07199},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2214589609', 'name': 'Pranav Putta'},\n",
       "   {'authorId': '2162457333', 'name': 'Edmund Mills'},\n",
       "   {'authorId': '2307473601', 'name': 'Naman Garg'},\n",
       "   {'authorId': '2247588788', 'name': 'S. Motwani'},\n",
       "   {'authorId': '2284774407', 'name': 'Chelsea Finn'},\n",
       "   {'authorId': '2316051381', 'name': 'Divyansh Garg'},\n",
       "   {'authorId': '102801230', 'name': 'Rafael Rafailov'}]},\n",
       " {'paperId': 'bdc8c92a44714b468b40ef3d77e96d966f93141b',\n",
       "  'externalIds': {'DBLP': 'conf/nips/QuZGK24',\n",
       "   'ArXiv': '2407.18219',\n",
       "   'DOI': '10.48550/arXiv.2407.18219',\n",
       "   'CorpusId': 271432135},\n",
       "  'corpusId': 271432135,\n",
       "  'publicationVenue': {'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd',\n",
       "   'name': 'Neural Information Processing Systems',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'],\n",
       "   'url': 'http://neurips.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/bdc8c92a44714b468b40ef3d77e96d966f93141b',\n",
       "  'title': 'Recursive Introspection: Teaching Language Model Agents How to Self-Improve',\n",
       "  'abstract': 'A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 66,\n",
       "  'citationCount': 29,\n",
       "  'influentialCitationCount': 2,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-07-25',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2407.18219'},\n",
       "  'citationStyles': {'bibtex': '@Article{Qu2024RecursiveIT,\\n author = {Yuxiao Qu and Tianjun Zhang and Naman Garg and Aviral Kumar},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {Recursive Introspection: Teaching Language Model Agents How to Self-Improve},\\n volume = {abs/2407.18219},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2312907266', 'name': 'Yuxiao Qu'},\n",
       "   {'authorId': '2313179548', 'name': 'Tianjun Zhang'},\n",
       "   {'authorId': '2307473601', 'name': 'Naman Garg'},\n",
       "   {'authorId': '2313046878', 'name': 'Aviral Kumar'}]},\n",
       " {'paperId': '287aa18cec7808fb5364d6ec25099f59c5014db7',\n",
       "  'externalIds': {'ArXiv': '2409.12618',\n",
       "   'DBLP': 'journals/corr/abs-2409-12618',\n",
       "   'DOI': '10.48550/arXiv.2409.12618',\n",
       "   'CorpusId': 272753547},\n",
       "  'corpusId': 272753547,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/287aa18cec7808fb5364d6ec25099f59c5014db7',\n",
       "  'title': 'Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning',\n",
       "  'abstract': 'Iterative human engagement is a common and effective means of leveraging the advanced language processing power of large language models (LLMs). Using well-structured prompts in a conversational manner, human users can effectively influence an LLM to develop more thoughtful and accurate responses. Motivated by this insight, we propose the Iteration of Thought (IoT) framework for enhancing LLM responses by generating\"thought\"-provoking prompts vis a vis an input query and the current iteration of an LLM\\'s response. Unlike static or semi-static approaches, e.g. Chain of Thought (CoT) or Tree of Thoughts (ToT), IoT adapts its reasoning path dynamically, based on evolving context, and without generating alternate explorative thoughts which are ultimately discarded. The three components of the IoT framework are (1) an Inner Dialogue Agent (IDA) responsible for generating instructive, context-specific prompts; (2) an LLM Agent (LLMA) that processes these prompts to refine its responses; and (3) an iterative prompting loop that implements a conversation between the former two components. We introduce two variants of our framework: Autonomous Iteration of Thought (AIoT), where an LLM decides when to stop iterating, and Guided Iteration of Thought (GIoT), which always forces a fixed number iterations. We investigate the performance of IoT across various datasets, spanning complex reasoning tasks from the GPQA dataset, explorative problem-solving in Game of 24, puzzle solving in Mini Crosswords, and multi-hop question answering from the HotpotQA dataset. Our results show that IoT represents a viable paradigm for autonomous response refinement in LLMs, showcasing significant improvements over CoT and thereby enabling more adaptive and efficient reasoning systems that minimize human intervention.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 35,\n",
       "  'citationCount': 3,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-09-19',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2409.12618'},\n",
       "  'citationStyles': {'bibtex': '@Article{Radha2024IterationOT,\\n author = {Santosh Kumar Radha and Yasamin Nouri Jelyani and A. Ghukasyan and O. Goktas},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Iteration of Thought: Leveraging Inner Dialogue for Autonomous Large Language Model Reasoning},\\n volume = {abs/2409.12618},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '46254996', 'name': 'Santosh Kumar Radha'},\n",
       "   {'authorId': '2321873048', 'name': 'Yasamin Nouri Jelyani'},\n",
       "   {'authorId': '13829278', 'name': 'A. Ghukasyan'},\n",
       "   {'authorId': '1768149875', 'name': 'O. Goktas'}]},\n",
       " {'paperId': '7c0c0445e89347798800aad3497fcf2f2d27d4e6',\n",
       "  'externalIds': {'MAG': '1977989560',\n",
       "   'DBLP': 'journals/amai/Rosin11',\n",
       "   'DOI': '10.1007/s10472-011-9258-6',\n",
       "   'CorpusId': 207081359},\n",
       "  'corpusId': 207081359,\n",
       "  'publicationVenue': {'id': 'f8eb9cd5-5961-4095-893b-76281b1a0665',\n",
       "   'name': 'Annals of Mathematics and Artificial Intelligence',\n",
       "   'type': 'journal',\n",
       "   'alternate_names': ['Ann Math Artif Intell'],\n",
       "   'issn': '1012-2443',\n",
       "   'url': 'https://www.springer.com/computer/ai/journal/10472',\n",
       "   'alternate_urls': ['https://link.springer.com/journal/10472']},\n",
       "  'url': 'https://www.semanticscholar.org/paper/7c0c0445e89347798800aad3497fcf2f2d27d4e6',\n",
       "  'title': 'Multi-armed bandits with episode context',\n",
       "  'abstract': None,\n",
       "  'venue': 'Annals of Mathematics and Artificial Intelligence',\n",
       "  'year': 2011,\n",
       "  'referenceCount': 36,\n",
       "  'citationCount': 262,\n",
       "  'influentialCitationCount': 31,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science', 'History', 'Mathematics'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'History', 'source': 'external'},\n",
       "   {'category': 'Mathematics', 'source': 'external'},\n",
       "   {'category': 'Economics', 'source': 's2-fos-model'},\n",
       "   {'category': 'Mathematics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2011-03-01',\n",
       "  'journal': {'name': 'Annals of Mathematics and Artificial Intelligence',\n",
       "   'pages': '203-230',\n",
       "   'volume': '61'},\n",
       "  'citationStyles': {'bibtex': '@Article{Rosin2011MultiarmedBW,\\n author = {Christopher D. Rosin},\\n booktitle = {Annals of Mathematics and Artificial Intelligence},\\n journal = {Annals of Mathematics and Artificial Intelligence},\\n pages = {203-230},\\n title = {Multi-armed bandits with episode context},\\n volume = {61},\\n year = {2011}\\n}\\n'},\n",
       "  'authors': [{'authorId': '46291010', 'name': 'Christopher D. Rosin'}]},\n",
       " {'paperId': '0a3791cbac5349ecd972ba2bef7b6a913954c5ef',\n",
       "  'externalIds': {'ArXiv': '2412.12119',\n",
       "   'DBLP': 'journals/corr/abs-2412-12119',\n",
       "   'DOI': '10.48550/arXiv.2412.12119',\n",
       "   'CorpusId': 274788614},\n",
       "  'corpusId': 274788614,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/0a3791cbac5349ecd972ba2bef7b6a913954c5ef',\n",
       "  'title': 'Mastering Board Games by External and Internal Planning with Language Models',\n",
       "  'abstract': \"While large language models perform well on a range of complex tasks (e.g., text generation, question answering, summarization), robust multi-step planning and reasoning remains a considerable challenge for them. In this paper we show that search-based planning can significantly improve LLMs' playing strength across several board games (Chess, Fischer Random / Chess960, Connect Four, and Hex). We introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external engine, and in internal search, the model directly generates in-context a linearized tree of potential futures and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, capturing the transition and value functions across these games. We find that our pre-training method minimizes hallucinations, as our model is highly accurate regarding state prediction and legal moves. Additionally, both internal and external search indeed improve win-rates against state-of-the-art bots, even reaching Grandmaster-level performance in chess while operating on a similar move count search budget per decision as human Grandmasters. The way we combine search with domain knowledge is not specific to board games, suggesting direct extensions into more general language model inference and training techniques.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 0,\n",
       "  'citationCount': 2,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-12-02',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2412.12119'},\n",
       "  'citationStyles': {'bibtex': \"@Article{Schultz2024MasteringBG,\\n author = {John Schultz and Jakub Adamek and Matej Jusup and Marc Lanctot and Michael Kaisers and Sarah Perrin and Daniel Hennes and Jeremy Shar and Cannada Lewis and Anian Ruoss and Tom Zahavy and Petar Velivckovi'c and Laurel Prince and Satinder Singh and Eric Malmi and Nenad Tomavsev},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Mastering Board Games by External and Internal Planning with Language Models},\\n volume = {abs/2412.12119},\\n year = {2024}\\n}\\n\"},\n",
       "  'authors': [{'authorId': '2335665852', 'name': 'John Schultz'},\n",
       "   {'authorId': '50290651', 'name': 'Jakub Adamek'},\n",
       "   {'authorId': '2308033439', 'name': 'Matej Jusup'},\n",
       "   {'authorId': '1975889', 'name': 'Marc Lanctot'},\n",
       "   {'authorId': '2282975282', 'name': 'Michael Kaisers'},\n",
       "   {'authorId': '2335683391', 'name': 'Sarah Perrin'},\n",
       "   {'authorId': '2335666034', 'name': 'Daniel Hennes'},\n",
       "   {'authorId': '2335664392', 'name': 'Jeremy Shar'},\n",
       "   {'authorId': '2335667529', 'name': 'Cannada Lewis'},\n",
       "   {'authorId': '12114187', 'name': 'Anian Ruoss'},\n",
       "   {'authorId': '3331540', 'name': 'Tom Zahavy'},\n",
       "   {'authorId': '1742197495', 'name': \"Petar Velivckovi'c\"},\n",
       "   {'authorId': '2258722620', 'name': 'Laurel Prince'},\n",
       "   {'authorId': '2336551795', 'name': 'Satinder Singh'},\n",
       "   {'authorId': '3288074', 'name': 'Eric Malmi'},\n",
       "   {'authorId': '2298270407', 'name': 'Nenad Tomavsev'}]},\n",
       " {'paperId': 'fca92fe287c44c9ec79ca1f2762b0bf2e5e8df2b',\n",
       "  'externalIds': {'ArXiv': '2308.10379',\n",
       "   'DBLP': 'conf/icml/SelAK0024',\n",
       "   'DOI': '10.48550/arXiv.2308.10379',\n",
       "   'CorpusId': 261049794},\n",
       "  'corpusId': 261049794,\n",
       "  'publicationVenue': {'id': 'fc0a208c-acb7-47dc-a0d4-af8190e21d29',\n",
       "   'name': 'International Conference on Machine Learning',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['ICML', 'Int Conf Mach Learn'],\n",
       "   'url': 'https://icml.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/fca92fe287c44c9ec79ca1f2762b0bf2e5e8df2b',\n",
       "  'title': 'Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models',\n",
       "  'abstract': 'Current literature, aiming to surpass the\"Chain-of-Thought\"approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models\\' (LLMs) reasoning capacities. Due to their myopic perspective, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM\\'s inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method\\'s efficacy and its nuances in application. The code and related content can be found in: https://algorithm-of-thoughts.github.io.',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 49,\n",
       "  'citationCount': 48,\n",
       "  'influentialCitationCount': 3,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.10379',\n",
       "   'status': 'CLOSED'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle', 'Conference'],\n",
       "  'publicationDate': '2023-08-20',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2308.10379'},\n",
       "  'citationStyles': {'bibtex': '@Article{Sel2023AlgorithmOT,\\n author = {Bilgehan Sel and Ahmad S. Al-Tawaha and Vanshaj Khattar and Lucy Wang and R. Jia and Ming Jin},\\n booktitle = {International Conference on Machine Learning},\\n journal = {ArXiv},\\n title = {Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models},\\n volume = {abs/2308.10379},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2090055589', 'name': 'Bilgehan Sel'},\n",
       "   {'authorId': '2121379922', 'name': 'Ahmad S. Al-Tawaha'},\n",
       "   {'authorId': '1515859806', 'name': 'Vanshaj Khattar'},\n",
       "   {'authorId': '2153518376', 'name': 'Lucy Wang'},\n",
       "   {'authorId': '39823639', 'name': 'R. Jia'},\n",
       "   {'authorId': '2072905592', 'name': 'Ming Jin'}]},\n",
       " {'paperId': '490f872109e92f5230a7452487441f0dd19b98a8',\n",
       "  'externalIds': {'ArXiv': '2406.14532',\n",
       "   'DBLP': 'conf/nips/SetlurGGGSK24',\n",
       "   'DOI': '10.48550/arXiv.2406.14532',\n",
       "   'CorpusId': 270620658},\n",
       "  'corpusId': 270620658,\n",
       "  'publicationVenue': {'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd',\n",
       "   'name': 'Neural Information Processing Systems',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'],\n",
       "   'url': 'http://neurips.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/490f872109e92f5230a7452487441f0dd19b98a8',\n",
       "  'title': 'RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold',\n",
       "  'abstract': 'Training on model-generated synthetic data is a promising approach for finetuning LLMs, but it remains unclear when it helps or hurts. In this paper, we investigate this question for math reasoning via an empirical study, followed by building a conceptual understanding of our observations. First, we find that while the typical approach of finetuning a model on synthetic correct or positive problem-solution pairs generated by capable models offers modest performance gains, sampling more correct solutions from the finetuned learner itself followed by subsequent fine-tuning on this self-generated data $\\\\textbf{doubles}$ the efficiency of the same synthetic problems. At the same time, training on model-generated positives can amplify various spurious correlations, resulting in flat or even inverse scaling trends as the amount of data increases. Surprisingly, we find that several of these issues can be addressed if we also utilize negative responses, i.e., model-generated responses that are deemed incorrect by a final answer verifier. Crucially, these negatives must be constructed such that the training can appropriately recover the utility or advantage of each intermediate step in the negative response. With this per-step scheme, we are able to attain consistent gains over only positive data, attaining performance similar to amplifying the amount of synthetic data by $\\\\mathbf{8 \\\\times}$. We show that training on per-step negatives can help to unlearn spurious correlations in the positive data, and is equivalent to advantage-weighted reinforcement learning (RL), implying that it inherits robustness benefits of RL over imitating positive data alone.',\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 66,\n",
       "  'citationCount': 30,\n",
       "  'influentialCitationCount': 4,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Mathematics', 'source': 's2-fos-model'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-06-20',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2406.14532'},\n",
       "  'citationStyles': {'bibtex': '@Article{Setlur2024RLOI,\\n author = {Amrith Rajagopal Setlur and Saurabh Garg and Xinyang Geng and Naman Garg and Virginia Smith and Aviral Kumar},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {RL on Incorrect Synthetic Data Scales the Efficiency of LLM Math Reasoning by Eight-Fold},\\n volume = {abs/2406.14532},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '80366270', 'name': 'Amrith Rajagopal Setlur'},\n",
       "   {'authorId': '2269884773', 'name': 'Saurabh Garg'},\n",
       "   {'authorId': '3468192', 'name': 'Xinyang Geng'},\n",
       "   {'authorId': '2307473601', 'name': 'Naman Garg'},\n",
       "   {'authorId': '2271125124', 'name': 'Virginia Smith'},\n",
       "   {'authorId': '2247685852', 'name': 'Aviral Kumar'}]},\n",
       " {'paperId': '66d9550a51cb493aaeeae72d396b0bdb1ca47fe9',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2410-08146',\n",
       "   'ArXiv': '2410.08146',\n",
       "   'DOI': '10.48550/arXiv.2410.08146',\n",
       "   'CorpusId': 273233462},\n",
       "  'corpusId': 273233462,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/66d9550a51cb493aaeeae72d396b0bdb1ca47fe9',\n",
       "  'title': 'Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning',\n",
       "  'abstract': 'A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), we ask:\"How should we design process rewards?\". Our key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. We theoretically characterize the set of good provers and our results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, our characterization shows that weak prover policies can substantially improve a stronger base policy, which we also observe empirically. We validate our claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is $>8\\\\%$ more accurate, and $1.5-5\\\\times$ more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with $5-6\\\\times$ gain in sample efficiency, and $>6\\\\%$ gain in accuracy, over ORMs.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 44,\n",
       "  'citationCount': 17,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-10-10',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2410.08146'},\n",
       "  'citationStyles': {'bibtex': '@Article{Setlur2024RewardingPS,\\n author = {Amrith Rajagopal Setlur and Chirag Nagpal and Adam Fisch and Xinyang Geng and Jacob Eisenstein and Rishabh Agarwal and Alekh Agarwal and Jonathan Berant and Aviral Kumar},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning},\\n volume = {abs/2410.08146},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '80366270', 'name': 'Amrith Rajagopal Setlur'},\n",
       "   {'authorId': '2963503', 'name': 'Chirag Nagpal'},\n",
       "   {'authorId': '2274102752', 'name': 'Adam Fisch'},\n",
       "   {'authorId': '3468192', 'name': 'Xinyang Geng'},\n",
       "   {'authorId': '2256409622', 'name': 'Jacob Eisenstein'},\n",
       "   {'authorId': '2317013277', 'name': 'Rishabh Agarwal'},\n",
       "   {'authorId': '2274120058', 'name': 'Alekh Agarwal'},\n",
       "   {'authorId': '2282138214', 'name': 'Jonathan Berant'},\n",
       "   {'authorId': '2317038858', 'name': 'Aviral Kumar'}]},\n",
       " {'paperId': '66d9550a51cb493aaeeae72d396b0bdb1ca47fe9',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2410-08146',\n",
       "   'ArXiv': '2410.08146',\n",
       "   'DOI': '10.48550/arXiv.2410.08146',\n",
       "   'CorpusId': 273233462},\n",
       "  'corpusId': 273233462,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/66d9550a51cb493aaeeae72d396b0bdb1ca47fe9',\n",
       "  'title': 'Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning',\n",
       "  'abstract': 'A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, potentially improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. To improve a base policy by running search against a PRM or using it as dense rewards for reinforcement learning (RL), we ask:\"How should we design process rewards?\". Our key insight is that, to be effective, the process reward for a step should measure progress: a change in the likelihood of producing a correct response in the future, before and after taking the step, corresponding to the notion of step-level advantages in RL. Crucially, this progress should be measured under a prover policy distinct from the base policy. We theoretically characterize the set of good provers and our results show that optimizing process rewards from such provers improves exploration during test-time search and online RL. In fact, our characterization shows that weak prover policies can substantially improve a stronger base policy, which we also observe empirically. We validate our claims by training process advantage verifiers (PAVs) to predict progress under such provers, and show that compared to ORMs, test-time search against PAVs is $>8\\\\%$ more accurate, and $1.5-5\\\\times$ more compute-efficient. Online RL with dense rewards from PAVs enables one of the first results with $5-6\\\\times$ gain in sample efficiency, and $>6\\\\%$ gain in accuracy, over ORMs.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 44,\n",
       "  'citationCount': 17,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-10-10',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2410.08146'},\n",
       "  'citationStyles': {'bibtex': '@Article{Setlur2024RewardingPS,\\n author = {Amrith Rajagopal Setlur and Chirag Nagpal and Adam Fisch and Xinyang Geng and Jacob Eisenstein and Rishabh Agarwal and Alekh Agarwal and Jonathan Berant and Aviral Kumar},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning},\\n volume = {abs/2410.08146},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '80366270', 'name': 'Amrith Rajagopal Setlur'},\n",
       "   {'authorId': '2963503', 'name': 'Chirag Nagpal'},\n",
       "   {'authorId': '2274102752', 'name': 'Adam Fisch'},\n",
       "   {'authorId': '3468192', 'name': 'Xinyang Geng'},\n",
       "   {'authorId': '2256409622', 'name': 'Jacob Eisenstein'},\n",
       "   {'authorId': '2317013277', 'name': 'Rishabh Agarwal'},\n",
       "   {'authorId': '2274120058', 'name': 'Alekh Agarwal'},\n",
       "   {'authorId': '2282138214', 'name': 'Jonathan Berant'},\n",
       "   {'authorId': '2317038858', 'name': 'Aviral Kumar'}]},\n",
       " {'paperId': '35b142ea69598e6241f0011312128031df55895c',\n",
       "  'externalIds': {'ArXiv': '2402.03300',\n",
       "   'DBLP': 'journals/corr/abs-2402-03300',\n",
       "   'DOI': '10.48550/arXiv.2402.03300',\n",
       "   'CorpusId': 267412607},\n",
       "  'corpusId': 267412607,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/35b142ea69598e6241f0011312128031df55895c',\n",
       "  'title': 'DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models',\n",
       "  'abstract': 'Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 57,\n",
       "  'citationCount': 267,\n",
       "  'influentialCitationCount': 59,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Mathematics', 'source': 's2-fos-model'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-02-05',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2402.03300'},\n",
       "  'citationStyles': {'bibtex': '@Article{Shao2024DeepSeekMathPT,\\n author = {Zhihong Shao and Peiyi Wang and Qihao Zhu and R. Xu and Jun-Mei Song and Mingchuan Zhang and Y. K. Li and Yu Wu and Daya Guo},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models},\\n volume = {abs/2402.03300},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '144485528', 'name': 'Zhihong Shao'},\n",
       "   {'authorId': '144202874', 'name': 'Peiyi Wang'},\n",
       "   {'authorId': '2278223869', 'name': 'Qihao Zhu'},\n",
       "   {'authorId': '2274917091', 'name': 'R. Xu'},\n",
       "   {'authorId': '2258088582', 'name': 'Jun-Mei Song'},\n",
       "   {'authorId': '2278384213', 'name': 'Mingchuan Zhang'},\n",
       "   {'authorId': '2278599324', 'name': 'Y. K. Li'},\n",
       "   {'authorId': '49176273', 'name': 'Yu Wu'},\n",
       "   {'authorId': '2278834796', 'name': 'Daya Guo'}]},\n",
       " {'paperId': '46299fee72ca833337b3882ae1d8316f44b32b3c',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2303-11366',\n",
       "   'DOI': '10.48550/arXiv.2303.11366',\n",
       "   'CorpusId': 257636839},\n",
       "  'corpusId': 257636839,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/46299fee72ca833337b3882ae1d8316f44b32b3c',\n",
       "  'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
       "  'abstract': 'Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective heuristic that enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment. To assess our approach, we evaluate the agent’s ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97% and 51%, respectively, and provide a discussion on the emergent property of self-reflection.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 29,\n",
       "  'citationCount': 249,\n",
       "  'influentialCitationCount': 21,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2303.11366',\n",
       "   'status': 'GREEN'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': None,\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2303.11366'},\n",
       "  'citationStyles': {'bibtex': '@Article{Shinn2023ReflexionAA,\\n author = {Noah Shinn and Beck Labash and A. Gopinath},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Reflexion: an autonomous agent with dynamic memory and self-reflection},\\n volume = {abs/2303.11366},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2212367248', 'name': 'Noah Shinn'},\n",
       "   {'authorId': '2212367414', 'name': 'Beck Labash'},\n",
       "   {'authorId': '2162047785', 'name': 'A. Gopinath'}]},\n",
       " {'paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "  'externalIds': {'DBLP': 'journals/nature/SilverHMGSDSAPL16',\n",
       "   'MAG': '2257979135',\n",
       "   'DOI': '10.1038/nature16961',\n",
       "   'CorpusId': 515925,\n",
       "   'PubMed': '26819042'},\n",
       "  'corpusId': 515925,\n",
       "  'publicationVenue': {'id': '6c24a0a0-b07d-4d7b-a19b-fd09a3ed453a',\n",
       "   'name': 'Nature',\n",
       "   'type': 'journal',\n",
       "   'issn': '0028-0836',\n",
       "   'url': 'https://www.nature.com/',\n",
       "   'alternate_urls': ['http://www.nature.com/nature/',\n",
       "    'https://www.nature.com/nature/',\n",
       "    'http://www.nature.com/nature/archive/index.html']},\n",
       "  'url': 'https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "  'title': 'Mastering the game of Go with deep neural networks and tree search',\n",
       "  'abstract': None,\n",
       "  'venue': 'Nature',\n",
       "  'year': 2016,\n",
       "  'referenceCount': 72,\n",
       "  'citationCount': 16540,\n",
       "  'influentialCitationCount': 561,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://www.nature.com/articles/nature16961.pdf',\n",
       "   'status': 'BRONZE'},\n",
       "  'fieldsOfStudy': ['Medicine', 'Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Medicine', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2016-01-27',\n",
       "  'journal': {'name': 'Nature', 'pages': '484-489', 'volume': '529'},\n",
       "  'citationStyles': {'bibtex': '@Article{Silver2016MasteringTG,\\n author = {David Silver and Aja Huang and Chris J. Maddison and A. Guez and L. Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Vedavyas Panneershelvam and Marc Lanctot and S. Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and I. Sutskever and T. Lillicrap and M. Leach and K. Kavukcuoglu and T. Graepel and D. Hassabis},\\n booktitle = {Nature},\\n journal = {Nature},\\n pages = {484-489},\\n title = {Mastering the game of Go with deep neural networks and tree search},\\n volume = {529},\\n year = {2016}\\n}\\n'},\n",
       "  'authors': [{'authorId': '145824029', 'name': 'David Silver'},\n",
       "   {'authorId': '1885349', 'name': 'Aja Huang'},\n",
       "   {'authorId': '2772217', 'name': 'Chris J. Maddison'},\n",
       "   {'authorId': '35099444', 'name': 'A. Guez'},\n",
       "   {'authorId': '2175946', 'name': 'L. Sifre'},\n",
       "   {'authorId': '47568983', 'name': 'George van den Driessche'},\n",
       "   {'authorId': '4337102', 'name': 'Julian Schrittwieser'},\n",
       "   {'authorId': '2460849', 'name': 'Ioannis Antonoglou'},\n",
       "   {'authorId': '2749418', 'name': 'Vedavyas Panneershelvam'},\n",
       "   {'authorId': '1975889', 'name': 'Marc Lanctot'},\n",
       "   {'authorId': '48373216', 'name': 'S. Dieleman'},\n",
       "   {'authorId': '2401609', 'name': 'Dominik Grewe'},\n",
       "   {'authorId': '4111313', 'name': 'John Nham'},\n",
       "   {'authorId': '2583391', 'name': 'Nal Kalchbrenner'},\n",
       "   {'authorId': '1701686', 'name': 'I. Sutskever'},\n",
       "   {'authorId': '2542999', 'name': 'T. Lillicrap'},\n",
       "   {'authorId': '40662181', 'name': 'M. Leach'},\n",
       "   {'authorId': '2645384', 'name': 'K. Kavukcuoglu'},\n",
       "   {'authorId': '1686971', 'name': 'T. Graepel'},\n",
       "   {'authorId': '48987704', 'name': 'D. Hassabis'}]},\n",
       " {'paperId': 'f9717d29840f4d8f1cc19d1b1e80c5d12ec40608',\n",
       "  'externalIds': {'MAG': '2902907165',\n",
       "   'DOI': '10.1126/science.aar6404',\n",
       "   'CorpusId': 54457125,\n",
       "   'PubMed': '30523106'},\n",
       "  'corpusId': 54457125,\n",
       "  'publicationVenue': {'id': 'f59506a8-d8bb-4101-b3d4-c4ac3ed03dad',\n",
       "   'name': 'Science',\n",
       "   'type': 'journal',\n",
       "   'issn': '0193-4511',\n",
       "   'alternate_issns': ['0036-8075'],\n",
       "   'url': 'https://www.jstor.org/journal/science',\n",
       "   'alternate_urls': ['https://www.sciencemag.org/',\n",
       "    'http://www.sciencemag.org/',\n",
       "    'http://www.jstor.org/journals/00368075.html',\n",
       "    'http://www.sciencemag.org/archive/']},\n",
       "  'url': 'https://www.semanticscholar.org/paper/f9717d29840f4d8f1cc19d1b1e80c5d12ec40608',\n",
       "  'title': 'A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play',\n",
       "  'abstract': 'One program to rule them all Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.',\n",
       "  'venue': 'Science',\n",
       "  'year': 2018,\n",
       "  'referenceCount': 55,\n",
       "  'citationCount': 3338,\n",
       "  'influentialCitationCount': 163,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://science.sciencemag.org/content/sci/362/6419/1140.full.pdf',\n",
       "   'status': 'BRONZE'},\n",
       "  'fieldsOfStudy': ['Computer Science', 'Medicine'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Medicine', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2018-12-07',\n",
       "  'journal': {'name': 'Science', 'pages': '1140 - 1144', 'volume': '362'},\n",
       "  'citationStyles': {'bibtex': '@Article{Silver2018AGR,\\n author = {David Silver and T. Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and A. Guez and Marc Lanctot and L. Sifre and D. Kumaran and T. Graepel and T. Lillicrap and K. Simonyan and D. Hassabis},\\n booktitle = {Science},\\n journal = {Science},\\n pages = {1140 - 1144},\\n title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},\\n volume = {362},\\n year = {2018}\\n}\\n'},\n",
       "  'authors': [{'authorId': '145824029', 'name': 'David Silver'},\n",
       "   {'authorId': '2067208983', 'name': 'T. Hubert'},\n",
       "   {'authorId': '4337102', 'name': 'Julian Schrittwieser'},\n",
       "   {'authorId': '2460849', 'name': 'Ioannis Antonoglou'},\n",
       "   {'authorId': '40227832', 'name': 'Matthew Lai'},\n",
       "   {'authorId': '35099444', 'name': 'A. Guez'},\n",
       "   {'authorId': '1975889', 'name': 'Marc Lanctot'},\n",
       "   {'authorId': '2175946', 'name': 'L. Sifre'},\n",
       "   {'authorId': '2106164', 'name': 'D. Kumaran'},\n",
       "   {'authorId': '1686971', 'name': 'T. Graepel'},\n",
       "   {'authorId': '2542999', 'name': 'T. Lillicrap'},\n",
       "   {'authorId': '34838386', 'name': 'K. Simonyan'},\n",
       "   {'authorId': '48987704', 'name': 'D. Hassabis'}]},\n",
       " {'paperId': '48362b169a235ca650918c489c8cea4c597da645',\n",
       "  'externalIds': {'ArXiv': '2312.06585',\n",
       "   'DBLP': 'journals/tmlr/SinghCAAPGLH0XP24',\n",
       "   'DOI': '10.48550/arXiv.2312.06585',\n",
       "   'CorpusId': 266163375},\n",
       "  'corpusId': 266163375,\n",
       "  'publicationVenue': None,\n",
       "  'url': 'https://www.semanticscholar.org/paper/48362b169a235ca650918c489c8cea4c597da645',\n",
       "  'title': 'Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models',\n",
       "  'abstract': 'Fine-tuning language models~(LMs) on human-generated data remains a prevalent practice. However, the performance of such models is often limited by the quantity and diversity of high-quality human data. In this paper, we explore whether we can go beyond human data on tasks where we have access to scalar feedback, for example, on math problems where one can verify correctness. To do so, we investigate a simple self-training method based on expectation-maximization, which we call ReST$^{EM}$, where we (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. Testing on advanced MATH reasoning and APPS coding benchmarks using PaLM-2 models, we find that ReST$^{EM}$ scales favorably with model size and significantly surpasses fine-tuning only on human data. Overall, our findings suggest self-training with feedback can substantially reduce dependence on human-generated data.',\n",
       "  'venue': 'Trans. Mach. Learn. Res.',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 30,\n",
       "  'citationCount': 112,\n",
       "  'influentialCitationCount': 15,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2023-12-11',\n",
       "  'journal': {'name': 'Trans. Mach. Learn. Res.', 'volume': '2024'},\n",
       "  'citationStyles': {'bibtex': '@Article{Singh2023BeyondHD,\\n author = {Avi Singh and John D. Co-Reyes and Rishabh Agarwal and Ankesh Anand and Piyush Patil and Peter J. Liu and James Harrison and Jaehoon Lee and Kelvin Xu and Aaron T Parisi and Abhishek Kumar and A. Alemi and Alex Rizkowsky and Azade Nova and Ben Adlam and Bernd Bohnet and Hanie Sedghi and Igor Mordatch and Isabelle Simpson and Izzeddin Gur and Jasper Snoek and Jeffrey Pennington and Jiri Hron and Kathleen Kenealy and Kevin Swersky and Kshiteej Mahajan and Laura Culp and Lechao Xiao and Maxwell Bileschi and Noah Constant and Roman Novak and Rosanne Liu and Tris Warkentin and Yundi Qian and Ethan Dyer and Behnam Neyshabur and Jascha Narain Sohl-Dickstein and Noah Fiedel},\\n booktitle = {Trans. Mach. Learn. Res.},\\n journal = {Trans. Mach. Learn. Res.},\\n title = {Beyond Human Data: Scaling Self-Training for Problem-Solving with Language Models},\\n volume = {2024},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2258779676', 'name': 'Avi Singh'},\n",
       "   {'authorId': '2273515439', 'name': 'John D. Co-Reyes'},\n",
       "   {'authorId': '2258553001', 'name': 'Rishabh Agarwal'},\n",
       "   {'authorId': '12679121', 'name': 'Ankesh Anand'},\n",
       "   {'authorId': '2200033844', 'name': 'Piyush Patil'},\n",
       "   {'authorId': '2258936205', 'name': 'Peter J. Liu'},\n",
       "   {'authorId': '2297192581', 'name': 'James Harrison'},\n",
       "   {'authorId': '2253808003', 'name': 'Jaehoon Lee'},\n",
       "   {'authorId': '2266735761', 'name': 'Kelvin Xu'},\n",
       "   {'authorId': '2266464462', 'name': 'Aaron T Parisi'},\n",
       "   {'authorId': '2273472693', 'name': 'Abhishek Kumar'},\n",
       "   {'authorId': '2247711955', 'name': 'A. Alemi'},\n",
       "   {'authorId': '2266464436', 'name': 'Alex Rizkowsky'},\n",
       "   {'authorId': '2176782672', 'name': 'Azade Nova'},\n",
       "   {'authorId': '1874006', 'name': 'Ben Adlam'},\n",
       "   {'authorId': '2266464503', 'name': 'Bernd Bohnet'},\n",
       "   {'authorId': '2812848', 'name': 'Hanie Sedghi'},\n",
       "   {'authorId': '2224543226', 'name': 'Igor Mordatch'},\n",
       "   {'authorId': '2266465352', 'name': 'Isabelle Simpson'},\n",
       "   {'authorId': '3737312', 'name': 'Izzeddin Gur'},\n",
       "   {'authorId': '144108062', 'name': 'Jasper Snoek'},\n",
       "   {'authorId': '143845796', 'name': 'Jeffrey Pennington'},\n",
       "   {'authorId': '29785199', 'name': 'Jiri Hron'},\n",
       "   {'authorId': '1914502282', 'name': 'Kathleen Kenealy'},\n",
       "   {'authorId': '1754860', 'name': 'Kevin Swersky'},\n",
       "   {'authorId': '2266464729', 'name': 'Kshiteej Mahajan'},\n",
       "   {'authorId': '2219763699', 'name': 'Laura Culp'},\n",
       "   {'authorId': '50819275', 'name': 'Lechao Xiao'},\n",
       "   {'authorId': '2316127165', 'name': 'Maxwell Bileschi'},\n",
       "   {'authorId': '2254263662', 'name': 'Noah Constant'},\n",
       "   {'authorId': '39068839', 'name': 'Roman Novak'},\n",
       "   {'authorId': '2266712538', 'name': 'Rosanne Liu'},\n",
       "   {'authorId': '1986491804', 'name': 'Tris Warkentin'},\n",
       "   {'authorId': '2266486708', 'name': 'Yundi Qian'},\n",
       "   {'authorId': '52136425', 'name': 'Ethan Dyer'},\n",
       "   {'authorId': '3007442', 'name': 'Behnam Neyshabur'},\n",
       "   {'authorId': '1407546424', 'name': 'Jascha Narain Sohl-Dickstein'},\n",
       "   {'authorId': '22640071', 'name': 'Noah Fiedel'}]},\n",
       " {'paperId': '8292083dd8f6ae898ea0ee54a6b97997d1a51c9d',\n",
       "  'externalIds': {'ArXiv': '2408.03314',\n",
       "   'DBLP': 'journals/corr/abs-2408-03314',\n",
       "   'DOI': '10.48550/arXiv.2408.03314',\n",
       "   'CorpusId': 271719990},\n",
       "  'corpusId': 271719990,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/8292083dd8f6ae898ea0ee54a6b97997d1a51c9d',\n",
       "  'title': 'Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters',\n",
       "  'abstract': 'Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model\\'s distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a\"compute-optimal\"scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 32,\n",
       "  'citationCount': 232,\n",
       "  'influentialCitationCount': 23,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-08-06',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2408.03314'},\n",
       "  'citationStyles': {'bibtex': '@Article{Snell2024ScalingLT,\\n author = {Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters},\\n volume = {abs/2408.03314},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2314917835', 'name': 'Charlie Snell'},\n",
       "   {'authorId': '2315069693', 'name': 'Jaehoon Lee'},\n",
       "   {'authorId': '2269023101', 'name': 'Kelvin Xu'},\n",
       "   {'authorId': '2275526115', 'name': 'Aviral Kumar'}]},\n",
       " {'paperId': '6c17de3e719c0f4d1df6a16f770c2a9a5f18206f',\n",
       "  'externalIds': {'DBLP': 'journals/corr/abs-2404-12253',\n",
       "   'ArXiv': '2404.12253',\n",
       "   'DOI': '10.48550/arXiv.2404.12253',\n",
       "   'CorpusId': 269214525},\n",
       "  'corpusId': 269214525,\n",
       "  'publicationVenue': {'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd',\n",
       "   'name': 'Neural Information Processing Systems',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'],\n",
       "   'url': 'http://neurips.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/6c17de3e719c0f4d1df6a16f770c2a9a5f18206f',\n",
       "  'title': 'Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing',\n",
       "  'abstract': \"Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 67,\n",
       "  'citationCount': 42,\n",
       "  'influentialCitationCount': 5,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-04-18',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2404.12253'},\n",
       "  'citationStyles': {'bibtex': '@Article{Tian2024TowardSO,\\n author = {Ye Tian and Baolin Peng and Linfeng Song and Lifeng Jin and Dian Yu and Haitao Mi and Dong Yu},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing},\\n volume = {abs/2404.12253},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2243391254', 'name': 'Ye Tian'},\n",
       "   {'authorId': '2284066568', 'name': 'Baolin Peng'},\n",
       "   {'authorId': '50258954', 'name': 'Linfeng Song'},\n",
       "   {'authorId': '50496698', 'name': 'Lifeng Jin'},\n",
       "   {'authorId': '2273685776', 'name': 'Dian Yu'},\n",
       "   {'authorId': '2238955130', 'name': 'Haitao Mi'},\n",
       "   {'authorId': '2239076081', 'name': 'Dong Yu'}]},\n",
       " {'paperId': 'e526a65b9ef5afb6639fd3a062f4045d24448232',\n",
       "  'externalIds': {'DBLP': 'journals/ml/Williams92',\n",
       "   'MAG': '2119717200',\n",
       "   'DOI': '10.1007/BF00992696',\n",
       "   'CorpusId': 19115634},\n",
       "  'corpusId': 19115634,\n",
       "  'publicationVenue': {'id': '22c9862f-a25e-40cd-9d31-d09e68a293e6',\n",
       "   'name': 'Machine-mediated learning',\n",
       "   'type': 'journal',\n",
       "   'alternate_names': ['Mach learn', 'Machine Learning', 'Mach Learn'],\n",
       "   'issn': '0732-6718',\n",
       "   'alternate_issns': ['0885-6125'],\n",
       "   'url': 'http://www.springer.com/computer/artificial/journal/10994',\n",
       "   'alternate_urls': ['https://link.springer.com/journal/10994',\n",
       "    'http://www.springer.com/west/home/computer/artificial?SGWID=4-147-70-35726603-0']},\n",
       "  'url': 'https://www.semanticscholar.org/paper/e526a65b9ef5afb6639fd3a062f4045d24448232',\n",
       "  'title': 'Simple statistical gradient-following algorithms for connectionist reinforcement learning',\n",
       "  'abstract': None,\n",
       "  'venue': 'Machine-mediated learning',\n",
       "  'year': 1992,\n",
       "  'referenceCount': 34,\n",
       "  'citationCount': 963,\n",
       "  'influentialCitationCount': 106,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://link.springer.com/content/pdf/10.1007/BF00992696.pdf',\n",
       "   'status': 'BRONZE'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '1992-05-01',\n",
       "  'journal': {'name': 'Machine Learning', 'pages': '229-256', 'volume': '8'},\n",
       "  'citationStyles': {'bibtex': '@Article{Williams1992SimpleSG,\\n author = {Ronald J. Williams},\\n booktitle = {Machine-mediated learning},\\n journal = {Machine Learning},\\n pages = {229-256},\\n title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},\\n volume = {8},\\n year = {1992}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2116648700', 'name': 'Ronald J. Williams'}]},\n",
       " {'paperId': '38333f6e8f0388968edc4b2ea7a683ce69677e69',\n",
       "  'externalIds': {'ArXiv': '2405.00451',\n",
       "   'DBLP': 'journals/corr/abs-2405-00451',\n",
       "   'DOI': '10.48550/arXiv.2405.00451',\n",
       "   'CorpusId': 269484186},\n",
       "  'corpusId': 269484186,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/38333f6e8f0388968edc4b2ea7a683ce69677e69',\n",
       "  'title': 'Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning',\n",
       "  'abstract': 'We introduce an approach aimed at enhancing the reasoning capabilities of Large Language Models (LLMs) through an iterative preference learning process inspired by the successful strategy employed by AlphaZero. Our work leverages Monte Carlo Tree Search (MCTS) to iteratively collect preference data, utilizing its look-ahead ability to break down instance-level rewards into more granular step-level signals. To enhance consistency in intermediate steps, we combine outcome validation and stepwise self-evaluation, continually updating the quality assessment of newly generated data. The proposed algorithm employs Direct Preference Optimization (DPO) to update the LLM policy using this newly generated step-level preference data. Theoretical analysis reveals the importance of using on-policy sampled data for successful self-improving. Extensive evaluations on various arithmetic and commonsense reasoning tasks demonstrate remarkable performance improvements over existing models. For instance, our approach outperforms the Mistral-7B Supervised Fine-Tuning (SFT) baseline on GSM8K, MATH, and ARC-C, with substantial increases in accuracy to $81.8\\\\%$ (+$5.9\\\\%$), $34.7\\\\%$ (+$5.8\\\\%$), and $76.4\\\\%$ (+$15.8\\\\%$), respectively. Additionally, our research delves into the training and inference compute tradeoff, providing insights into how our method effectively maximizes performance gains. Our code is publicly available at https://github.com/YuxiXie/MCTS-DPO.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 60,\n",
       "  'citationCount': 51,\n",
       "  'influentialCitationCount': 7,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-05-01',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2405.00451'},\n",
       "  'citationStyles': {'bibtex': '@Article{Xie2024MonteCT,\\n author = {Yuxi Xie and Anirudh Goyal and Wenyue Zheng and Min-Yen Kan and T. Lillicrap and Kenji Kawaguchi and Michael Shieh},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning},\\n volume = {abs/2405.00451},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2299483768', 'name': 'Yuxi Xie'},\n",
       "   {'authorId': '1996705', 'name': 'Anirudh Goyal'},\n",
       "   {'authorId': '2289857220', 'name': 'Wenyue Zheng'},\n",
       "   {'authorId': '2257033898', 'name': 'Min-Yen Kan'},\n",
       "   {'authorId': '2542999', 'name': 'T. Lillicrap'},\n",
       "   {'authorId': '2266466003', 'name': 'Kenji Kawaguchi'},\n",
       "   {'authorId': '2289844602', 'name': 'Michael Shieh'}]},\n",
       " {'paperId': '4b516216d7d150a081fd74993bddf36b6b22c118',\n",
       "  'externalIds': {'ArXiv': '2205.10816',\n",
       "   'DBLP': 'conf/nips/YangSAN22',\n",
       "   'DOI': '10.48550/arXiv.2205.10816',\n",
       "   'CorpusId': 248986984},\n",
       "  'corpusId': 248986984,\n",
       "  'publicationVenue': {'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd',\n",
       "   'name': 'Neural Information Processing Systems',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'],\n",
       "   'url': 'http://neurips.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/4b516216d7d150a081fd74993bddf36b6b22c118',\n",
       "  'title': 'Chain of Thought Imitation with Procedure Cloning',\n",
       "  'abstract': \"Imitation learning aims to extract high-performance policies from logged demonstrations of expert behavior. It is common to frame imitation learning as a supervised learning problem in which one fits a function approximator to the input-output mapping exhibited by the logged demonstrations (input observations to output actions). While the framing of imitation learning as a supervised input-output learning problem allows for applicability in a wide variety of settings, it is also an overly simplistic view of the problem in situations where the expert demonstrations provide much richer insight into expert behavior. For example, applications such as path navigation, robot manipulation, and strategy games acquire expert demonstrations via planning, search, or some other multi-step algorithm, revealing not just the output action to be imitated but also the procedure for how to determine this action. While these intermediate computations may use tools not available to the agent during inference (e.g., environment simulators), they are nevertheless informative as a way to explain an expert's mapping of state to actions. To properly leverage expert procedure information without relying on the privileged tools the expert may have used to perform the procedure, we propose procedure cloning, which applies supervised sequence prediction to imitate the series of expert computations. This way, procedure cloning learns not only what to do (i.e., the output action), but how and why to do it (i.e., the procedure). Through empirical analysis on navigation, simulated robotic manipulation, and game-playing environments, we show that imitating the intermediate computations of an expert's behavior enables procedure cloning to learn policies exhibiting significant generalization to unseen environment configurations, including those configurations for which running the expert's procedure directly is infeasible.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2022,\n",
       "  'referenceCount': 101,\n",
       "  'citationCount': 25,\n",
       "  'influentialCitationCount': 2,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2205.10816',\n",
       "   'status': 'GREEN'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2022-05-22',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2205.10816'},\n",
       "  'citationStyles': {'bibtex': '@Article{Yang2022ChainOT,\\n author = {Mengjiao Yang and D. Schuurmans and P. Abbeel and Ofir Nachum},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {Chain of Thought Imitation with Procedure Cloning},\\n volume = {abs/2205.10816},\\n year = {2022}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2111076891', 'name': 'Mengjiao Yang'},\n",
       "   {'authorId': '50319359', 'name': 'D. Schuurmans'},\n",
       "   {'authorId': '1689992', 'name': 'P. Abbeel'},\n",
       "   {'authorId': '7624658', 'name': 'Ofir Nachum'}]},\n",
       " {'paperId': '2f3822eb380b5e753a6d579f31dfc3ec4c4a0820',\n",
       "  'externalIds': {'ArXiv': '2305.10601',\n",
       "   'DBLP': 'conf/nips/YaoYZS00N23',\n",
       "   'DOI': '10.48550/arXiv.2305.10601',\n",
       "   'CorpusId': 258762525},\n",
       "  'corpusId': 258762525,\n",
       "  'publicationVenue': {'id': 'd9720b90-d60b-48bc-9df8-87a30b9a60dd',\n",
       "   'name': 'Neural Information Processing Systems',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['Neural Inf Process Syst', 'NeurIPS', 'NIPS'],\n",
       "   'url': 'http://neurips.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820',\n",
       "  'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models',\n",
       "  'abstract': \"Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.\",\n",
       "  'venue': 'Neural Information Processing Systems',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 52,\n",
       "  'citationCount': 1356,\n",
       "  'influentialCitationCount': 123,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'http://arxiv.org/pdf/2305.10601',\n",
       "   'status': 'GREEN'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2023-05-17',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2305.10601'},\n",
       "  'citationStyles': {'bibtex': '@Article{Yao2023TreeOT,\\n author = {Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},\\n booktitle = {Neural Information Processing Systems},\\n journal = {ArXiv},\\n title = {Tree of Thoughts: Deliberate Problem Solving with Large Language Models},\\n volume = {abs/2305.10601},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2093302161', 'name': 'Shunyu Yao'},\n",
       "   {'authorId': '150978762', 'name': 'Dian Yu'},\n",
       "   {'authorId': '2144551262', 'name': 'Jeffrey Zhao'},\n",
       "   {'authorId': '1697494', 'name': 'Izhak Shafran'},\n",
       "   {'authorId': '1799860', 'name': 'T. Griffiths'},\n",
       "   {'authorId': '145144022', 'name': 'Yuan Cao'},\n",
       "   {'authorId': '144958935', 'name': 'Karthik Narasimhan'}]},\n",
       " {'paperId': 'fec3d7e1272d640eddd55307127bebf4fa55ac33',\n",
       "  'externalIds': {'ArXiv': '2407.20311',\n",
       "   'DBLP': 'journals/corr/abs-2407-20311',\n",
       "   'DOI': '10.48550/arXiv.2407.20311',\n",
       "   'CorpusId': 271544257},\n",
       "  'corpusId': 271544257,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/fec3d7e1272d640eddd55307127bebf4fa55ac33',\n",
       "  'title': 'Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process',\n",
       "  'abstract': \"Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model's hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions? Our study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 0,\n",
       "  'citationCount': 28,\n",
       "  'influentialCitationCount': 2,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Physics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-07-29',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2407.20311'},\n",
       "  'citationStyles': {'bibtex': '@Article{Ye2024PhysicsOL,\\n author = {Tian Ye and Zicheng Xu and Yuanzhi Li and Zeyuan Allen-Zhu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process},\\n volume = {abs/2407.20311},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2313730228', 'name': 'Tian Ye'},\n",
       "   {'authorId': '2313878436', 'name': 'Zicheng Xu'},\n",
       "   {'authorId': '2110486765', 'name': 'Yuanzhi Li'},\n",
       "   {'authorId': '1388725932', 'name': 'Zeyuan Allen-Zhu'}]},\n",
       " {'paperId': '389d2ce6611a8501f08c5aae956df6ddb420d1d2',\n",
       "  'externalIds': {'ArXiv': '2408.16293',\n",
       "   'DBLP': 'journals/corr/abs-2408-16293',\n",
       "   'DOI': '10.48550/arXiv.2408.16293',\n",
       "   'CorpusId': 272146356},\n",
       "  'corpusId': 272146356,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/389d2ce6611a8501f08c5aae956df6ddb420d1d2',\n",
       "  'title': 'Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems',\n",
       "  'abstract': 'Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to\"self-correct\"their mistakes via multi-round prompting. In this paper, we follow this line of work but focus on understanding the usefulness of incorporating\"error-correction\"data directly into the pretraining stage. This data consists of erroneous solution steps immediately followed by their corrections. Using a synthetic math dataset, we show promising results: this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting) compared to pretraining on the same amount of error-free data. We also delve into many details, such as (1) how this approach differs from beam search, (2) how such data can be prepared, (3) whether masking is needed on the erroneous tokens, (4) the amount of error required, (5) whether such data can be deferred to the fine-tuning stage, and many others.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 0,\n",
       "  'citationCount': 15,\n",
       "  'influentialCitationCount': 2,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Physics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-08-29',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2408.16293'},\n",
       "  'citationStyles': {'bibtex': '@Article{Ye2024PhysicsOL,\\n author = {Tian Ye and Zicheng Xu and Yuanzhi Li and Zeyuan Allen-Zhu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems},\\n volume = {abs/2408.16293},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2313730228', 'name': 'Tian Ye'},\n",
       "   {'authorId': '2313878436', 'name': 'Zicheng Xu'},\n",
       "   {'authorId': '2110486765', 'name': 'Yuanzhi Li'},\n",
       "   {'authorId': '1388725932', 'name': 'Zeyuan Allen-Zhu'}]},\n",
       " {'paperId': 'ac62f95e60a42bbceacfb390abecd95659947bc1',\n",
       "  'externalIds': {'ArXiv': '2410.02052',\n",
       "   'DBLP': 'journals/corr/abs-2410-02052',\n",
       "   'DOI': '10.48550/arXiv.2410.02052',\n",
       "   'CorpusId': 273098809},\n",
       "  'corpusId': 273098809,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/ac62f95e60a42bbceacfb390abecd95659947bc1',\n",
       "  'title': 'ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning',\n",
       "  'abstract': \"Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon tasks. To address these limitations, we present ExACT, an approach to combine test-time search and self-learning to build o1-like models for agentic applications. We first introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test time algorithm designed to enhance AI agents' ability to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate for reliable state evaluation. Next, we introduce Exploratory Learning, a novel learning strategy to teach agents to search at inference time without relying on any external search algorithms. On the challenging VisualWebArena benchmark, our GPT-4o based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge and experience gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. After Exploratory Learning, GPT-4o 1) demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success, and 2) matches 87% of R-MCTS's performance while using significantly less compute. Notably, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' capabilities for agentic applications via test-time search and self-learning.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 43,\n",
       "  'citationCount': 6,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-10-02',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2410.02052'},\n",
       "  'citationStyles': {'bibtex': '@Article{Yu2024ExACTTA,\\n author = {Xiao Yu and Baolin Peng and Vineeth Vajipey and Hao Cheng and Michel Galley and Jianfeng Gao and Zhou Yu},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning},\\n volume = {abs/2410.02052},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2261083958', 'name': 'Xiao Yu'},\n",
       "   {'authorId': '1780690', 'name': 'Baolin Peng'},\n",
       "   {'authorId': '2324054869', 'name': 'Vineeth Vajipey'},\n",
       "   {'authorId': '2280195943', 'name': 'Hao Cheng'},\n",
       "   {'authorId': '2253458981', 'name': 'Michel Galley'},\n",
       "   {'authorId': '2256752511', 'name': 'Jianfeng Gao'},\n",
       "   {'authorId': '2261255943', 'name': 'Zhou Yu'}]},\n",
       " {'paperId': '91206346edbe28abb606d7b3425cd455d4019d4f',\n",
       "  'externalIds': {'ArXiv': '2308.01825',\n",
       "   'DBLP': 'journals/corr/abs-2308-01825',\n",
       "   'DOI': '10.48550/arXiv.2308.01825',\n",
       "   'CorpusId': 260438790},\n",
       "  'corpusId': 260438790,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/91206346edbe28abb606d7b3425cd455d4019d4f',\n",
       "  'title': 'Scaling Relationship on Learning Mathematical Reasoning with Large Language Models',\n",
       "  'abstract': \"Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\\\\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\\\\% significantly.\",\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 54,\n",
       "  'citationCount': 138,\n",
       "  'influentialCitationCount': 19,\n",
       "  'isOpenAccess': True,\n",
       "  'openAccessPdf': {'url': 'https://arxiv.org/pdf/2308.01825',\n",
       "   'status': 'GREEN'},\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Mathematics', 'source': 's2-fos-model'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2023-08-03',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2308.01825'},\n",
       "  'citationStyles': {'bibtex': '@Article{Yuan2023ScalingRO,\\n author = {Zheng Yuan and Hongyi Yuan and Cheng Li and Guanting Dong and Chuanqi Tan and Chang Zhou},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Scaling Relationship on Learning Mathematical Reasoning with Large Language Models},\\n volume = {abs/2308.01825},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2112340945', 'name': 'Zheng Yuan'},\n",
       "   {'authorId': '2114128654', 'name': 'Hongyi Yuan'},\n",
       "   {'authorId': '1753450171', 'name': 'Cheng Li'},\n",
       "   {'authorId': '51490462', 'name': 'Guanting Dong'},\n",
       "   {'authorId': '2111727840', 'name': 'Chuanqi Tan'},\n",
       "   {'authorId': '2192678144', 'name': 'Chang Zhou'}]},\n",
       " {'paperId': '3707939a856655fcabf0acd5cba1a1009987b439',\n",
       "  'externalIds': {'ArXiv': '2408.15240',\n",
       "   'DBLP': 'journals/corr/abs-2408-15240',\n",
       "   'DOI': '10.48550/arXiv.2408.15240',\n",
       "   'CorpusId': 271963324},\n",
       "  'corpusId': 271963324,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/3707939a856655fcabf0acd5cba1a1009987b439',\n",
       "  'title': 'Generative Verifiers: Reward Modeling as Next-Token Prediction',\n",
       "  'abstract': 'Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, we instead propose training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional test-time compute via majority voting for better verification. We demonstrate that GenRM outperforms discriminative, DPO verifiers, and LLM-as-a-Judge, resulting in large performance gains with Best-of-N, namely 5% $\\\\rightarrow$ 45.3% on algorithmic tasks and 73% $\\\\rightarrow$ 93.4% on GSM8K. In easy-to-hard generalization settings, we observe improvements of 28% $\\\\rightarrow$ 44.6% on MATH, and 37.9% $\\\\rightarrow$ 53.5% on MMLU abstract algebra. Furthermore, we find that training GenRM with synthetic verification rationales is sufficient to pick out subtle errors on math problems. Finally, we demonstrate that GenRM scales favorably with model size and test-time compute.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 61,\n",
       "  'citationCount': 65,\n",
       "  'influentialCitationCount': 1,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-08-27',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2408.15240'},\n",
       "  'citationStyles': {'bibtex': '@Article{Zhang2024GenerativeVR,\\n author = {Lunjun Zhang and Arian Hosseini and Hritik Bansal and Mehran Kazemi and Aviral Kumar and Rishabh Agarwal},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Generative Verifiers: Reward Modeling as Next-Token Prediction},\\n volume = {abs/2408.15240},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2318804131', 'name': 'Lunjun Zhang'},\n",
       "   {'authorId': '2090537547', 'name': 'Arian Hosseini'},\n",
       "   {'authorId': '2317010356', 'name': 'Hritik Bansal'},\n",
       "   {'authorId': '2317010095', 'name': 'Mehran Kazemi'},\n",
       "   {'authorId': '2317038858', 'name': 'Aviral Kumar'},\n",
       "   {'authorId': '2317013277', 'name': 'Rishabh Agarwal'}]},\n",
       " {'paperId': '75732edda18744c1027ed766ec9579d27a3a59a2',\n",
       "  'externalIds': {'ArXiv': '2409.14586',\n",
       "   'DBLP': 'journals/corr/abs-2409-14586',\n",
       "   'DOI': '10.48550/arXiv.2409.14586',\n",
       "   'CorpusId': 272826716},\n",
       "  'corpusId': 272826716,\n",
       "  'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10',\n",
       "   'name': 'arXiv.org',\n",
       "   'alternate_names': ['ArXiv'],\n",
       "   'issn': '2331-8422',\n",
       "   'url': 'https://arxiv.org'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/75732edda18744c1027ed766ec9579d27a3a59a2',\n",
       "  'title': 'Backtracking Improves Generation Safety',\n",
       "  'abstract': 'Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic. In the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text. This is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety. Deviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to\"undo\"and recover from their own unsafe generation through the introduction of a special [RESET] token. Our method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness. We show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1\\\\% $\\\\to$ 1.5\\\\%) in our evaluations without regression in helpfulness. Our method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so.',\n",
       "  'venue': 'arXiv.org',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 52,\n",
       "  'citationCount': 3,\n",
       "  'influentialCitationCount': 0,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle'],\n",
       "  'publicationDate': '2024-09-22',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2409.14586'},\n",
       "  'citationStyles': {'bibtex': '@Article{Zhang2024BacktrackingIG,\\n author = {Yiming Zhang and Jianfeng Chi and Hailey Nguyen and K. Upasani and Daniel M. Bikel and Jason Weston and Eric Michael Smith},\\n booktitle = {arXiv.org},\\n journal = {ArXiv},\\n title = {Backtracking Improves Generation Safety},\\n volume = {abs/2409.14586},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '49889218', 'name': 'Yiming Zhang'},\n",
       "   {'authorId': '31357678', 'name': 'Jianfeng Chi'},\n",
       "   {'authorId': '2314075528', 'name': 'Hailey Nguyen'},\n",
       "   {'authorId': '17097160', 'name': 'K. Upasani'},\n",
       "   {'authorId': '2265492068', 'name': 'Daniel M. Bikel'},\n",
       "   {'authorId': '2322443044', 'name': 'Jason Weston'},\n",
       "   {'authorId': '2268821751', 'name': 'Eric Michael Smith'}]},\n",
       " {'paperId': '700bd9681f1b9e9e2212e10415d27b11c7e6836b',\n",
       "  'externalIds': {'ArXiv': '2310.04406',\n",
       "   'DBLP': 'conf/icml/ZhouYSWW24',\n",
       "   'DOI': '10.48550/arXiv.2310.04406',\n",
       "   'CorpusId': 263829963},\n",
       "  'corpusId': 263829963,\n",
       "  'publicationVenue': {'id': 'fc0a208c-acb7-47dc-a0d4-af8190e21d29',\n",
       "   'name': 'International Conference on Machine Learning',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['ICML', 'Int Conf Mach Learn'],\n",
       "   'url': 'https://icml.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/700bd9681f1b9e9e2212e10415d27b11c7e6836b',\n",
       "  'title': 'Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models',\n",
       "  'abstract': 'While language models (LMs) have shown potential across a range of decision-making tasks, their reliance on simple acting processes limits their broad deployment as autonomous agents. In this paper, we introduce Language Agent Tree Search (LATS) -- the first general framework that synergizes the capabilities of LMs in reasoning, acting, and planning. By leveraging the in-context learning ability of LMs, we integrate Monte Carlo Tree Search into LATS to enable LMs as agents, along with LM-powered value functions and self-reflections for proficient exploration and enhanced decision-making. A key feature of our approach is the incorporation of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that surpasses the constraints of existing techniques. Our experimental evaluation across diverse domains, including programming, interactive question-answering (QA), web navigation, and math, validates the effectiveness and generality of LATS in decision-making while maintaining competitive or improved reasoning performance. Notably, LATS achieves state-of-the-art pass@1 accuracy (92.7%) for programming on HumanEval with GPT-4 and demonstrates gradient-free performance (average score of 75.9) comparable to gradient-based fine-tuning for web navigation on WebShop with GPT-3.5. Code can be found at https://github.com/lapisrocks/LanguageAgentTreeSearch',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2023,\n",
       "  'referenceCount': 84,\n",
       "  'citationCount': 129,\n",
       "  'influentialCitationCount': 11,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'},\n",
       "   {'category': 'Linguistics', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle', 'Conference'],\n",
       "  'publicationDate': '2023-10-06',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2310.04406'},\n",
       "  'citationStyles': {'bibtex': '@Article{Zhou2023LanguageAT,\\n author = {Andy Zhou and Kai Yan and Michal Shlapentokh-Rothman and Haohan Wang and Yu-Xiong Wang},\\n booktitle = {International Conference on Machine Learning},\\n journal = {ArXiv},\\n title = {Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models},\\n volume = {abs/2310.04406},\\n year = {2023}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2244620107', 'name': 'Andy Zhou'},\n",
       "   {'authorId': '2256998882', 'name': 'Kai Yan'},\n",
       "   {'authorId': '2256998400', 'name': 'Michal Shlapentokh-Rothman'},\n",
       "   {'authorId': '2256769308', 'name': 'Haohan Wang'},\n",
       "   {'authorId': '2257110226', 'name': 'Yu-Xiong Wang'}]},\n",
       " {'paperId': '668858489bbec3ce45f7a84a6a557b329f9ec91a',\n",
       "  'externalIds': {'DBLP': 'conf/icml/ZhouZPLK24',\n",
       "   'ArXiv': '2402.19446',\n",
       "   'DOI': '10.48550/arXiv.2402.19446',\n",
       "   'CorpusId': 268091206},\n",
       "  'corpusId': 268091206,\n",
       "  'publicationVenue': {'id': 'fc0a208c-acb7-47dc-a0d4-af8190e21d29',\n",
       "   'name': 'International Conference on Machine Learning',\n",
       "   'type': 'conference',\n",
       "   'alternate_names': ['ICML', 'Int Conf Mach Learn'],\n",
       "   'url': 'https://icml.cc/'},\n",
       "  'url': 'https://www.semanticscholar.org/paper/668858489bbec3ce45f7a84a6a557b329f9ec91a',\n",
       "  'title': 'ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL',\n",
       "  'abstract': 'A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or\"agent\"tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).',\n",
       "  'venue': 'International Conference on Machine Learning',\n",
       "  'year': 2024,\n",
       "  'referenceCount': 65,\n",
       "  'citationCount': 31,\n",
       "  'influentialCitationCount': 3,\n",
       "  'isOpenAccess': False,\n",
       "  'openAccessPdf': None,\n",
       "  'fieldsOfStudy': ['Computer Science'],\n",
       "  's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'},\n",
       "   {'category': 'Computer Science', 'source': 's2-fos-model'}],\n",
       "  'publicationTypes': ['JournalArticle', 'Conference'],\n",
       "  'publicationDate': '2024-02-29',\n",
       "  'journal': {'name': 'ArXiv', 'volume': 'abs/2402.19446'},\n",
       "  'citationStyles': {'bibtex': '@Article{Zhou2024ArCHerTL,\\n author = {Yifei Zhou and Andrea Zanette and Jiayi Pan and Sergey Levine and Aviral Kumar},\\n booktitle = {International Conference on Machine Learning},\\n journal = {ArXiv},\\n title = {ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL},\\n volume = {abs/2402.19446},\\n year = {2024}\\n}\\n'},\n",
       "  'authors': [{'authorId': '2288585404', 'name': 'Yifei Zhou'},\n",
       "   {'authorId': '2288537572', 'name': 'Andrea Zanette'},\n",
       "   {'authorId': '2152946868', 'name': 'Jiayi Pan'},\n",
       "   {'authorId': '2268967207', 'name': 'Sergey Levine'},\n",
       "   {'authorId': '1488785534', 'name': 'Aviral Kumar'}]}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_meatadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.align_content_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.align_reference_info(reference_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_json_rvsd_path = os.path.join(processed_file_path, \"processed_content_list.json\")\n",
    "with open(pdf_json_rvsd_path, \"w\") as file:\n",
    "    json.dump(pdf.pdf_json, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "pdf_json_rvsd_path = \"/home/jiezi/Code/GitHub/PaperReadThrough/data/2501.04682v1/processed_content_list.json\"\n",
    "with open(pdf_json_rvsd_path, \"r\") as file:\n",
    "    pdf_json_rvsd = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name patterns for image / table / equation names\n",
    "IMG_REGX_NAME_PTRN = r\"(pic|picture|img|image|chart|figure|fig|table|tbl)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "TBL_REGX_NAME_PTRN = r\"(tbl|table|chart|figure|fig)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\"\n",
    "EQT_REGX_NAME_PTRN = r\"(formula|equation|notation|syntax)\\s*([0-9]+(?:\\.[0-9]+)?|[0-9]+|[IVXLCDM]+|[a-zA-Z]+)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDFSeg:\n",
    "    def __init__(self, pdf_json):\n",
    "        self.pdf_json = pdf_json\n",
    "\n",
    "    def get_toc_hierachy(self):\n",
    "        \"\"\"generate ToC tree\n",
    "        Args:\n",
    "            pdf_json:\n",
    "        Returns:\n",
    "            tree form hierachy of sections\n",
    "        \"\"\"\n",
    "        toc_hierachy = []\n",
    "        section_stack = []\n",
    "\n",
    "        for i, item in enumerate(self.pdf_json):\n",
    "            if item['type'] == 'title':\n",
    "                level = item['text_level']\n",
    "                title = item['text']\n",
    "\n",
    "                while section_stack and section_stack[-1]['level'] >= level:\n",
    "                    popped_section = section_stack.pop()\n",
    "                    popped_section['end_position'] = i - 1\n",
    "                    if section_stack:\n",
    "                        section_stack[-1]['subsection'].append(popped_section)\n",
    "                    else:\n",
    "                        toc_hierachy.append(popped_section)\n",
    "\n",
    "                new_section = {'title': title, 'level': level, 'start_position': i, 'end_position': -1, 'subsection': []}\n",
    "                section_stack.append(new_section)\n",
    "\n",
    "        while section_stack:\n",
    "            popped_section = section_stack.pop()\n",
    "            popped_section['end_position'] = len(self.pdf_json) - 1\n",
    "            if section_stack:\n",
    "                section_stack[-1]['subsection'].append(popped_section)\n",
    "            else:\n",
    "                toc_hierachy.append(popped_section)\n",
    "\n",
    "        return toc_hierachy\n",
    "    \n",
    "    def gen_seg_paras(self, toc_hierachy, seg_text_length:Optional[int]=20000):\n",
    "        \"\"\"segment content json based on toc hierachy\"\"\"\n",
    "        pdf_texts = [item.get('text', '') for item in self.pdf_json]\n",
    "\n",
    "        all_seg_paras = []\n",
    "        for section in toc_hierachy:\n",
    "            section_paras = []\n",
    "            \n",
    "            start_pos = section['start_position']\n",
    "            end_pos = section['end_position']\n",
    "            tmp_text = \"\\n\".join(pdf_texts[start_pos:end_pos+1])\n",
    "            \n",
    "            if len(tmp_text) > seg_text_length and section.get('subsection', []) != []:\n",
    "                # if the section is too long, then breakdown to subsection\n",
    "                for subsection in section.get('subsection'):\n",
    "                    sub_start_pos = subsection['start_position']\n",
    "                    sub_end_pos = subsection['end_position']\n",
    "                    section_paras.append(self.pdf_json[sub_start_pos:sub_end_pos+1])\n",
    "                    tmp_text = \"\\n\".join(pdf_texts[sub_start_pos:sub_end_pos+1])\n",
    "                    print('subsection', subsection.get('title'), len(tmp_text))\n",
    "            else:\n",
    "                section_paras.append(self.pdf_json[start_pos:end_pos+1])\n",
    "                print('section', section.get('title'), len(tmp_text))\n",
    "                    \n",
    "            all_seg_paras.extend(section_paras)\n",
    "        return all_seg_paras\n",
    "\n",
    "    def gen_md_from_json(self, content_json):\n",
    "        \"\"\"input json with predefined format and convert to markdown\"\"\"\n",
    "        md_text = \"\"\n",
    "        if len(content_json) > 0:\n",
    "            for item in content_json:\n",
    "                if item.get('type') == 'title':\n",
    "                    md_text += f\"{'#'*item.get('text_level')} {item.get('text')}  \\n\" \n",
    "\n",
    "                elif item.get('type') in ['image']:\n",
    "                    alt_text = \"\\n\".join(item.get('img_caption', [])) \n",
    "                    md_text += f\"\\n![{alt_text}]({item.get('img_path')} '{item.get('id')}')  \\n\"  \n",
    "                    md_text += \"\\n\".join(item.get('img_footnote'), []) \n",
    "\n",
    "                elif item.get('type') in ['table']:\n",
    "                    alt_text = \"\\n\".join(item.get('table_caption', [])) \n",
    "                    md_text += f\"\\n![{alt_text}]({item.get('img_path')} '{item.get('id')}')  \\n\"  \n",
    "                    md_text += \"\\n\".join(item.get('table_footnote'), []) \n",
    "\n",
    "                elif item.get('type') in ['equation']:\n",
    "                    md_text += f\"\"\"```latex\\n{item.get('text')}\\n```\"\"\"\n",
    "\n",
    "                elif item.get('type') in ['text', 'reference']:\n",
    "                    md_text += f\"{item.get('text')}  \\n\"  \n",
    "        return md_text\n",
    "    \n",
    "\n",
    "    def restore_seg_elements(self, seg_paras):\n",
    "        \"\"\"put all elements (images, tables, equations, refs) metioned in place where the refered to\"\"\"\n",
    "\n",
    "        img_lst = [x for x in self.pdf_json if x.get('type')=='image']\n",
    "        tbl_lst = [x for x in self.pdf_json if x.get('type')=='table']\n",
    "        eqt_lst = [x for x in self.pdf_json if x.get('type')=='equation']\n",
    "        ref_lst = [x for x in self.pdf_json if x.get('type')=='reference']\n",
    "\n",
    "        seg_paras_rvsd = []\n",
    "        for seg in seg_paras:\n",
    "            seg_img_lst = [x for x in seg if x.get('type')=='image']\n",
    "            seg_tbl_lst = [x for x in seg if x.get('type')=='table']\n",
    "            seg_eqt_lst = [x for x in seg if x.get('type')=='equation']\n",
    "            seg_ref_lst = [x for x in seg if x.get('type')=='reference']\n",
    "\n",
    "            for item in seg:\n",
    "                if item.get('if_being_reffered') is None:\n",
    "                    item_text = item.get('text', '')\n",
    "\n",
    "                    mtch_rslts = re.finditer(IMG_REGX_NAME_PTRN, item_text, re.IGNORECASE)\n",
    "                    for match in mtch_rslts:\n",
    "                        img_id = match.group(0)\n",
    "                        if img_id not in [x.get('id') for x in seg_img_lst]:\n",
    "                            added_items = [x for x in img_lst if x.get('id')==img_id]\n",
    "                            print(added_items)\n",
    "                            for y in added_items:\n",
    "                                y['if_being_reffered'] = True\n",
    "                            seg_img_lst.extend(added_items)\n",
    "                            seg.extend(added_items)\n",
    "\n",
    "                    mtch_rslts = re.finditer(TBL_REGX_NAME_PTRN, item_text, re.IGNORECASE)\n",
    "                    for match in mtch_rslts:\n",
    "                        tbl_id = match.group(0)\n",
    "                        if tbl_id not in [x.get('id') for x in seg_tbl_lst]:\n",
    "                            added_items = [x for x in tbl_lst if x.get('id')==tbl_id]\n",
    "                            for y in added_items:\n",
    "                                y['if_being_reffered'] = True\n",
    "                            seg_tbl_lst.extend(added_items)\n",
    "                            seg.extend(added_items)\n",
    "\n",
    "                    mtch_rslts = re.finditer(EQT_REGX_NAME_PTRN, item_text, re.IGNORECASE)\n",
    "                    for match in mtch_rslts:\n",
    "                        eqt_id = match.group(0)\n",
    "                        if eqt_id not in [x.get('id') for x in seg_eqt_lst]:\n",
    "                            added_items = [x for x in eqt_lst if x.get('id')==eqt_id]\n",
    "                            for y in added_items:\n",
    "                                y['if_being_reffered'] = True\n",
    "                            seg_eqt_lst.extend(added_items)\n",
    "                            seg.extend(added_items)\n",
    "            seg_paras_rvsd.append(seg)\n",
    "        \n",
    "        return seg_paras_rvsd\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = PDFSeg(pdf_json_rvsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_hierachy = seg.get_toc_hierachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '1. Introduction ',\n",
       "  'level': 1,\n",
       "  'start_position': 18,\n",
       "  'end_position': 55,\n",
       "  'subsection': [{'title': '1.1. Motivation ',\n",
       "    'level': 2,\n",
       "    'start_position': 19,\n",
       "    'end_position': 51,\n",
       "    'subsection': []},\n",
       "   {'title': '1.2. Outline ',\n",
       "    'level': 2,\n",
       "    'start_position': 52,\n",
       "    'end_position': 55,\n",
       "    'subsection': []}]},\n",
       " {'title': '2. Meta Chain-Of-Thought ',\n",
       "  'level': 1,\n",
       "  'start_position': 56,\n",
       "  'end_position': 79,\n",
       "  'subsection': [{'title': '2.1. Deriving The Meta-CoT Process ',\n",
       "    'level': 2,\n",
       "    'start_position': 58,\n",
       "    'end_position': 73,\n",
       "    'subsection': []},\n",
       "   {'title': '2.2. Why Does (Classical) CoT Fail? ',\n",
       "    'level': 2,\n",
       "    'start_position': 74,\n",
       "    'end_position': 79,\n",
       "    'subsection': []}]},\n",
       " {'title': '3. Towards Deliberate Reasoning With Language Models - Search ',\n",
       "  'level': 1,\n",
       "  'start_position': 80,\n",
       "  'end_position': 117,\n",
       "  'subsection': [{'title': '3.1. Inference-Time Compute: Search ',\n",
       "    'level': 2,\n",
       "    'start_position': 86,\n",
       "    'end_position': 91,\n",
       "    'subsection': []},\n",
       "   {'title': '3.2. Inference-Time Compute: Verifcation ',\n",
       "    'level': 2,\n",
       "    'start_position': 92,\n",
       "    'end_position': 98,\n",
       "    'subsection': []},\n",
       "   {'title': '3.3. From Best-of-N To General Search ',\n",
       "    'level': 2,\n",
       "    'start_position': 99,\n",
       "    'end_position': 109,\n",
       "    'subsection': []},\n",
       "   {'title': '3.4. Is Search (Inference Time Compute) A Fundamental Capability Shift? ',\n",
       "    'level': 2,\n",
       "    'start_position': 110,\n",
       "    'end_position': 117,\n",
       "    'subsection': []}]},\n",
       " {'title': '4. Towards Meta-CoT Reasoning ',\n",
       "  'level': 1,\n",
       "  'start_position': 118,\n",
       "  'end_position': 194,\n",
       "  'subsection': [{'title': '4.1. Bootstrapping Meta-CoT ',\n",
       "    'level': 2,\n",
       "    'start_position': 122,\n",
       "    'end_position': 132,\n",
       "    'subsection': [{'title': '4.1.1. Self-Taught Reasoner ',\n",
       "      'level': 3,\n",
       "      'start_position': 124,\n",
       "      'end_position': 128,\n",
       "      'subsection': []},\n",
       "     {'title': '4.1.2. Meta-STaR ',\n",
       "      'level': 3,\n",
       "      'start_position': 129,\n",
       "      'end_position': 132,\n",
       "      'subsection': []}]},\n",
       "   {'title': '4.2. Empirical Examples Of Internalizing Search ',\n",
       "    'level': 2,\n",
       "    'start_position': 133,\n",
       "    'end_position': 168,\n",
       "    'subsection': [{'title': '4.2.1. Small-Scale Empirical Results on Internalizing Search ',\n",
       "      'level': 3,\n",
       "      'start_position': 135,\n",
       "      'end_position': 142,\n",
       "      'subsection': []},\n",
       "     {'title': '4.2.2. In-context Exploration For LLMs ',\n",
       "      'level': 3,\n",
       "      'start_position': 143,\n",
       "      'end_position': 150,\n",
       "      'subsection': []},\n",
       "     {'title': '4.2.3. Using variable Compute ',\n",
       "      'level': 3,\n",
       "      'start_position': 151,\n",
       "      'end_position': 157,\n",
       "      'subsection': []},\n",
       "     {'title': '4.2.4. Backtracking in LLMs ',\n",
       "      'level': 3,\n",
       "      'start_position': 158,\n",
       "      'end_position': 168,\n",
       "      'subsection': []}]},\n",
       "   {'title': '4.3. Synthetic Meta-CoT Via Search ',\n",
       "    'level': 2,\n",
       "    'start_position': 169,\n",
       "    'end_position': 182,\n",
       "    'subsection': [{'title': '4.3.1. Monte-Carlo Tree Search ',\n",
       "      'level': 3,\n",
       "      'start_position': 175,\n",
       "      'end_position': 179,\n",
       "      'subsection': []},\n",
       "     {'title': '4.3.2. $A^{*}$ search ',\n",
       "      'level': 3,\n",
       "      'start_position': 180,\n",
       "      'end_position': 182,\n",
       "      'subsection': []}]},\n",
       "   {'title': '4.4. Do Advanced Reasoning Systems Implement In-Context Search? ',\n",
       "    'level': 2,\n",
       "    'start_position': 183,\n",
       "    'end_position': 194,\n",
       "    'subsection': []}]},\n",
       " {'title': '5. Process Supervision ',\n",
       "  'level': 1,\n",
       "  'start_position': 195,\n",
       "  'end_position': 205,\n",
       "  'subsection': [{'title': '5.1. Learning Process Reward Models ',\n",
       "    'level': 2,\n",
       "    'start_position': 197,\n",
       "    'end_position': 198,\n",
       "    'subsection': []},\n",
       "   {'title': '5.2. PRM Quality And Its Efect On Search ',\n",
       "    'level': 2,\n",
       "    'start_position': 199,\n",
       "    'end_position': 202,\n",
       "    'subsection': []},\n",
       "   {'title': '5.3. Verifable Versus Open-Ended Problems ',\n",
       "    'level': 2,\n",
       "    'start_position': 203,\n",
       "    'end_position': 205,\n",
       "    'subsection': []}]},\n",
       " {'title': '6. Meta Reinforcement Learning - Learning How To Think ',\n",
       "  'level': 1,\n",
       "  'start_position': 206,\n",
       "  'end_position': 263,\n",
       "  'subsection': [{'title': '6.1. Meta-RL In Small Domains ',\n",
       "    'level': 2,\n",
       "    'start_position': 229,\n",
       "    'end_position': 232,\n",
       "    'subsection': []},\n",
       "   {'title': '6.2. Meta-RL In Language Model Reasoning ',\n",
       "    'level': 2,\n",
       "    'start_position': 233,\n",
       "    'end_position': 240,\n",
       "    'subsection': []},\n",
       "   {'title': '6.3. Efciency Or Super-Intelligence? ',\n",
       "    'level': 2,\n",
       "    'start_position': 241,\n",
       "    'end_position': 248,\n",
       "    'subsection': []},\n",
       "   {'title': '6.4. Can System 2 Reasoning Emerge From Pure RL? ',\n",
       "    'level': 2,\n",
       "    'start_position': 249,\n",
       "    'end_position': 263,\n",
       "    'subsection': [{'title': '6.4.1. Inducing Meta-Reasoning In LLMs ',\n",
       "      'level': 3,\n",
       "      'start_position': 252,\n",
       "      'end_position': 263,\n",
       "      'subsection': []}]}]},\n",
       " {'title': '7. Putting It All Together - A Pipeline for System 2 Reasoning ',\n",
       "  'level': 1,\n",
       "  'start_position': 264,\n",
       "  'end_position': 296,\n",
       "  'subsection': [{'title': '7.1. Instruction Tuning ',\n",
       "    'level': 2,\n",
       "    'start_position': 266,\n",
       "    'end_position': 267,\n",
       "    'subsection': []},\n",
       "   {'title': '7.2. Post-Training With RL ',\n",
       "    'level': 2,\n",
       "    'start_position': 268,\n",
       "    'end_position': 296,\n",
       "    'subsection': [{'title': '7.2.2. Discount Rates ',\n",
       "      'level': 3,\n",
       "      'start_position': 289,\n",
       "      'end_position': 296,\n",
       "      'subsection': []}]}]},\n",
       " {'title': '8. Going Forward ',\n",
       "  'level': 1,\n",
       "  'start_position': 297,\n",
       "  'end_position': 349,\n",
       "  'subsection': [{'title': '8.1. The \"Big MATH\" Project ',\n",
       "    'level': 2,\n",
       "    'start_position': 301,\n",
       "    'end_position': 318,\n",
       "    'subsection': [{'title': '8.1.1. Data Sourcing ',\n",
       "      'level': 3,\n",
       "      'start_position': 310,\n",
       "      'end_position': 312,\n",
       "      'subsection': []},\n",
       "     {'title': '8.1.2. Data Filtering ',\n",
       "      'level': 3,\n",
       "      'start_position': 313,\n",
       "      'end_position': 318,\n",
       "      'subsection': []}]},\n",
       "   {'title': '8.2. Infrastructure ',\n",
       "    'level': 2,\n",
       "    'start_position': 319,\n",
       "    'end_position': 325,\n",
       "    'subsection': []},\n",
       "   {'title': '8.3. Open Research Questions ',\n",
       "    'level': 2,\n",
       "    'start_position': 326,\n",
       "    'end_position': 349,\n",
       "    'subsection': [{'title': '8.3.1. Open-Ended Verifcation And CoT Faithfulness ',\n",
       "      'level': 3,\n",
       "      'start_position': 328,\n",
       "      'end_position': 331,\n",
       "      'subsection': []},\n",
       "     {'title': '8.3.2. Process Guidance And The Verifer Gap ',\n",
       "      'level': 3,\n",
       "      'start_position': 332,\n",
       "      'end_position': 334,\n",
       "      'subsection': []},\n",
       "     {'title': '8.3.3. Scaling Laws For Reasoning And Search ',\n",
       "      'level': 3,\n",
       "      'start_position': 335,\n",
       "      'end_position': 337,\n",
       "      'subsection': []},\n",
       "     {'title': '8.3.4. Meta-Search/Search 2 ',\n",
       "      'level': 3,\n",
       "      'start_position': 338,\n",
       "      'end_position': 342,\n",
       "      'subsection': []},\n",
       "     {'title': '8.3.5. Reasoning with External Tools ',\n",
       "      'level': 3,\n",
       "      'start_position': 343,\n",
       "      'end_position': 349,\n",
       "      'subsection': []}]}]},\n",
       " {'title': '9. Conclusion ',\n",
       "  'level': 1,\n",
       "  'start_position': 350,\n",
       "  'end_position': 353,\n",
       "  'subsection': []},\n",
       " {'title': '10. Acknowledgments ',\n",
       "  'level': 1,\n",
       "  'start_position': 354,\n",
       "  'end_position': 355,\n",
       "  'subsection': []},\n",
       " {'title': 'References ',\n",
       "  'level': 1,\n",
       "  'start_position': 356,\n",
       "  'end_position': 413,\n",
       "  'subsection': []},\n",
       " {'title': 'A. Prompting ',\n",
       "  'level': 1,\n",
       "  'start_position': 414,\n",
       "  'end_position': 417,\n",
       "  'subsection': []},\n",
       " {'title': 'B. Regret Analysis ',\n",
       "  'level': 1,\n",
       "  'start_position': 418,\n",
       "  'end_position': 419,\n",
       "  'subsection': []},\n",
       " {'title': 'C. Diferent Instruction Tuning Objectives ',\n",
       "  'level': 1,\n",
       "  'start_position': 420,\n",
       "  'end_position': 430,\n",
       "  'subsection': []},\n",
       " {'title': 'D. MCTS Details ',\n",
       "  'level': 1,\n",
       "  'start_position': 431,\n",
       "  'end_position': 440,\n",
       "  'subsection': []},\n",
       " {'title': 'E. Chains-Of-Thought ',\n",
       "  'level': 1,\n",
       "  'start_position': 441,\n",
       "  'end_position': 643,\n",
       "  'subsection': []}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toc_hierachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section 1. Introduction  6335\n",
      "section 2. Meta Chain-Of-Thought  10669\n",
      "section 3. Towards Deliberate Reasoning With Language Models - Search  13821\n",
      "subsection 4.1. Bootstrapping Meta-CoT  3760\n",
      "subsection 4.2. Empirical Examples Of Internalizing Search  13436\n",
      "subsection 4.3. Synthetic Meta-CoT Via Search  4532\n",
      "subsection 4.4. Do Advanced Reasoning Systems Implement In-Context Search?  4534\n",
      "section 5. Process Supervision  6885\n",
      "subsection 6.1. Meta-RL In Small Domains  1275\n",
      "subsection 6.2. Meta-RL In Language Model Reasoning  2672\n",
      "subsection 6.3. Efciency Or Super-Intelligence?  3958\n",
      "subsection 6.4. Can System 2 Reasoning Emerge From Pure RL?  9318\n",
      "section 7. Putting It All Together - A Pipeline for System 2 Reasoning  15162\n",
      "subsection 8.1. The \"Big MATH\" Project  10507\n",
      "subsection 8.2. Infrastructure  2648\n",
      "subsection 8.3. Open Research Questions  11793\n",
      "section 9. Conclusion  1745\n",
      "section 10. Acknowledgments  197\n",
      "section References  32378\n",
      "section A. Prompting  711\n",
      "section B. Regret Analysis  257\n",
      "section C. Diferent Instruction Tuning Objectives  2665\n",
      "section D. MCTS Details  2937\n",
      "section E. Chains-Of-Thought  64521\n"
     ]
    }
   ],
   "source": [
    "seg_paras = seg.gen_seg_paras(toc_hierachy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'type': 'image', 'img_path': 'images/c9d400a79bd62951a363cac2aabd041f71ae1d695b6a98d1affe998739c86cb1.jpg', 'img_caption': ['Figure 1: Top: Performance of current frontier models by size on the HARP mathematics benchmark (Yue et al., 2024) by difculty level and topic. The OpenAI O1 series signifcantly out-performs prior generation models across the board. Source: Figure 3 in (Yue et al., 2024). Bottom Average number of tokens generated by each model grouped by difculty level, as well as average number of tokens in human-generated solutions (using GPT4 tokenizer). Source: Figure 4 in (Yue et al., 2024). '], 'img_footnote': [], 'page_idx': 7, 'id': 'Figure 1', 'related_ids': ['Figure 3', 'Figure 4'], 'if_aligned': True}]\n",
      "[]\n",
      "[{'type': 'image', 'img_path': 'images/98af3e8774697add630cde871085b466600f00539cd676efc56ec7b6e7631f9c.jpg', 'img_caption': ['Figure 13: Resulting $\\\\mathbf{A}^{*}$ search tree on the math problem from OpenAI (2024). This trace presents more of a best-frst approach with fewer backtracks, concentrated around key steps, as compared to the one produced by MCTS in Figure 12. '], 'img_footnote': [], 'page_idx': 22, 'id': 'Figure 13', 'related_ids': ['Figure 12'], 'if_aligned': True}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'type': 'image', 'img_path': 'images/8c989d4b2b9ea1eb55971eb6812c88935953c1ef2e8d66451b9faf96f5219c79.jpg', 'img_caption': ['Figure 17: Left: Scaling curves for Best-of-N (BoN) using PRMs trained with diferent number of questions with oracle and majority vote. Right: Beam search $\\\\mathrm{N}{=}5$ , beam width $=4$ ) accuracy and number of tokens used during search with the same PRMs. With more training data, the PRM’s ability to verify at outcome-level and process-level improves. '], 'img_footnote': [], 'page_idx': 28, 'id': 'Figure 17', 'related_ids': [], 'if_aligned': True}]\n",
      "[{'type': 'image', 'img_path': 'images/6c9177669d0ef88466bdb872144cb9a411626a12e1f505dcd53cfed30fa735cc.jpg', 'img_caption': ['Figure 20: The benefts of reinforcement learning for langauge model reasoning. When comparing Expert Iteration $(\\\\mathrm{SoS}\\\\!+\\\\!\\\\mathrm{STaR})$ vs. the RL-based $\\\\mathrm{SoS+APA}_{\\\\mathrm{}}$ , we see that the use of RL leads to improved policy performance (left), with fewer arithmetic errors (center), and improved efciency (right). Source: (left to right) Figures 4a, 6a, and 6b from (Gandhi et al., 2024). '], 'img_footnote': [], 'page_idx': 30, 'id': 'Figure 20', 'related_ids': ['Figures'], 'if_aligned': True}]\n",
      "[]\n",
      "[]\n",
      "[{'type': 'image', 'img_path': 'images/6a39a587fa7886e943f5ea863b19fbaba78aed8e33ad3c92f1a043a18e4269e6.jpg', 'img_caption': ['Figure 21: Overview of Reinforcement Learning with Execution Feedback. This training routine directly maps to the E-RL2 framework (Stadie et al., 2019). Source: Figure 2 in (Gehring et al., 2024). '], 'img_footnote': [], 'page_idx': 31, 'id': 'Figure 21', 'related_ids': ['Figure 2'], 'if_aligned': True}]\n",
      "[]\n",
      "[{'type': 'image', 'img_path': 'images/6c9177669d0ef88466bdb872144cb9a411626a12e1f505dcd53cfed30fa735cc.jpg', 'img_caption': ['Figure 20: The benefts of reinforcement learning for langauge model reasoning. When comparing Expert Iteration $(\\\\mathrm{SoS}\\\\!+\\\\!\\\\mathrm{STaR})$ vs. the RL-based $\\\\mathrm{SoS+APA}_{\\\\mathrm{}}$ , we see that the use of RL leads to improved policy performance (left), with fewer arithmetic errors (center), and improved efciency (right). Source: (left to right) Figures 4a, 6a, and 6b from (Gandhi et al., 2024). '], 'img_footnote': [], 'page_idx': 30, 'id': 'Figure 20', 'related_ids': ['Figures'], 'if_aligned': True, 'if_being_reffered': True}]\n",
      "[]\n",
      "[{'type': 'image', 'img_path': 'images/611e5d526b2d832a8cf0d8a5762f16cb5baba521e529f9ff53b140024c141596.jpg', 'img_caption': ['Figure 22: Scaling results for Reinforcement Learning with Execution Feedback. Left: Pass $@1$ and pass $@10$ for 8 and 70B models when given either ground truth feedback or random execution feedback. Right: Model solve rates at various turn limits (1, 3, 5, and 10) and sample budgets. Source: Figure 4 in (Gehring et al., 2024). '], 'img_footnote': [], 'page_idx': 32, 'id': 'Figure 22', 'related_ids': ['Figure 4'], 'if_aligned': True}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'type': 'image', 'img_path': 'images/1b26d4f624508324ccfb855c6718b15c6633e921b1cac96e6dd6aec78f538530.jpg', 'img_caption': ['Figure 9: Inference compute scaling relationships for the o1 model (Left, sourced from (OpenAI, 2024) on AIME, Stream-of-Search on the Game of 24 (Middle) and MAV-MCTS on Chess (Right, sourced from (Schultz et al., 2024)). These fgures show performance of a single model under diferent token sampling budgets. '], 'img_footnote': [], 'page_idx': 17, 'id': 'Figure 9', 'related_ids': [], 'if_aligned': True}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[{'type': 'image', 'img_path': 'images/8cc13ba40376bdede242a8bbb1e64189ca212f1f0fff2e17d16625a0f5926730.jpg', 'img_caption': ['Figure 3: Scaling trends for verifer models on algorithmic reasoning, grade-school math (GSM8k), and transfer from GSM8k to MATH. The performance of all verifers improves in the best-of-N setting, as N increases. Figure sourced from (Zhang et al., 2024a). '], 'img_footnote': [], 'page_idx': 10, 'id': 'Figure 3', 'related_ids': ['Figure sourced'], 'if_aligned': True}]\n",
      "[{'type': 'image', 'img_path': 'images/611e5d526b2d832a8cf0d8a5762f16cb5baba521e529f9ff53b140024c141596.jpg', 'img_caption': ['Figure 22: Scaling results for Reinforcement Learning with Execution Feedback. Left: Pass $@1$ and pass $@10$ for 8 and 70B models when given either ground truth feedback or random execution feedback. Right: Model solve rates at various turn limits (1, 3, 5, and 10) and sample budgets. Source: Figure 4 in (Gehring et al., 2024). '], 'img_footnote': [], 'page_idx': 32, 'id': 'Figure 22', 'related_ids': ['Figure 4'], 'if_aligned': True, 'if_being_reffered': True}]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "seg_paras_rvsd = seg.restore_seg_elements(seg_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_text = seg.gen_md_from_json(pdf_json_rvsd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4fun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
